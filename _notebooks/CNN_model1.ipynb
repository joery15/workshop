{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_model1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRb__-98Awrd"
      },
      "source": [
        "# CNN_model1\n",
        "\n",
        "mnist case\n",
        "releated-link https://raw.githubusercontent.com/skyu0221/online-dropbox/master/ml/capstone2/train.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NQkD6etG7vR"
      },
      "source": [
        "\n",
        "## Part 1 - understand a model\n",
        "\n",
        "### Optimizers\n",
        "\n",
        "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater than zero. The goal of training a model is to find a set of weights and biases (i.e. parameters) that have, on average, a low loss across all examples. The term cost is used interchangably with loss. See the [loss section](https://keras.io/losses/) in the Keras documentation for a list and descriptions of what is available.\n",
        "\n",
        "![Side by side loss](https://drive.google.com/uc?id=1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae)\n",
        "<b>Figure 1.</b> Left: high loss and right: low loss.\n",
        "\n",
        "<!-- https://drive.google.com/file/d/1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae/view?usp=sharing\n",
        "<img src=\"./fig/LossSideBySide.png\" width=\"500\">\n",
        "<figcaption>Figure. Left: high loss and right: low loss.</figcaption>\n",
        " -->\n",
        "The optimizer is the algorithm used to minimize the loss/cost. Optimizers in neural networks work by finding the gradient/derivative of the loss with respect to the parameters (i.e. the weights). \"Gradient\" is the correct term since a we are looking at multi-dimensional systems (i.e. many parameters), however, the terms are often used interchangably. For those who didn't take multivariate calculus, just think of the gradient as a derivative. The derivative of the loss with respect to a parameters tells us how much the loss changes when we nudge a weight up or down. So, by knowing how a given parameter affects the loss the optimizer can change it so as to decrease the loss. The various optimizers differ in how they change the weights. \n",
        "\n",
        "#### Mini-overview over popular optimizers\n",
        "\n",
        "* **Stochastic Gradient Descent (SGD)**. This is the most basic and easy to understand optimizer. It updates the weights in the negative direction of the gradient by taking the average gradient of mini-batch of data (e.g. 20-1000 examples) in each step. Vanilla SGD only has one hyper-parameter, the learning rate.\n",
        "* **Momentum**. This optimizer \"gains speed\" when the gradient has pointed in the same direction for several consecutive updates. That is, it has a momentum and want to keep moving in that direction. It gains momentum by accumulating an exponentially decaying moving average of past gradients. The step size depends on how large and aligned the sequence of gradients are. The most important hyper-parameter is alpha and common values are 0.5 and 0.9.\n",
        "* **Nesterov Momentum**. This is a modification of the standard momentum optimizer.\n",
        "* **AdaGrad**. This optimizer Ada-ptively sets the learning rate depending on the steepness/magnitude of the Grad-ients. This is done so that weights with big gradients get a smaller effective learning rate, and weights with small gradients will get a greater effective learning rate. The result is quicker progress in the more gently sloped directions of the weight space and a slowdown in stepp regions.\n",
        "* **RMSProp**. This is modification of AdaGrad, where the accumulated gradient decays, that is, the influence of previous gradients gradually decreases.\n",
        "* **Adam**. The name comes from \"adaptive moments\", and it is a combination of RMSProp and momentum. It has several hyper-parameters.\n",
        "\n",
        "The above list just gives a quick overview of some of the most common. However, old optimizers are constantly improved and new are developed. SGD and momentum are most basic and easiest to understand and implement. They are still in use, but the more advanced optimizers tend to be better for practical use. Which one to use is generally an emperical question depending on both the data and the model.\n",
        "\n",
        "For a more complete overview of optimization algorithms see [this comparison](http://ruder.io/optimizing-gradient-descent/), and to see what is available in Keras, see the [optimizer section](https://keras.io/optimizers/) of the documentation.\n",
        "\n",
        "See the images below for a comparison of optimizers in a 2D space (NAG: Nesterov accelerated gradient, Adadelta: an extension of AdaGrad).\n",
        "\n",
        "![Contours - optimizer comparison](https://drive.google.com/uc?id=1CmrD-UPZ7EIUjRuO_ib7k9CL1FO2bbLk)\n",
        "<b>Figure 2.</b> Comparison of six different optimizers.\n",
        "\n",
        "\n",
        "![Saddle point - optimizer comparison](https://drive.google.com/uc?id=1QVhN9rAvCjXtGyNZkmFivyyCzNsntObh)\n",
        "<b>Figure 3.</b> Comparison of six different optimizers at a saddle point.\n",
        "\n",
        "<!-- <img src=\"./fig/contours_evaluation_optimizers.gif\" width=\"500\">\n",
        "<img src=\"./fig/saddle_point_evaluation_optimizers.gif\" width=\"500\"> -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83Zunsc6G7vS"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# for the random seed\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# set the random seeds to get reproducible results\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "X, y = X[:1000], y[:1000]\n",
        "X = X.reshape(X.shape[0], 28, 28, 1)\n",
        "# Normalize\n",
        "X = X / 255.\n",
        "# number of unique classes\n",
        "num_classes = len(np.unique(y))\n",
        "y = y.astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
        "\n",
        "num_tot = y.shape[0]\n",
        "num_train = y_train.shape[0]\n",
        "num_test = y_test.shape[0]\n",
        "\n",
        "y_oh = np.zeros((num_tot, num_classes))\n",
        "y_oh[range(num_tot), y] = 1\n",
        "\n",
        "y_oh_train = np.zeros((num_train, num_classes))\n",
        "y_oh_train[range(num_train), y_train] = 1\n",
        "\n",
        "y_oh_test = np.zeros((num_test, num_classes))\n",
        "y_oh_test[range(num_test), y_test] = 1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBjvQ0ZnZ4iL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee1cfe4-2274-4b9e-8586-e6ce8d4bdda0"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMemOR9oG7vS"
      },
      "source": [
        "### Question 1\n",
        "**The data set**\n",
        "\n",
        "Plot a three examples from the data set.\n",
        "* What type of data are in the data set?\n",
        "\n",
        "    <span style=\"color:red\"> < **numpy.ndarray** > </span>\n",
        "    \n",
        "\n",
        "* What does the line ```X = X.reshape(X.shape[0], 28, 28, 1)``` do?\n",
        "\n",
        "  < make the X into 4D array which elements is 1000,28,28,1>\n",
        "\n",
        "Look at how the encoding of the targets (i.e. ```y```) is changed. E.g. the lines\n",
        "```\n",
        "    y_oh = np.zeros((num_tot, num_classes))\n",
        "    y_oh[range(num_tot), y] = 1\n",
        "```\n",
        "Print out a few rows of ```y``` next to ```y_oh```.\n",
        "* What is the relationship between ```y``` and ```y_oh```?\n",
        "\n",
        "    <span style=\"color:red\"> <  **when the columns index in y_oh equal to y, then the value =1ï¼Œ otherwise value in y_oh euqal to 0** > </span>\n",
        "    \n",
        "    \n",
        "* What is the type of encoding in ```y_oh``` called and why is it used?\n",
        "\n",
        "    <span style=\"color:red\"> <**one-hot encoding**> \n",
        "    </span>\n",
        "    <**Because it transform y into the value between 0 and 1, which is suitable for the  equally categorical y**>\n",
        "    \n",
        "* Plot three data examples in the same figure and set the correct label as title. \n",
        "    * It should be possible to see what the data represent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5CQf_iasNn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "03a23157-b319-4aa3-abe2-784db7103add"
      },
      "source": [
        "X_plot, y_plot = X[:3], y[:3]\n",
        "X_plot=X_plot.reshape(3,28,28)\n",
        "fig=plt.figure(figsize=(10,10))\n",
        "fig.add_subplot(1,3,1)\n",
        "plt.imshow(X_plot[0])\n",
        "plt.title(y_plot[0])\n",
        "\n",
        "fig.add_subplot(1,3,2)\n",
        "plt.imshow(X_plot[1])\n",
        "plt.title(y_plot[1])\n",
        "\n",
        "fig.add_subplot(1,3,3)\n",
        "plt.imshow(X_plot[2])\n",
        "plt.title(y_plot[2])\n",
        "\n",
        "plt.show()\n",
        "#for i range(1,4):\n",
        "#  img=X_plot[i]\n",
        "#  fig.add_subplot(1,3,i)\n",
        "#  plt.title(y[i])\n",
        "#  plt.imshow(img)\n",
        "#plt.show\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGsl5ZbNud5M"
      },
      "source": [
        "# Question 2\n",
        "\n",
        "\n",
        "To avoid the problem of overfitting, we need to artificially expand the data set. We can use existing data to generate 'fake data'. We add a small change to each picture on the built-in data:\n",
        "\n",
        "* Randomly rotate some training images by 10 degrees\n",
        "* Randomly Zoom by 10% some training images\n",
        "* Randomly shift images horizontally by 10% of the width\n",
        "* Randomly shift images vertically by 10% of the height"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D66xcoR_udeK"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            zoom_range = 0.1, # Randomly zoom image \n",
        "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "            horizontal_flip=False,  # randomly flip images\n",
        "            vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4MlblbwG7vS"
      },
      "source": [
        "### Question 3\n",
        "**The model**\n",
        "\n",
        "Below is some code for bulding and training a model with Keras.\n",
        "* What type of network is implemented below? I.e. a normal MLP, RNN, CNN, Logistic Regression...?\n",
        "\n",
        "    <span style=\"color:red\"> <***CNN***> </span>\n",
        "    \n",
        "    \n",
        "* What does ```Dropout()``` do?\n",
        "\n",
        "    <span style=\"color:red\"> <***controll overfitting***> </span>\n",
        "\n",
        "\n",
        "* Which type of activation function is used for the hidden layers?\n",
        "\n",
        "    <span style=\"color:red\"> <***Relu***> </span>\n",
        "\n",
        "\n",
        "* Which type of activation function is used for the output layer?\n",
        "\n",
        "    <span style=\"color:red\"> <***Softmax***> </span>\n",
        "\n",
        "\n",
        "* Why are two different activation functions used?\n",
        "\n",
        "    <span style=\"color:red\"> <***Different activation functions allow for different non-linearities which might work better for solving a specific function: ReLU is fast to compute so nice for deep networks with high training times; Softmax is used for multi-class classification problems and predict a multinomial probability distribution***> </span>\n",
        "\n",
        "\n",
        "* What optimizer is used in the model below?\n",
        "\n",
        "    <span style=\"color:red\"> <***Stochastic Gradient Descent (SGD)***> </span>\n",
        "\n",
        "\n",
        "* How often are the weights updated (i.e. after how many data examples)?\n",
        "\n",
        "    <span style=\"color:red\"> <**after the 32 samples, the weight will ungrades once**> </span>\n",
        "\n",
        "\n",
        "* What loss function is used?\n",
        "\n",
        "    <span style=\"color:red\"> <***Categorical_crossentropy***> </span>\n",
        "\n",
        "\n",
        "* How many parameters (i.e. weights and biases, NOT hyper-parameters) does the model have?\n",
        "\n",
        "    <span style=\"color:red\"> <***108,618***> </span>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNvAAvzjG7vS",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ef0ead-e830-46e5-9dda-a71ff87d60b0"
      },
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_oh_train, batch_size=32, epochs=60)\n",
        "\n",
        "# Evaluate performance\n",
        "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
        "\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "25/25 [==============================] - 1s 17ms/step - loss: 2.2322\n",
            "Epoch 2/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.3094\n",
            "Epoch 3/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.5476\n",
            "Epoch 4/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.3458\n",
            "Epoch 5/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.2481\n",
            "Epoch 6/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.1618\n",
            "Epoch 7/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.1702\n",
            "Epoch 8/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.1203\n",
            "Epoch 9/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0991\n",
            "Epoch 10/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.0544\n",
            "Epoch 11/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0416\n",
            "Epoch 12/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.0405\n",
            "Epoch 13/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0346\n",
            "Epoch 14/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0197\n",
            "Epoch 15/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0155\n",
            "Epoch 16/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.0179\n",
            "Epoch 17/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.0119\n",
            "Epoch 18/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.0071\n",
            "Epoch 19/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0077\n",
            "Epoch 20/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0040\n",
            "Epoch 21/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0042\n",
            "Epoch 22/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0032\n",
            "Epoch 23/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.0035\n",
            "Epoch 24/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0023\n",
            "Epoch 25/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0022\n",
            "Epoch 26/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0022\n",
            "Epoch 27/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0021\n",
            "Epoch 28/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.0019\n",
            "Epoch 29/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0019\n",
            "Epoch 30/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0015\n",
            "Epoch 31/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0018\n",
            "Epoch 32/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0015\n",
            "Epoch 33/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0013\n",
            "Epoch 34/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0013\n",
            "Epoch 35/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0012\n",
            "Epoch 36/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0011\n",
            "Epoch 37/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0012\n",
            "Epoch 38/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.0011\n",
            "Epoch 39/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0012\n",
            "Epoch 40/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0011\n",
            "Epoch 41/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.0010\n",
            "Epoch 42/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 9.5389e-04\n",
            "Epoch 43/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 8.7589e-04\n",
            "Epoch 44/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 7.5664e-04\n",
            "Epoch 45/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 7.8364e-04\n",
            "Epoch 46/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 8.6218e-04\n",
            "Epoch 47/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 7.4382e-04\n",
            "Epoch 48/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 5.9547e-04\n",
            "Epoch 49/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 7.4040e-04\n",
            "Epoch 50/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 6.5876e-04\n",
            "Epoch 51/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 6.1762e-04\n",
            "Epoch 52/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 5.8019e-04\n",
            "Epoch 53/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 5.5301e-04\n",
            "Epoch 54/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 5.6151e-04\n",
            "Epoch 55/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 6.9891e-04\n",
            "Epoch 56/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 6.1614e-04\n",
            "Epoch 57/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 5.2972e-04\n",
            "Epoch 58/60\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 6.1560e-04\n",
            "Epoch 59/60\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 5.8387e-04\n",
            "Epoch 60/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 5.5804e-04\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4677\n",
            "Accuracy: 0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suvhACy89m-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8e22e2e-78d6-40c3-db8b-50e21c75af47"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 108,618\n",
            "Trainable params: 108,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Abk3-GG7vS"
      },
      "source": [
        "## Part 3 - train a model\n",
        "\n",
        "A model's performance depends on many factors apart from the model architecture (e.g. type and number of layers) and the dataset. Here you will get to explore some of the factors that affect model performance. Much of the skill in training deep learning models lies in quickly finding good values/options for these choises.\n",
        "\n",
        "In order to observe the learning process it is best to compare the training set loss with the loss on the test set. How to visualize these variables with Keras is described under [Training history visualization](https://keras.io/visualization/#training-history-visualization) in the documentation.\n",
        "\n",
        "You will explore the effect of 1) optimizer, 2) training duration, and 3) dropout (see the question above).\n",
        "\n",
        "When training, an **epoch** is one pass through the full training set.\n",
        "\n",
        "\n",
        "\n",
        "## Further information\n",
        "For ideas about hyper-parameter tuning, take a look at the strategies described in the sklearn documentation under [model selection](https://scikit-learn.org/stable/model_selection.html), or in this [blog post](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html) from TensorFlow. For a more thorough discussion about optimizers see [this video](https://www.youtube.com/watch?v=DiNzQP7kK-s) discussing the article [Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers](https://arxiv.org/abs/2007.01547).\n",
        "\n",
        "\n",
        "**Good luck!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2swN1lCoWeu"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "* **Vizualize the training**. Use the model above to observe the training process. Train it for 150 epochs and then plot both \"loss\" and \"val_loss\" (i.e. loss on the valiadtion set, here the terms \"validation set\" and \"test set\" are used interchangably, but this is not always true). What is the optimal number of epochs for minimizing the test set loss? \n",
        "    * Remember to first reset the weights (this can be done by calling ```model.compile()```), otherwise the training just continues from where it was stopped earlier.\n",
        "\n",
        "    <**Based on the graph showed from the first following code, the optimal number of epochs is around 15**>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySbqfAV1B7DP"
      },
      "source": [
        "def built_CNN(droupout):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "  # Max pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(droupout))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "  # Max pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(droupout))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzf0UkjDzWCW"
      },
      "source": [
        "#Question: Vizualize the training\n",
        "model=built_CNN(0)\n",
        "model.compile(loss='categorical_crossentropy')\n",
        "history=model.fit(X_train, y_oh_train, batch_size=32, epochs=150,validation_data=(X_test,y_oh_test))\n",
        "\n",
        "plt.plot(history.history['val_loss'],label='test loss')\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "#the best epoch is 15, the maximum epoch can be up to 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "R0BNHsd2wDfY",
        "outputId": "2ad8ddcb-0d88-4943-dd34-e92cc9badf82"
      },
      "source": [
        "# Look at confusion matrix \n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = model.predict(X_test)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(y_oh_test,axis = 1) \n",
        "# compute the confusion matrix\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(10))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "2dMZHr9JB46t",
        "outputId": "c9009261-6c48-4109-a502-9348feb08529"
      },
      "source": [
        "# Display some error results \n",
        "\n",
        "# Errors are difference between predicted labels and true labels\n",
        "errors = (Y_pred_classes - Y_true != 0)\n",
        "\n",
        "Y_pred_classes_errors = Y_pred_classes[errors]\n",
        "Y_pred_errors = Y_pred[errors]\n",
        "Y_true_errors = Y_true[errors]\n",
        "X_val_errors = X_test[errors]\n",
        "\n",
        "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
        "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
        "    n = 0\n",
        "    nrows = 2\n",
        "    ncols = 3\n",
        "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
        "    fig.set_size_inches(10, 8)\n",
        "    for row in range(nrows):\n",
        "        for col in range(ncols):\n",
        "            error = errors_index[n]\n",
        "            ax[row,col].imshow((img_errors[error]).reshape((28,28)), cmap='gray')\n",
        "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
        "            n += 1\n",
        "\n",
        "# Probabilities of the wrong predicted numbers\n",
        "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
        "\n",
        "# Predicted probabilities of the true values in the error set\n",
        "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
        "\n",
        "# Difference between the probability of the predicted label and the true label\n",
        "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
        "\n",
        "# Sorted list of the delta prob errors\n",
        "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
        "\n",
        "# Top 6 errors \n",
        "most_important_errors = sorted_dela_errors[-6:]\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "# Show the top 6 errors\n",
        "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pMgm-tzofyD"
      },
      "source": [
        "# Question2\n",
        "\n",
        "* **Optimizer**. Select three different optimizers and for each find the close-to-optimal hyper-parameter(s). In your answer, include a) your three choises, b) best hyper-parameters for each of the three optimizers and, c) the code that produced the results.\n",
        "    * *NOTE* that how long the training takes varies with optimizer. I.e., make sure that the model is trained for long enough to reach optimal performance.\n",
        "\n",
        "    <**a) the three choices: SGD, Adam, RMSProp \n",
        "    b) hyper-parameters for each of the three optimizers can be shown from each search and the result as well.**>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiNDuK1fV0aE"
      },
      "source": [
        "#!pip install -U keras-tuner\n",
        "import tensorflow as tf\n",
        "import kerastuner as kt\n",
        "# Question: three optiomizer: SGD,Adam,RMSProp\n",
        "# the hyper-parameter is based on page13 of the article \n",
        "# Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers.\n",
        "\n",
        "#for SGD optimizer\n",
        "def build_model(hp):\n",
        "  model=built_CNN(0)\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(hp.Float('learning_rate', min_value=1e-4, max_value=1,sampling='log')),\n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "tuner = kt.Hyperband(build_model,objective='val_accuracy',max_epochs=30,project_name='SGD-1')\n",
        "tuner.search(X_train, y_oh_train,epochs=15,validation_data=(X_test,y_oh_test))\n",
        "tuner.results_summary()\n",
        "# the accur is 0.949999988079071\n",
        "#and the learning rate 0.18379900346409866"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLqwRC8XoL5e"
      },
      "source": [
        "# for Adam optimizer\n",
        "def build_model(hp):\n",
        "  model=built_CNN(0)\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1,sampling='log'),\n",
        "                                                   hp.Float('beta1', min_value=0.5, max_value=0.999,sampling='log'),\n",
        "                                                   hp.Float('beta2', min_value=0.8, max_value=0.999,sampling='log')),\n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "tuner = kt.Hyperband(build_model,objective='val_accuracy',max_epochs=30,project_name='Adam')\n",
        "tuner.search(X_train, y_oh_train,epochs=15,validation_data=(X_test,y_oh_test))\n",
        "tuner.results_summary()\n",
        "#accuracy: 0.9599999785423279\n",
        "#learning_rate: 0.00880194573492123\n",
        "#beta1: 0.8059667512580438\n",
        "#beta2: 0.818272724625341"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDB0Y44Hu_Jo"
      },
      "source": [
        "#for RMS prop optimizer\n",
        "def build_model(hp):\n",
        "  model=built_CNN(0)\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(hp.Float('learning_rate', min_value=1e-4, max_value=1, sampling='log'),\n",
        "                                                      hp.Float('rho', min_value=1e-3, max_value=1, sampling='log')),\n",
        "    loss='categorical_crossentropy', \n",
        "    metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "tuner = kt.Hyperband(build_model,objective='val_accuracy',max_epochs=30,project_name='RMSprop')\n",
        "tuner.search(X_train, y_oh_train,epochs=15,validation_data=(X_test,y_oh_test))\n",
        "tuner.results_summary()\n",
        "# the val_accuracy is 0.9649999737739563\n",
        "# learning_rate: 0.017378972480895958\n",
        "# rho: 0.011239435767055977"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNN7wJ7-o028"
      },
      "source": [
        "# Question3\n",
        "\n",
        "* **Dropout**. Use the best optimizer and do hyper-parameter seach and find the best value for ```Dropout()```.\n",
        "\n",
        "    <**the best optimizer is RMS Prop and the learning rate is 0.0174, the rho is 0.01124**>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9k-AWiyLRUw"
      },
      "source": [
        "#Question: find the best value for Dropout()\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "def CNN(dropout):\n",
        "  model=built_CNN(droupout=dropout)\n",
        "#Compile the model\n",
        "  RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.017378972480895958,rho=0.011239435767055977)\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=[\"accuracy\"])\n",
        "# Train the model\n",
        "  return model\n",
        "\n",
        "classifier = KerasClassifier(build_fn= CNN)\n",
        "parameters = {'dropout':[0,0.25,0.5,0.75,0.9]}\n",
        "grid_search = GridSearchCV(estimator = classifier,param_grid = parameters)\n",
        "grid_search = grid_search.fit(X_train, y_oh_train,epochs=15,validation_data = (X_test,y_oh_test))\n",
        "best_parameters = grid_search.best_params_\n",
        "print(best_parameters)\n",
        "# best parameters is 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7u8oLraPy30E",
        "outputId": "9b1ee09c-5b4e-4bf9-e2a0-8d9473688c1d"
      },
      "source": [
        "# the result for douput search\n",
        "import pandas as pd\n",
        "pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dropout</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.90875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.90750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.91125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.83625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.38500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dropout  Accuracy\n",
              "0     0.00   0.90875\n",
              "1     0.25   0.90750\n",
              "2     0.50   0.91125\n",
              "3     0.75   0.83625\n",
              "4     0.90   0.38500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBo5zdUHpIEB"
      },
      "source": [
        "# Question4\n",
        "\n",
        "* **Best model**. Combine the what you learned from the above three questions to build the best model. How much better is it than the worst and average models?\n",
        "\n",
        "    <span style=\"color:red\"> <**the accuracy in the best model is 0.915, the worst model is 0.59, the average one is 0.755**> </span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQY-cE_S5doI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e48ef9-6d8a-4b1f-ebaf-b5d6a45e168c"
      },
      "source": [
        "# Question: Best model: compare the best model with worst model, average model\n",
        "#for the best modelï¼šdepout=0.5,epochs=15,optimizer=RMSprop(learning_rate: 0.01737897,rho=rho=0.011239435767055977,accur_score=0.96)\n",
        "model=built_CNN(0.5)\n",
        "RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.017378972480895958,rho=0.011239435767055977)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop)\n",
        "model.fit(X_train, y_oh_train, batch_size=32, epochs=15)\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])\n",
        "#accur: 0.915"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "25/25 [==============================] - 1s 18ms/step - loss: 2.4598\n",
            "Epoch 2/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.0114\n",
            "Epoch 3/15\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.7831\n",
            "Epoch 4/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6503\n",
            "Epoch 5/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6528\n",
            "Epoch 6/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.7504\n",
            "Epoch 7/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6909\n",
            "Epoch 8/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.9260\n",
            "Epoch 9/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6351\n",
            "Epoch 10/15\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.6558\n",
            "Epoch 11/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6920\n",
            "Epoch 12/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.7351\n",
            "Epoch 13/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.5912\n",
            "Epoch 14/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6744\n",
            "Epoch 15/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6180\n",
            "Accuracy: 0.915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp6CM2LVG7vT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d86d85-b107-4507-af03-aa13946b7442"
      },
      "source": [
        "# for the average model: drouput=0.75,epochs=43,optimizer=adam(learning_rate: 0.0065,beta1=0.5763,beta2=0.009668,accur_score=0.86), from previour running of three optimizer models\n",
        "model=built_CNN(0.75)\n",
        "adam=tf.keras.optimizers.Adam(learning_rate=0.0065,beta_1=0.05763, beta_2=0.09668)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam)\n",
        "model.fit(X_train, y_oh_train, batch_size=32, epochs=23)\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])\n",
        "#accur: 0.755"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/23\n",
            "25/25 [==============================] - 1s 18ms/step - loss: 2.4009\n",
            "Epoch 2/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.7752\n",
            "Epoch 3/23\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 1.3729\n",
            "Epoch 4/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.1287\n",
            "Epoch 5/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.0517\n",
            "Epoch 6/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.8927\n",
            "Epoch 7/23\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.9042\n",
            "Epoch 8/23\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.7722\n",
            "Epoch 9/23\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.7155\n",
            "Epoch 10/23\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.8163\n",
            "Epoch 11/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6652\n",
            "Epoch 12/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.7522\n",
            "Epoch 13/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.7830\n",
            "Epoch 14/23\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.6586\n",
            "Epoch 15/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6536\n",
            "Epoch 16/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6772\n",
            "Epoch 17/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6028\n",
            "Epoch 18/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6327\n",
            "Epoch 19/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.7142\n",
            "Epoch 20/23\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.8677\n",
            "Epoch 21/23\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.7775\n",
            "Epoch 22/23\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.6911\n",
            "Epoch 23/23\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.8155\n",
            "Accuracy: 0.755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3cLRjEcuarp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f5c4b6-3a12-4c2c-8a1d-202204793223"
      },
      "source": [
        "# for the worst model: drouput=0.9 epochs=27 optimizer=SGD(learning_rate: 0.031045,accur_score=0.1), from previour running of three optimizer models\n",
        "model=built_CNN(0.9)\n",
        "sgd=tf.keras.optimizers.SGD(lr=0.031045)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X_train, y_oh_train, batch_size=32, epochs=27)\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])\n",
        "# accur=0.59"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/27\n",
            "25/25 [==============================] - 1s 19ms/step - loss: 2.7200\n",
            "Epoch 2/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.2927\n",
            "Epoch 3/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2932\n",
            "Epoch 4/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2827\n",
            "Epoch 5/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2868\n",
            "Epoch 6/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2587\n",
            "Epoch 7/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2625\n",
            "Epoch 8/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2763\n",
            "Epoch 9/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2505\n",
            "Epoch 10/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.2213\n",
            "Epoch 11/27\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 2.2102\n",
            "Epoch 12/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.2237\n",
            "Epoch 13/27\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 2.2014\n",
            "Epoch 14/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.1821\n",
            "Epoch 15/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.1136\n",
            "Epoch 16/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.0928\n",
            "Epoch 17/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.1198\n",
            "Epoch 18/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.0968\n",
            "Epoch 19/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.0929\n",
            "Epoch 20/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.1045\n",
            "Epoch 21/27\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 1.9597\n",
            "Epoch 22/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.9966\n",
            "Epoch 23/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.9483\n",
            "Epoch 24/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.9431\n",
            "Epoch 25/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.9616\n",
            "Epoch 26/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.9447\n",
            "Epoch 27/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.9289\n",
            "Accuracy: 0.59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RX1FzCSpZS9"
      },
      "source": [
        "# Question 4\n",
        "\n",
        "* **Results on the test set**. When doing this search for good model configuration/hyper-parameter values, the data set was split into *two* parts: a training set and a test set (the term \"validation\" was used interchangably wiht \"test\"). For your final model, is the performance (i.e. accuracy) on the test set representative for the performance one would expect on a previously unseen data set (drawn from the same distribution)? Why?\n",
        "\n",
        "    <span style=\"color:red\"> <**the unseen data, drawn from the same distribution, has the similar feature with validation. Thus, the performance for the best model is representative for unseen data,as we can see from the data in between 1000 and 1100 from data code**> </span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRnsCvzHTpOS"
      },
      "source": [
        "#Question:Results on the test set--get the new data\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "X, y = X[1000:1100], y[1000:1100]\n",
        "X = X.reshape(X.shape[0], 28, 28, 1)\n",
        "# Normalize\n",
        "X = X / 255.\n",
        "# number of unique classes\n",
        "num_classes = len(np.unique(y))\n",
        "y = y.astype(int)\n",
        "num_tot = y.shape[0]\n",
        "y_oh = np.zeros((num_tot, num_classes))\n",
        "y_oh[range(num_tot), y] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXlAZOiCTe_D",
        "outputId": "be2227ec-5450-47da-e829-eda8b2e4140a"
      },
      "source": [
        "# for the best model\n",
        "model=built_CNN(0.5)\n",
        "RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.017378972480895958,rho=0.011239435767055977)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop)\n",
        "model.fit(X_train, y_oh_train, batch_size=32, epochs=15)\n",
        "predictions = model.predict(X, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "print('Accuracy:', (predictions == y).sum() / predictions.shape[0])\n",
        "#accur: 0.82"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "25/25 [==============================] - 1s 19ms/step - loss: 2.3950\n",
            "Epoch 2/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.9490\n",
            "Epoch 3/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.7512\n",
            "Epoch 4/15\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.6449\n",
            "Epoch 5/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6058\n",
            "Epoch 6/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.7094\n",
            "Epoch 7/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6782\n",
            "Epoch 8/15\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 0.7850\n",
            "Epoch 9/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6068\n",
            "Epoch 10/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6128\n",
            "Epoch 11/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.7589\n",
            "Epoch 12/15\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.6959\n",
            "Epoch 13/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.6882\n",
            "Epoch 14/15\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 0.9664\n",
            "Epoch 15/15\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.7823\n",
            "Accuracy: 0.82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spvDPJtiT_o8",
        "outputId": "d64519bc-d6d7-4f51-ad03-c9e0a1e16cff"
      },
      "source": [
        "# for the worst model: drouput=0.9 epochs=27 optimizer=SGD(learning_rate: 0.031045,accur_score=0.1), from previour running of three optimizer models\n",
        "model=built_CNN(0.9)\n",
        "sgd=tf.keras.optimizers.SGD(lr=0.031045)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "model.fit(X_train, y_oh_train, batch_size=32, epochs=27)\n",
        "predictions = model.predict(X, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "print('Accuracy:', (predictions == y).sum() / predictions.shape[0])\n",
        "# accur=0.66"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/27\n",
            "25/25 [==============================] - 1s 17ms/step - loss: 2.5105\n",
            "Epoch 2/27\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 2.2885\n",
            "Epoch 3/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2951\n",
            "Epoch 4/27\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 2.2786\n",
            "Epoch 5/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2695\n",
            "Epoch 6/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2702\n",
            "Epoch 7/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 2.2469\n",
            "Epoch 8/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.2231\n",
            "Epoch 9/27\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 2.2266\n",
            "Epoch 10/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.1580\n",
            "Epoch 11/27\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 2.1434\n",
            "Epoch 12/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.1354\n",
            "Epoch 13/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.0635\n",
            "Epoch 14/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 2.1005\n",
            "Epoch 15/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.9976\n",
            "Epoch 16/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.9456\n",
            "Epoch 17/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.9843\n",
            "Epoch 18/27\n",
            "25/25 [==============================] - 0s 17ms/step - loss: 1.9434\n",
            "Epoch 19/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.9339\n",
            "Epoch 20/27\n",
            "25/25 [==============================] - 0s 18ms/step - loss: 1.9911\n",
            "Epoch 21/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.8736\n",
            "Epoch 22/27\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.9109\n",
            "Epoch 23/27\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 1.8034\n",
            "Epoch 24/27\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 1.7933\n",
            "Epoch 25/27\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 1.8410\n",
            "Epoch 26/27\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.7891\n",
            "Epoch 27/27\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.6571\n",
            "Accuracy: 0.66\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}