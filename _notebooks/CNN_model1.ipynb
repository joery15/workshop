{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRb__-98Awrd"
   },
   "source": [
    "# CNN_model1\n",
    "\n",
    "mnist case\n",
    "releated-link https://raw.githubusercontent.com/skyu0221/online-dropbox/master/ml/capstone2/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NQkD6etG7vR"
   },
   "source": [
    "\n",
    "## Part 1 - understand a model\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater than zero. The goal of training a model is to find a set of weights and biases (i.e. parameters) that have, on average, a low loss across all examples. The term cost is used interchangably with loss. See the [loss section](https://keras.io/losses/) in the Keras documentation for a list and descriptions of what is available.\n",
    "\n",
    "![Side by side loss](https://drive.google.com/uc?id=1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae)\n",
    "<b>Figure 1.</b> Left: high loss and right: low loss.\n",
    "\n",
    "<!-- https://drive.google.com/file/d/1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae/view?usp=sharing\n",
    "<img src=\"./fig/LossSideBySide.png\" width=\"500\">\n",
    "<figcaption>Figure. Left: high loss and right: low loss.</figcaption>\n",
    " -->\n",
    "The optimizer is the algorithm used to minimize the loss/cost. Optimizers in neural networks work by finding the gradient/derivative of the loss with respect to the parameters (i.e. the weights). \"Gradient\" is the correct term since a we are looking at multi-dimensional systems (i.e. many parameters), however, the terms are often used interchangably. For those who didn't take multivariate calculus, just think of the gradient as a derivative. The derivative of the loss with respect to a parameters tells us how much the loss changes when we nudge a weight up or down. So, by knowing how a given parameter affects the loss the optimizer can change it so as to decrease the loss. The various optimizers differ in how they change the weights. \n",
    "\n",
    "#### Mini-overview over popular optimizers\n",
    "\n",
    "* **Stochastic Gradient Descent (SGD)**. This is the most basic and easy to understand optimizer. It updates the weights in the negative direction of the gradient by taking the average gradient of mini-batch of data (e.g. 20-1000 examples) in each step. Vanilla SGD only has one hyper-parameter, the learning rate.\n",
    "* **Momentum**. This optimizer \"gains speed\" when the gradient has pointed in the same direction for several consecutive updates. That is, it has a momentum and want to keep moving in that direction. It gains momentum by accumulating an exponentially decaying moving average of past gradients. The step size depends on how large and aligned the sequence of gradients are. The most important hyper-parameter is alpha and common values are 0.5 and 0.9.\n",
    "* **Nesterov Momentum**. This is a modification of the standard momentum optimizer.\n",
    "* **AdaGrad**. This optimizer Ada-ptively sets the learning rate depending on the steepness/magnitude of the Grad-ients. This is done so that weights with big gradients get a smaller effective learning rate, and weights with small gradients will get a greater effective learning rate. The result is quicker progress in the more gently sloped directions of the weight space and a slowdown in stepp regions.\n",
    "* **RMSProp**. This is modification of AdaGrad, where the accumulated gradient decays, that is, the influence of previous gradients gradually decreases.\n",
    "* **Adam**. The name comes from \"adaptive moments\", and it is a combination of RMSProp and momentum. It has several hyper-parameters.\n",
    "\n",
    "The above list just gives a quick overview of some of the most common. However, old optimizers are constantly improved and new are developed. SGD and momentum are most basic and easiest to understand and implement. They are still in use, but the more advanced optimizers tend to be better for practical use. Which one to use is generally an emperical question depending on both the data and the model.\n",
    "\n",
    "For a more complete overview of optimization algorithms see [this comparison](http://ruder.io/optimizing-gradient-descent/), and to see what is available in Keras, see the [optimizer section](https://keras.io/optimizers/) of the documentation.\n",
    "\n",
    "See the images below for a comparison of optimizers in a 2D space (NAG: Nesterov accelerated gradient, Adadelta: an extension of AdaGrad).\n",
    "\n",
    "![Contours - optimizer comparison](https://drive.google.com/uc?id=1CmrD-UPZ7EIUjRuO_ib7k9CL1FO2bbLk)\n",
    "<b>Figure 2.</b> Comparison of six different optimizers.\n",
    "\n",
    "\n",
    "![Saddle point - optimizer comparison](https://drive.google.com/uc?id=1QVhN9rAvCjXtGyNZkmFivyyCzNsntObh)\n",
    "<b>Figure 3.</b> Comparison of six different optimizers at a saddle point.\n",
    "\n",
    "<!-- <img src=\"./fig/contours_evaluation_optimizers.gif\" width=\"500\">\n",
    "<img src=\"./fig/saddle_point_evaluation_optimizers.gif\" width=\"500\"> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.4.0-cp38-cp38-macosx_10_11_x86_64.whl (175.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 175.5 MB 22 kB/s  eta 0:00:01    |██                              | 10.6 MB 7.0 MB/s eta 0:00:24     |██████████▎                     | 56.6 MB 6.5 MB/s eta 0:00:19     |███████████▏                    | 61.4 MB 7.4 MB/s eta 0:00:16     |████████████████▉               | 92.1 MB 6.1 MB/s eta 0:00:14     |██████████████████              | 98.8 MB 7.5 MB/s eta 0:00:11     |██████████████████▎             | 100.3 MB 7.5 MB/s eta 0:00:11     |████████████████████▋           | 113.1 MB 7.9 MB/s eta 0:00:08     |█████████████████████████       | 137.4 MB 7.7 MB/s eta 0:00:05     |█████████████████████████▍      | 139.2 MB 5.9 MB/s eta 0:00:07     |███████████████████████████     | 147.8 MB 7.2 MB/s eta 0:00:04     |████████████████████████████▏   | 154.5 MB 5.8 MB/s eta 0:00:04     |████████████████████████████▌   | 156.4 MB 5.8 MB/s eta 0:00:04     |███████████████████████████████▋| 173.7 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py~=2.10.0 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.14.0-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-macosx_10_9_x86_64.whl (15.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.6 MB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.4\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 6.2 MB/s eta 0:00:01     |██████████████████████████████  | 10.0 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.32.0\n",
      "  Downloading grpcio-1.32.0-cp38-cp38-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (0.35.1)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 6.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 6.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (49.6.0.post20200814)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.10)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.0-py3-none-any.whl (12 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 6.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=4ef403cbd84b7dfdc3388226e37976f2eebdbe539e332563d7af594625aa4978\n",
      "  Stored in directory: /Users/apple/Library/Caches/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-macosx_10_9_x86_64.whl size=32624 sha256=ddd9f01e3ff4a9718fa86fe7265109c2b09d3dc579eacba1c08d6c0d1667a8f5\n",
      "  Stored in directory: /Users/apple/Library/Caches/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: protobuf, numpy, astunparse, termcolor, absl-py, markdown, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, grpcio, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, flatbuffers, wrapt, tensorflow-estimator, opt-einsum, gast, keras-preprocessing, google-pasta, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.2.0 flatbuffers-1.12 gast-0.3.3 google-auth-1.24.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.3 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 termcolor-1.1.0 wrapt-1.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    " pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "83Zunsc6G7vS"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# for the random seed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# set the random seeds to get reproducible results\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X, y = X[:1000], y[:1000]\n",
    "X = X.reshape(X.shape[0], 28, 28, 1)\n",
    "# Normalize\n",
    "X = X / 255.\n",
    "# number of unique classes\n",
    "num_classes = len(np.unique(y))\n",
    "y = y.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "\n",
    "num_tot = y.shape[0]\n",
    "num_train = y_train.shape[0]\n",
    "num_test = y_test.shape[0]\n",
    "\n",
    "y_oh = np.zeros((num_tot, num_classes))\n",
    "y_oh[range(num_tot), y] = 1\n",
    "\n",
    "y_oh_train = np.zeros((num_train, num_classes))\n",
    "y_oh_train[range(num_train), y_train] = 1\n",
    "\n",
    "y_oh_test = np.zeros((num_test, num_classes))\n",
    "y_oh_test[range(num_test), y_test] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBjvQ0ZnZ4iL",
    "outputId": "bee1cfe4-2274-4b9e-8586-e6ce8d4bdda0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMemOR9oG7vS"
   },
   "source": [
    "### Question 1\n",
    "**The data set**\n",
    "\n",
    "Plot a three examples from the data set.\n",
    "* What type of data are in the data set?\n",
    "\n",
    "    <span style=\"color:red\"> < **numpy.ndarray** > </span>\n",
    "    \n",
    "\n",
    "* What does the line ```X = X.reshape(X.shape[0], 28, 28, 1)``` do?\n",
    "\n",
    "  < make the X into 4D array which elements is 1000,28,28,1>\n",
    "\n",
    "Look at how the encoding of the targets (i.e. ```y```) is changed. E.g. the lines\n",
    "```\n",
    "    y_oh = np.zeros((num_tot, num_classes))\n",
    "    y_oh[range(num_tot), y] = 1\n",
    "```\n",
    "Print out a few rows of ```y``` next to ```y_oh```.\n",
    "* What is the relationship between ```y``` and ```y_oh```?\n",
    "\n",
    "    <span style=\"color:red\"> <  **when the columns index in y_oh equal to y, then the value =1， otherwise value in y_oh euqal to 0** > </span>\n",
    "    \n",
    "    \n",
    "* What is the type of encoding in ```y_oh``` called and why is it used?\n",
    "\n",
    "    <span style=\"color:red\"> <**one-hot encoding**> \n",
    "    </span>\n",
    "    <**Because it transform y into the value between 0 and 1, which is suitable for the  equally categorical y**>\n",
    "    \n",
    "* Plot three data examples in the same figure and set the correct label as title. \n",
    "    * It should be possible to see what the data represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "_5CQf_iasNn2",
    "outputId": "03a23157-b319-4aa3-abe2-784db7103add"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_plot, y_plot = X[:3], y[:3]\n",
    "X_plot=X_plot.reshape(3,28,28)\n",
    "fig=plt.figure(figsize=(10,10))\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.imshow(X_plot[0])\n",
    "plt.title(y_plot[0])\n",
    "\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.imshow(X_plot[1])\n",
    "plt.title(y_plot[1])\n",
    "\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.imshow(X_plot[2])\n",
    "plt.title(y_plot[2])\n",
    "\n",
    "plt.show()\n",
    "#for i range(1,4):\n",
    "#  img=X_plot[i]\n",
    "#  fig.add_subplot(1,3,i)\n",
    "#  plt.title(y[i])\n",
    "#  plt.imshow(img)\n",
    "#plt.show\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGsl5ZbNud5M"
   },
   "source": [
    "# Question 2\n",
    "\n",
    "\n",
    "To avoid the problem of overfitting, we need to artificially expand the data set. We can use existing data to generate 'fake data'. We add a small change to each picture on the built-in data:\n",
    "\n",
    "* Randomly rotate some training images by 10 degrees\n",
    "* Randomly Zoom by 10% some training images\n",
    "* Randomly shift images horizontally by 10% of the width\n",
    "* Randomly shift images vertically by 10% of the height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "D66xcoR_udeK"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range = 0.1, # Randomly zoom image \n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=False,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4MlblbwG7vS"
   },
   "source": [
    "### Question 3\n",
    "**The model**\n",
    "\n",
    "Below is some code for bulding and training a model with Keras.\n",
    "* What type of network is implemented below? I.e. a normal MLP, RNN, CNN, Logistic Regression...?\n",
    "\n",
    "    <span style=\"color:red\"> <***CNN***> </span>\n",
    "    \n",
    "    \n",
    "* What does ```Dropout()``` do?\n",
    "\n",
    "    <span style=\"color:red\"> <***controll overfitting***> </span>\n",
    "\n",
    "\n",
    "* Which type of activation function is used for the hidden layers?\n",
    "\n",
    "    <span style=\"color:red\"> <***Relu***> </span>\n",
    "\n",
    "\n",
    "* Which type of activation function is used for the output layer?\n",
    "\n",
    "    <span style=\"color:red\"> <***Softmax***> </span>\n",
    "\n",
    "\n",
    "* Why are two different activation functions used?\n",
    "\n",
    "    <span style=\"color:red\"> <***Different activation functions allow for different non-linearities which might work better for solving a specific function: ReLU is fast to compute so nice for deep networks with high training times; Softmax is used for multi-class classification problems and predict a multinomial probability distribution***> </span>\n",
    "\n",
    "\n",
    "* What optimizer is used in the model below?\n",
    "\n",
    "    <span style=\"color:red\"> <***Stochastic Gradient Descent (SGD)***> </span>\n",
    "\n",
    "\n",
    "* How often are the weights updated (i.e. after how many data examples)?\n",
    "\n",
    "    <span style=\"color:red\"> <**after the 32 samples, the weight will ungrades once**> </span>\n",
    "\n",
    "\n",
    "* What loss function is used?\n",
    "\n",
    "    <span style=\"color:red\"> <***Categorical_crossentropy***> </span>\n",
    "\n",
    "\n",
    "* How many parameters (i.e. weights and biases, NOT hyper-parameters) does the model have?\n",
    "\n",
    "    <span style=\"color:red\"> <***108,618***> </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNvAAvzjG7vS",
    "outputId": "07ef0ead-e830-46e5-9dda-a71ff87d60b0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 2.2322\n",
      "Epoch 2/60\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 1.3094\n",
      "Epoch 3/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.5476\n",
      "Epoch 4/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.3458\n",
      "Epoch 5/60\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.2481\n",
      "Epoch 6/60\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.1618\n",
      "Epoch 7/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.1702\n",
      "Epoch 8/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.1203\n",
      "Epoch 9/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0991\n",
      "Epoch 10/60\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.0544\n",
      "Epoch 11/60\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.0416\n",
      "Epoch 12/60\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.0405\n",
      "Epoch 13/60\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 0.0346\n",
      "Epoch 14/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0197\n",
      "Epoch 15/60\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.0155\n",
      "Epoch 16/60\n",
      "25/25 [==============================] - 1s 19ms/step - loss: 0.0179\n",
      "Epoch 17/60\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 0.0119\n",
      "Epoch 18/60\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0071\n",
      "Epoch 19/60\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0077\n",
      "Epoch 20/60\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0040\n",
      "Epoch 21/60\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0042\n",
      "Epoch 22/60\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 0.0032\n",
      "Epoch 23/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0035\n",
      "Epoch 24/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0023\n",
      "Epoch 25/60\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 0.0022\n",
      "Epoch 26/60\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.0022\n",
      "Epoch 27/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0021\n",
      "Epoch 28/60\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.0019\n",
      "Epoch 29/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0019\n",
      "Epoch 30/60\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 0.0015\n",
      "Epoch 31/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0018\n",
      "Epoch 32/60\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.0015\n",
      "Epoch 33/60\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 0.0013\n",
      "Epoch 34/60\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.0013\n",
      "Epoch 35/60\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 0.0012\n",
      "Epoch 36/60\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 0.0011\n",
      "Epoch 37/60\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0012\n",
      "Epoch 38/60\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 0.0011\n",
      "Epoch 39/60\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 0.0012\n",
      "Epoch 40/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0011\n",
      "Epoch 41/60\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 0.0010\n",
      "Epoch 42/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 9.5389e-04\n",
      "Epoch 43/60\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 8.7589e-04\n",
      "Epoch 44/60\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 7.5664e-04\n",
      "Epoch 45/60\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 7.8364e-04\n",
      "Epoch 46/60\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 8.6218e-04\n",
      "Epoch 47/60\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 7.4382e-04\n",
      "Epoch 48/60\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 5.9547e-04\n",
      "Epoch 49/60\n",
      "25/25 [==============================] - 0s 16ms/step - loss: 7.4040e-04\n",
      "Epoch 50/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 6.5876e-04\n",
      "Epoch 51/60\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 6.1763e-04\n",
      "Epoch 52/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 5.8019e-04\n",
      "Epoch 53/60\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 5.5300e-04\n",
      "Epoch 54/60\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 5.6150e-04\n",
      "Epoch 55/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 6.9891e-04\n",
      "Epoch 56/60\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 6.1614e-04\n",
      "Epoch 57/60\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 5.2971e-04\n",
      "Epoch 58/60\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 6.1559e-04\n",
      "Epoch 59/60\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 5.8387e-04\n",
      "Epoch 60/60\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 5.5806e-04\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4677\n",
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# Max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.))\n",
    "\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_oh_train, batch_size=32, epochs=60)\n",
    "\n",
    "# Evaluate performance\n",
    "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suvhACy89m-I",
    "outputId": "c8e22e2e-78d6-40c3-db8b-50e21c75af47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               102528    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 108,618\n",
      "Trainable params: 108,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64Abk3-GG7vS"
   },
   "source": [
    "## Part 3 - train a model\n",
    "\n",
    "A model's performance depends on many factors apart from the model architecture (e.g. type and number of layers) and the dataset. Here you will get to explore some of the factors that affect model performance. Much of the skill in training deep learning models lies in quickly finding good values/options for these choises.\n",
    "\n",
    "In order to observe the learning process it is best to compare the training set loss with the loss on the test set. How to visualize these variables with Keras is described under [Training history visualization](https://keras.io/visualization/#training-history-visualization) in the documentation.\n",
    "\n",
    "You will explore the effect of 1) optimizer, 2) training duration, and 3) dropout (see the question above).\n",
    "\n",
    "When training, an **epoch** is one pass through the full training set.\n",
    "\n",
    "\n",
    "\n",
    "## Further information\n",
    "For ideas about hyper-parameter tuning, take a look at the strategies described in the sklearn documentation under [model selection](https://scikit-learn.org/stable/model_selection.html), or in this [blog post](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html) from TensorFlow. For a more thorough discussion about optimizers see [this video](https://www.youtube.com/watch?v=DiNzQP7kK-s) discussing the article [Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers](https://arxiv.org/abs/2007.01547).\n",
    "\n",
    "\n",
    "**Good luck!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2swN1lCoWeu"
   },
   "source": [
    "### Question 1\n",
    "\n",
    "* **Vizualize the training**. Use the model above to observe the training process. Train it for 150 epochs and then plot both \"loss\" and \"val_loss\" (i.e. loss on the valiadtion set, here the terms \"validation set\" and \"test set\" are used interchangably, but this is not always true). What is the optimal number of epochs for minimizing the test set loss? \n",
    "    * Remember to first reset the weights (this can be done by calling ```model.compile()```), otherwise the training just continues from where it was stopped earlier.\n",
    "\n",
    "    <**Based on the graph showed from the first following code, the optimal number of epochs is around 15**>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ySbqfAV1B7DP"
   },
   "outputs": [],
   "source": [
    "def built_CNN(droupout):\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "  # Max pooling\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(droupout))\n",
    "  model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "  # Max pooling\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dropout(droupout))\n",
    "  model.add(Dense(10, activation='softmax'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Kzf0UkjDzWCW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "25/25 [==============================] - 2s 54ms/step - loss: 1.8953 - val_loss: 0.7826\n",
      "Epoch 2/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.6568 - val_loss: 0.6186\n",
      "Epoch 3/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.4365 - val_loss: 0.3979\n",
      "Epoch 4/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.3441 - val_loss: 0.3366\n",
      "Epoch 5/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.2509 - val_loss: 0.3243\n",
      "Epoch 6/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.1892 - val_loss: 0.2582\n",
      "Epoch 7/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.1602 - val_loss: 0.2807\n",
      "Epoch 8/150\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 0.1399 - val_loss: 0.2427\n",
      "Epoch 9/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.0934 - val_loss: 0.3118\n",
      "Epoch 10/150\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 0.0798 - val_loss: 0.2588\n",
      "Epoch 11/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.0485 - val_loss: 0.2371\n",
      "Epoch 12/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.0360 - val_loss: 0.3020\n",
      "Epoch 13/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0490 - val_loss: 0.2909\n",
      "Epoch 14/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.0221 - val_loss: 0.3879\n",
      "Epoch 15/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0207 - val_loss: 0.3378\n",
      "Epoch 16/150\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.0226 - val_loss: 0.3872\n",
      "Epoch 17/150\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 0.0172 - val_loss: 0.3649\n",
      "Epoch 18/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.0100 - val_loss: 0.4093\n",
      "Epoch 19/150\n",
      "25/25 [==============================] - 1s 40ms/step - loss: 0.0032 - val_loss: 0.4962\n",
      "Epoch 20/150\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 0.0178 - val_loss: 0.3783\n",
      "Epoch 21/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.0020 - val_loss: 0.3878\n",
      "Epoch 22/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 0.0017 - val_loss: 0.4075\n",
      "Epoch 23/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.0101 - val_loss: 0.4415\n",
      "Epoch 24/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0051 - val_loss: 0.4632\n",
      "Epoch 25/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 9.7377e-04 - val_loss: 0.4132\n",
      "Epoch 26/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 4.2378e-04 - val_loss: 0.4558\n",
      "Epoch 27/150\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 3.1871e-04 - val_loss: 0.4721\n",
      "Epoch 28/150\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 0.0142 - val_loss: 0.4786\n",
      "Epoch 29/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 3.4771e-04 - val_loss: 0.4965\n",
      "Epoch 30/150\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 1.3487e-04 - val_loss: 0.5176\n",
      "Epoch 31/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 2.7171e-04 - val_loss: 0.9493\n",
      "Epoch 32/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.0521 - val_loss: 0.5321\n",
      "Epoch 33/150\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 9.2626e-05 - val_loss: 0.5211\n",
      "Epoch 34/150\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 5.8497e-05 - val_loss: 0.5599\n",
      "Epoch 35/150\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 6.8575e-05 - val_loss: 0.5617\n",
      "Epoch 36/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0022 - val_loss: 0.7457\n",
      "Epoch 37/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0137 - val_loss: 0.5642\n",
      "Epoch 38/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 2.2032e-05 - val_loss: 0.5907\n",
      "Epoch 39/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 2.5783e-05 - val_loss: 0.5831\n",
      "Epoch 40/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 1.7770e-05 - val_loss: 0.6375\n",
      "Epoch 41/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.0029 - val_loss: 0.6517\n",
      "Epoch 42/150\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 2.8404e-05 - val_loss: 0.6366\n",
      "Epoch 43/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 1.8908e-05 - val_loss: 0.6247\n",
      "Epoch 44/150\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 0.0011 - val_loss: 0.6935\n",
      "Epoch 45/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 1.2097e-04 - val_loss: 0.6365\n",
      "Epoch 46/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 2.5407e-05 - val_loss: 0.6563\n",
      "Epoch 47/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 1.2257e-05 - val_loss: 0.6806\n",
      "Epoch 48/150\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 2.3047e-05 - val_loss: 1.0114\n",
      "Epoch 49/150\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 0.0132 - val_loss: 0.6948\n",
      "Epoch 50/150\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 1.3771e-05 - val_loss: 0.6859\n",
      "Epoch 51/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 6.6569e-06 - val_loss: 0.7091\n",
      "Epoch 52/150\n",
      "25/25 [==============================] - 0s 17ms/step - loss: 2.0074e-05 - val_loss: 1.0218\n",
      "Epoch 53/150\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 0.0030 - val_loss: 0.7992\n",
      "Epoch 54/150\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 2.4198e-05 - val_loss: 0.7977\n",
      "Epoch 55/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 5.8568e-06 - val_loss: 0.7926\n",
      "Epoch 56/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 2.3752e-06 - val_loss: 0.7948\n",
      "Epoch 57/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 1.6958e-06 - val_loss: 0.8068\n",
      "Epoch 58/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 1.7380e-06 - val_loss: 0.8454\n",
      "Epoch 59/150\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 0.0204 - val_loss: 0.8864\n",
      "Epoch 60/150\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 1.5471e-05 - val_loss: 0.8708\n",
      "Epoch 61/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 6.1760e-06 - val_loss: 0.8715\n",
      "Epoch 62/150\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 3.2474e-06 - val_loss: 0.8667\n",
      "Epoch 63/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 2.2883e-06 - val_loss: 0.8569\n",
      "Epoch 64/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 1.3623e-06 - val_loss: 0.8887\n",
      "Epoch 65/150\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 4.2805e-07 - val_loss: 0.8917\n",
      "Epoch 66/150\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 3.9318e-07 - val_loss: 0.9176\n",
      "Epoch 67/150\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 0.0021 - val_loss: 0.9189\n",
      "Epoch 68/150\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 7.6342e-05 - val_loss: 0.9215\n",
      "Epoch 69/150\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 2.1161e-06 - val_loss: 0.9136\n",
      "Epoch 70/150\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 8.7108e-07 - val_loss: 0.9045\n",
      "Epoch 71/150\n",
      "25/25 [==============================] - 1s 45ms/step - loss: 4.0674e-07 - val_loss: 0.9067\n",
      "Epoch 72/150\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 2.9937e-07 - val_loss: 0.9195\n",
      "Epoch 73/150\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 1.9173e-07 - val_loss: 0.9341\n",
      "Epoch 74/150\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 1.1596e-07 - val_loss: 0.9474\n",
      "Epoch 75/150\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 6.9283e-08 - val_loss: 0.9848\n",
      "Epoch 76/150\n",
      "25/25 [==============================] - 1s 43ms/step - loss: 2.2000e-08 - val_loss: 1.0364\n",
      "Epoch 77/150\n",
      "25/25 [==============================] - 1s 52ms/step - loss: 1.4543e-08 - val_loss: 1.0406\n",
      "Epoch 78/150\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 5.6385e-09 - val_loss: 1.0583\n",
      "Epoch 79/150\n",
      "25/25 [==============================] - 1s 30ms/step - loss: 6.1775e-09 - val_loss: 1.0650\n",
      "Epoch 80/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 1s 32ms/step - loss: 2.9873e-09 - val_loss: 1.0768\n",
      "Epoch 81/150\n",
      "25/25 [==============================] - 1s 45ms/step - loss: 1.8504e-09 - val_loss: 1.0816\n",
      "Epoch 82/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 2.6199e-09 - val_loss: 1.0936\n",
      "Epoch 83/150\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 1.1282e-09 - val_loss: 1.0906\n",
      "Epoch 84/150\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 2.0477e-09 - val_loss: 1.0958\n",
      "Epoch 85/150\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 1.3992e-09 - val_loss: 1.0965\n",
      "Epoch 86/150\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 7.6978e-10 - val_loss: 1.0961\n",
      "Epoch 87/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 1.4035e-09 - val_loss: 1.0960\n",
      "Epoch 88/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 1.4449e-09 - val_loss: 1.1025\n",
      "Epoch 89/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 1.0136e-09 - val_loss: 1.1000\n",
      "Epoch 90/150\n",
      "25/25 [==============================] - 1s 39ms/step - loss: 1.1206e-09 - val_loss: 1.1051e\n",
      "Epoch 91/150\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 4.2063e-10 - val_loss: 1.1058\n",
      "Epoch 92/150\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 8.3930e-10 - val_loss: 1.1088\n",
      "Epoch 93/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 7.5864e-10 - val_loss: 1.1138\n",
      "Epoch 94/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 4.3006e-10 - val_loss: 1.1123\n",
      "Epoch 95/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 5.1010e-10 - val_loss: 1.1152\n",
      "Epoch 96/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 3.9903e-10 - val_loss: 1.1162\n",
      "Epoch 97/150\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 7.3499e-10 - val_loss: 1.1234\n",
      "Epoch 98/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 3.7326e-10 - val_loss: 1.1227\n",
      "Epoch 99/150\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 4.9919e-10 - val_loss: 1.1289\n",
      "Epoch 100/150\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 4.1601e-10 - val_loss: 1.1288\n",
      "Epoch 101/150\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 3.0735e-10 - val_loss: 1.1341\n",
      "Epoch 102/150\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 1.4715e-10 - val_loss: 1.1345\n",
      "Epoch 103/150\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 7.6539e-10 - val_loss: 1.1351\n",
      "Epoch 104/150\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 4.0289e-10 - val_loss: 1.1381\n",
      "Epoch 105/150\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 7.2398e-10 - val_loss: 1.1433\n",
      "Epoch 106/150\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 1.0341e-10 - val_loss: 1.1441\n",
      "Epoch 107/150\n",
      "25/25 [==============================] - 1s 48ms/step - loss: 1.7799e-10 - val_loss: 1.1488\n",
      "Epoch 108/150\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 4.3694e-10 - val_loss: 1.1503\n",
      "Epoch 109/150\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 3.5991e-10 - val_loss: 1.1541\n",
      "Epoch 110/150\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 9.6832e-11 - val_loss: 1.1565\n",
      "Epoch 111/150\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 0.0000e+00 - val_loss: 1.1581\n",
      "Epoch 112/150\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 1.7150e-10 - val_loss: 1.1635\n",
      "Epoch 113/150\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 1.6982e-10 - val_loss: 1.1648\n",
      "Epoch 114/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 9.6832e-11 - val_loss: 1.1667\n",
      "Epoch 115/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 1.1462e-11 - val_loss: 1.1700\n",
      "Epoch 116/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 1.0341e-10 - val_loss: 1.1764\n",
      "Epoch 117/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 5.9663e-11 - val_loss: 1.1771\n",
      "Epoch 118/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 1.7432e-11 - val_loss: 1.1797\n",
      "Epoch 119/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 1.7432e-11 - val_loss: 1.1849\n",
      "Epoch 120/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - val_loss: 1.1869\n",
      "Epoch 121/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - val_loss: 1.1899\n",
      "Epoch 122/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - val_loss: 1.1923\n",
      "Epoch 123/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 9.6832e-11 - val_loss: 1.1964\n",
      "Epoch 124/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 1.5650e-10 - val_loss: 1.2019\n",
      "Epoch 125/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 1.1462e-11 - val_loss: 1.2053\n",
      "Epoch 126/150\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.0000e+00 - val_loss: 1.2081\n",
      "Epoch 127/150\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 8.6598e-11 - val_loss: 1.2132\n",
      "Epoch 128/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 1.6982e-10 - val_loss: 1.2159\n",
      "Epoch 129/150\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 2.3205e-10 - val_loss: 1.2227\n",
      "Epoch 130/150\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 1.4715e-10 - val_loss: 1.2260\n",
      "Epoch 131/150\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 5.9663e-11 - val_loss: 1.2302\n",
      "Epoch 132/150\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 1.5469e-10 - val_loss: 1.2328\n",
      "Epoch 133/150\n",
      "25/25 [==============================] - 1s 27ms/step - loss: 2.8238e-10 - val_loss: 1.2399\n",
      "Epoch 134/150\n",
      "25/25 [==============================] - 1s 29ms/step - loss: 0.0000e+00 - val_loss: 1.2418\n",
      "Epoch 135/150\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 1.1462e-11 - val_loss: 1.2455\n",
      "Epoch 136/150\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 2.4967e-10 - val_loss: 1.2527\n",
      "Epoch 137/150\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0000e+00 - val_loss: 1.2569\n",
      "Epoch 138/150\n",
      "25/25 [==============================] - 1s 39ms/step - loss: 0.0000e+00 - val_loss: 1.2621\n",
      "Epoch 139/150\n",
      "25/25 [==============================] - 1s 42ms/step - loss: 3.0175e-11 - val_loss: 1.2667\n",
      "Epoch 140/150\n",
      "25/25 [==============================] - 2s 62ms/step - loss: 1.7432e-11 - val_loss: 1.2754\n",
      "Epoch 141/150\n",
      "25/25 [==============================] - 2s 82ms/step - loss: 0.0000e+00 - val_loss: 1.2796\n",
      "Epoch 142/150\n",
      "25/25 [==============================] - 1s 39ms/step - loss: 2.3384e-10 - val_loss: 1.2800\n",
      "Epoch 143/150\n",
      "25/25 [==============================] - 1s 54ms/step - loss: 7.0274e-10 - val_loss: 1.2849\n",
      "Epoch 144/150\n",
      "25/25 [==============================] - 1s 43ms/step - loss: 2.8980e-10 - val_loss: 1.2908\n",
      "Epoch 145/150\n",
      "25/25 [==============================] - 2s 101ms/step - loss: 0.0000e+00 - val_loss: 1.2976\n",
      "Epoch 146/150\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 2.3662e-11 - val_loss: 1.3090\n",
      "Epoch 147/150\n",
      "25/25 [==============================] - 1s 40ms/step - loss: 3.6998e-11 - val_loss: 1.3097\n",
      "Epoch 148/150\n",
      "25/25 [==============================] - 1s 26ms/step - loss: 4.4646e-10 - val_loss: 1.3184\n",
      "Epoch 149/150\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 0.0000e+00 - val_loss: 1.3243\n",
      "Epoch 150/150\n",
      "25/25 [==============================] - 1s 28ms/step - loss: 2.3662e-11 - val_loss: 1.3292\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/lUlEQVR4nO3dd3hc1Zn48e87Rb0Xy0Vyt3FvCGNabNOM6SUJHcJCCPklhMCSNWw2QDbZhQ0pBAIhJHFCCyVUQwwYCMYQMLhgg3u3JTf13qac3x93Rh5JI1mWNZqR7/t5Hj2auffO3Feydd855z33HDHGoJRSyr4c0Q5AKaVUdGkiUEopm9NEoJRSNqeJQCmlbE4TgVJK2ZwmAqWUsjlNBEodhogMFxEjIq5uHPstEfm4L+JSqrdoIlDHFBHZJSItIpLTbvuawMV8eJRCO6KEolRf0kSgjkU7gSuDT0RkMpAYvXCUim2aCNSx6GngupDn1wNPhR4gIuki8pSIlIrIbhH5LxFxBPY5ReSXIlImIjuA88K89s8isl9E9orIz0XEeTQBi8hgEVkkIhUisk1Evh2yb6aIrBSRGhE5KCK/DmxPEJFnRKRcRKpEZIWI5B1NHMqeNBGoY9FyIE1Exgcu0JcDz7Q75hEgHRgJzMZKHDcE9n0bOB+YDhQCX2/32icBLzA6cMzZwE1HGfNzQDEwOHC+/xWRMwL7fgv81hiTBowCXgxsvz7wMxQA2cAtQONRxqFsSBOBOlYFWwVnAZuAvcEdIcnhbmNMrTFmF/Ar4NrAId8EHjLGFBljKoD7Q16bB8wHfmiMqTfGlAC/Aa7oaaAiUgCcCiwwxjQZY9YAfwqJxwOMFpEcY0ydMWZ5yPZsYLQxxmeMWWWMqelpHMq+NBGoY9XTwFXAt2jXLQTkAHHA7pBtu4EhgceDgaJ2+4KGAW5gf6A7pgr4AzDgKGIdDFQYY2o7iedGYCywKdD9c35g+9PAO8DzIrJPRH4hIu6jiEPZlCYCdUwyxuzGKhqfC7zSbncZ1qfpYSHbhnKo1bAfq7sldF9QEdAM5BhjMgJfacaYiUcR7j4gS0RSw8VjjNlqjLkSK9n8H/CSiCQbYzzGmJ8aYyYAJ2N1Z12HUkdIE4E6lt0InG6MqQ/daIzxYfWz/4+IpIrIMOAODtURXgR+ICL5IpIJ3BXy2v3AEuBXIpImIg4RGSUis48grvhAoTdBRBKwLvifAPcHtk0JxP4sgIhcIyK5xhg/UBV4D5+IzBWRyYGurhqs5OY7gjiUAjQRqGOYMWa7MWZlJ7tvBeqBHcDHwN+AhYF9f8TqclkLrKZji+I6rK6lDUAl8BIw6AhCq8Mq6ga/Tsca7jocq3XwKnCvMebdwPHnAOtFpA6rcHyFMaYJGBg4dw2wEfiQjkVxpQ5LdGEapZSyN20RKKWUzWkiUEopm9NEoJRSNqeJQCmlbK7fzYKYk5Njhg8fHu0wlFKqX1m1alWZMSY33L5+lwiGDx/OypWdjQhUSikVjojs7myfdg0ppZTNaSJQSimb00SglFI2p4lAKaVsThOBUkrZnCYCpZSyOU0ESillc/ZJBAc3wD9/DnWl0Y5EKaViin0SQdkWWPYg1GsiUEqpUPZJBI7ATdR+T3TjUEqpGGOfROAMrOnt90Y3DqWUijH2SQQOp/Xdp4lAKaVC2SgRaItAKaXCsVEi0BqBUkqFY59EoDUCpZQKyz6JQGsESikVlo0SgbYIlFIqHBslAq0RKKVUOBFLBCKyUERKRGTdYY47QUR8IvL1SMUChNQIfBE9jVJK9TeRbBH8FTinqwNExAn8H/BOBOOwtNYItEWglFKhIpYIjDHLgIrDHHYr8DJQEqk4WmmNQCmlwopajUBEhgCXAI9349ibRWSliKwsLe3hpHFaI1BKqbCiWSx+CFhgjDlsp70x5gljTKExpjA3N7dnZwvWCHT4qFJKteGK4rkLgedFBCAHOFdEvMaY1yJytmCNQLuGlFKqjaglAmPMiOBjEfkr8GbEkgCE1Ai0a0gppUJFLBGIyHPAHCBHRIqBewE3gDHmsHWBXtdaI9AWgVJKhYpYIjDGXHkEx34rUnG00hqBUkqFZZ87iyXwo2qLQCml2rBRIhCrTqA1AqWUasM+iQCsOoG2CJRS/VB1o4eK+paIvHc0h4/2PadbawRKqX6jusHDG1/u4531B/h0ezm3zB7FnfOO6/Xz2CsROJzaIlBKxbxdZfU89N4WFq87QIvXz8icZG46bSTzJw+MyPlslgi0RqCUij0en5831u7jQE0TRRWNvLSqCLfTwZUnFPCNwgImDUmP6Pltlgi0RqCUii1FFQ3c9vwXrN5TBUCc08El04dw59nHMSAtoU9isFcicLq0RqCUigk7y+p5dvluXlhRBMAjV07nrAl5xLscBKbe6TP2SgTaIlBKRVGTx8cHm0p49rM9fLytDJdDmDdxIAvOGcfQ7KSoxWWzRKA1AqVU36isb+Ff28v4sriaxhYfZXXNLNtSSn2Lj8HpCfz7WWO5/ISCPuv+6Yq9EoHTrUtVKqUian91Iw++s5lXv9iLMRDvcpAc7yIpzsmF04Ywb2Iep47OweWMndu47JUIHE5dqlIp1esaW3x8uKWUJesPsHjdfvx+uPGUEZw7ZRBThqTH1EU/HJslArfWCJRSvaKoooF/bSvjn5tKWLa1lCaPn/RENxdOHcytp4+hICt6ff5HymaJwKU1AqVUj/n9hvc3lfD7pdtah3sOSk/g8sICzp44kJkjsnDH+Kf/cOyVCLRGoJQ6AjtK61i5u5J9VY1s3F/Dil2VVNS3kJ+ZyI/PHc/ccbmMyk3p8+Gevc1eicDhhJbmaEehlIpRPr9hT0UDmw/U8srqYt7deBBjrMmLCzKTOH3cAGaPzWX+pIEx3+9/JGyWCLRGoJQ6pMXrZ9OBGtYUVfHJtnL+tb2M2ibrGpGe6ObWuaO5dEY+gzMSiXMdOxf+9myWCLRGoJTdGWMormzk+RV7+Ntne6hssK4Jg9MTOG/yIGYMy2RsXirjBqaS4HZGOdq+Eck1ixcC5wMlxphJYfZfDSwIPK0DvmuMWRupeABrigmtEShlOweqm3h3wwGWbDjI2qIqapq8iMBZ4/O4cNpgpg/NZHB6Qr/v6++pSLYI/gr8Dniqk/07gdnGmEoRmQ88AZwYwXisFoHeR6DUMc3j8/Pp9nK+2FPFV3ur+GpvNQdrrNrgyNxkzp86mHEDU5k9Npdh2clRjjY2RHLx+mUiMryL/Z+EPF0O5EcqllZaI1DqmOPzG3aU1vFlcTUrd1fy9rr9VDZ4EIGROcmcPCqHSUPS+dqYHMbkpUY73JgUKzWCG4G3OtspIjcDNwMMHTq052fRSeeU6tcq61vYU9HArvJ6viqu5su91azfW019i9XlmxTn5IzxeVw0dTCzRmWTEh8rl7jYFvXfkojMxUoEp3Z2jDHmCayuIwoLC02PT+bURKBUf+H3GyobWthWUseaoioWf7WftcXVrfvjXA4mDk7j68fnMzk/g6n56YzMTcHpsGc//9GIaiIQkSnAn4D5xpjyiJ9QawRKxSy/37B8ZznPLt/Dh1tKqWtu+6Ft8pB0fjTvOMbmpVKQlcio3JR+eRdvLIpaIhCRocArwLXGmC19clKtESgVdRX1LXxZXEVJTTMHapo4WNPEzrJ6viyupq7ZS3qimwumDiY3NZ70RDejcpMZNzCNgenRn675WBXJ4aPPAXOAHBEpBu4F3ADGmMeBe4Bs4LHAkC2vMaYwUvEAWiNQqg8ZY6hs8FBc2UBRRSPFlQ0s31HOR1vL8PoP9fBmJceRn5nIxdMHc8LwLOZNHGib8fuxIpKjhq48zP6bgJsidf6wtEagVET4/YZ1+6rZVlJHSW0z6/fVsHxHOaW1bad0GZKRyI2njWDucQMYkpHIgLR44l160Y+2qBeL+5TWCJTqFVsP1vL2ugMUVTZQ0+hl9Z5KSkIu+gNS4zl5VDaTh6QzNCuJ/Mwk8rMSSUtwRzFq1RmbJQI3GB+ts0gppcKqb/by0dYyKupbaPb6aPL4afT42FFax7q91ewqb0AE8lITSI53Ujg8kzPH5zGtIIMBaQkkxzlte5duf2SzRBD4cf1ea0pqpRRen5+lm0v5x1f7aWzx0eT1sXxHOU0ef4dj8zMTmTAojRtOGcH8SQNjYr1ddfTslQicmgiUKq5s4M8f72TRmn0AeP2G6kYP2clx5KTEIwLfLCzgvMmDGJ6TTLzLQYLbSZzTgUPH6B+T7JUIgi0CnwfcidGNRakI2V1ez1vrDvBlcRVZyXEkup1sOVjHtpI6KhtaaGjx4XII8yYNJCPRjddnmDtuAGeMH6Dj8m3KZokg0ArQkUOqn6pv9lJa20x+ZiKNHh//3FTC2qJqGlqs7ev31XCgpgmAoVlJ1DZ5qGv2MnpAKicMzyQ7JZ7c1HgunDqYwRn6YUhZbJYIAsPUNBGoGGGMNZ5eRDDGsG5vDTvL66lv9uLzG1wOwRn4+mJPFa+sLqa+xYfbKQhCi89PottJSoKLrKQ4ThqVzdT8dM6ckEd+ZlLrObRwq7pir0Tg1BaBih3r9lbzg+e/oKrBw7iBqeypaKC4srHT4+NcDi6YMpiZIzLZWdaA3xjOnpDHjKGZXfbdaxJQh2OvRBBaI1AqSsrqmnnti7384p3NZCfHcdb4PDYdqGHMgBR+cMYYphdkkJLgwukQ/H7w+v34/IbM5Dgdh68iwmaJQFsEKnr2Vzey4OWv+GhrKcbAaWNyeOjyaWSnxEc7NGVzNksEWiNQ0bF0cwl3vLiWZo+PW08fw9kT8pg4OE27bVRMsFciCNYItGtI9ZG3vtrPH5btYE1RFWPzUvj9NcczKjcl2mEp1Ya9EkHoncVKRdizn+3mx6+uY2ROMj+9cCLfLCwgMU4nWFOxx2aJQGsEqm98sKmEn7y2jrnH5fLH6wpx6Y1aKobZLBFojUBFjjGGZ5bv5s0v97NqdyUTB6fzu6tmaBJQMc9eiUBrBCqCVu6u5Cevr+e4vFRuOm0k3z5tBMm6eLrqB+z1v1RrBDHLGMOTn+zisuPzSe2nY+Xf23gQl0N46bsn9dufQdlTxNqsIrJQREpEZF0n+0VEHhaRbSLypYjMiFQsrbRGEFWV9S2d7ttWUsd9b2zg3Q0H+zCi3vX+xhJOHJmlSUD1O5HsvPwrcE4X++cDYwJfNwO/j2AsFq0RRM3yHeUU/s977KsKP4VCTZP1b1LZ0D+77XaX17OtpI4zxuVFOxSljljEEoExZhlQ0cUhFwFPGctyIENEBkUqHkBrBFFUXNmIz2/YX90Udn99s5UIqhs6bzXEsvc2lgBw5nhNBKr/ieZwhiFAUcjz4sC2DkTkZhFZKSIrS0tLe35GrRFETUOLt833zvZXNfbPJP3+xoOMGZDC0OykaIei1BGLZiIId2+9CXegMeYJY0yhMaYwNze352fUGkHU1Df7At/D/+7rAvur+2EiqG708PnOCs7Q1oDqp6KZCIqBgpDn+cC+iJ5RawRRE/zEH0wI7QUTRFU/rBEsWrMXr98wf9LAaIeiVI9EMxEsAq4LjB6aBVQbY/ZH9IxaI4ia1hZBJ11D9f20a8gYw9PLdzN5SDpT8tOjHY5SPRKx+whE5DlgDpAjIsXAvYAbwBjzOLAYOBfYBjQAN0QqllZaI4iaYIugrpOuof5aLP58ZwVbDtbxi8um6Eyiqt+KWCIwxlx5mP0G+F6kzh+W1giipr7FahE0dNo1ZG3vby2Cp5fvJi3BxQVTB0c7FKV6zF6ToNisRuDx+TnnoWV8sKkk2qHQ0NzNFkGjB78/7JiBmLPlYC1vrzvA14/XWUVV/2avRGCzGkFNo4dNB2rZsL8m2qG01gA6Gz4a3G8M1HaSLGLJv7aVcdnvPyEjKY4bThke7XCUOir2SgQ2qxE0ef0ANLaE747pSw0tweGj4WOpC9leHeMjh15fs5frF37OoPQEXvveyRRk6b0Dqn+zWSKwV42gyeNr8z2agl0/nY0aaghpBVQ19n3BuLbJw+o9lSzfUY5Vvgrv6eW7+eELazh+WCYvffdk8jM1Caj+z2azjzoAsU0iCLYEGmMgERxqEXR2Q5mXrOQ4Kupb+uxegje/3MevlmyhrK6Z2qZDcU0YlMYdZ43ljPEDWkcC+f2GX7+7hd99sI3Txw3gsatnkODWuoA6NtgrEYBVJ7BJjaDZG2wR+KMcSUiLoLNRQy1ehmQkWomgD0YOldY2c/fLXzEoI4HLZuSTmxrP2LxUqhpa+N0H27jpqZWcNiaH288aS0Ozj6c+3cWSDQe5vLCAn18yCbcuNqOOIfZLBA6XbVoEwQQQ7a4hY8yhFkGnXUM+JgxK4Ku91X1yL8EDb22iyevj8WuOZ2S7xeQvmT6EZz/bw6+WbObSxz4BwCFwz/kTuOGU4Xq/gDrm2DARuG2UCGKjRtDi8+MNDAntvFjsZXBGIhC5+YaMMRysaeajraW8vLqY784Z1SEJALicDq4/eTjnTxnER1vLyEtLYPSAFHJT4yMSl1LRZsNE4LRRIgiMGopyIgjeRBbndIStEXh9fpq9fjKT4kiKcx5xjeCv/9pJZnIcF00LO3ltq/vf2sQTy3YAMDw7iVtPH93l8dkp8Vw8vev3VOpYYL9EYKMaQWOMtAiC3UG5qfHsrbLWJXA6JGS/FV9yvIuMRPcR1QhKapr47zc34DfW4jbXzhoW9rjiygYWfryTcyYO5KbTRjBpSLoWe5UKsF8icLjBH/1RNH0hmAAao1wsDtYHgomgocXbZjnHYCshOc5JelLcEbUIXvliL34DM0dk8ZPX1vH+xoP4/Ia8tATOHJ/H7LG5JMY5efzD7YjAPRdMaO2CUkpZbJgInOC3R4sgmAiao90iaD7UIgArMYRNBIEWQXU37yMwxvD3lUUUDsvk6Rtn8pPX1vFlcTWJcU7Wrj/AS6uKGZAaz+1njeXFFcV8/fh8TQIxzuPxUFxcTFNT+JXs1OElJCSQn5+P2939tbPtlwic9ikWN3tjpEYQ0iIAqzAcuoRLsGsoJd5FeqKb7aV13XrfL4qq2F5azwOXjiTe5eQXX5/aus/j8/Pp9nLuf2sTd7/yFU6H8N3ZXdcEVPQVFxeTmprK8OE6OqsnjDGUl5dTXFzMiBEjuv06+yUCh8s2NYJYGTXU2iJICbQI2o0cCu5PinOSkeTu9qihv68sJsHt4LwpHZe6djsdfG1sLieNyuapT3cT53LoMpL9QFNTkyaBoyAiZGdnc6RL+towEdinRhArdxYHWwQD0g61CELVhXQNpSdZxWJjTJcXg4YWL2+u3ce5kwa16WZqz+10cOOp3f9kpKJPk8DR6cnvz363R9qpRhByZ3FX8+dEWuuooWCLoN1NZcHnKfEuMhLjaPH6D3s39Jtr91Pb7OWKmUMjELGyq6qqKh577LEev/6hhx6ioaEh7L45c+awcuXKHr93JNkvEdioRhB6MQ3WC6Ih2BU0IC0BCNcisPYnxVtdQ3D4ieee/XwPowekcMLwzN4OV9lYJBNBLItoIhCRc0Rks4hsE5G7wuxPF5E3RGStiKwXkb5ZrtJmNYL2j/tasEWQkxIHHOoqat3ffKhFkJ4YSARdDCFdv6+atUVVXDVzqHYjqF511113sX37dqZNm8aPfvQjAB588EFOOOEEpkyZwr333gtAfX095513HlOnTmXSpEm88MILPPzww+zbt4+5c+cyd+7cLs/z3HPPMXnyZCZNmsSCBQsA8Pl8fOtb32LSpElMnjyZ3/zmNwA8/PDDTJgwgSlTpnDFFVdE5OeO5JrFTuBR4CygGFghIouMMRtCDvsesMEYc4GI5AKbReRZY0zkJptxuGxTIwhtETR6fGREKY6GFh8JbgdpgYt8+7uLG5q9OAQS3U4yAsd0VTD+22d7iHM5uHSG3vV7LPvpG+vZsK93F1WaMDiNey+Y2On+Bx54gHXr1rFmzRoAlixZwtatW/n8888xxnDhhReybNkySktLGTx4MP/4xz8AqK6uJj09nV//+td88MEH5OTkdHqOffv2sWDBAlatWkVmZiZnn302r732GgUFBezdu5d169YBVuskGNPOnTuJj49v3dbbutUiEJFkEXEEHo8VkQtF5HCDVGcC24wxOwIX9ueBi9odY4BUsT7WpQAVQGT7bRwu29QIgrOPQnRnIK1v9pIc5yIpcCdv+/mG6pp9JMe5EBHSA11DO8vqOyyoY4zh2c928+LKIs6fMoiMpLi++QGUbS1ZsoQlS5Ywffp0ZsyYwaZNm9i6dSuTJ0/mvffeY8GCBXz00Uekp6d3+z1XrFjBnDlzyM3NxeVycfXVV7Ns2TJGjhzJjh07uPXWW3n77bdJS0sDYMqUKVx99dU888wzuFyR+eze3XddBpwmIpnA+8BK4HLg6i5eMwQoCnleDJzY7pjfAYuAfUAqcLkxpsMVS0RuBm4GGDr0KIuDNqoRhF5Io7lKWUOLj6R4Jy6ngwS3o8MMpPXNXpLirSQRvNfg7le+4u5XvmJqQQbzJw1EgBW7Knlv40Fmj83lnvMn9PWPofpYV5/c+4oxhrvvvpvvfOc7HfatWrWKxYsXc/fdd3P22Wdzzz33dPs9w8nMzGTt2rW88847PProo7z44ossXLiQf/zjHyxbtoxFixbxs5/9jPXr1/d6QuhujUCMMQ3ApcAjxphLgMP9JYbrvG3/G5gHrAEGA9OA34lIWocXGfOEMabQGFOYm5vbzZA7avL4MOIEnz0SQZPX1zqnT5M3ijWCQIsAIDnO1aFrqL7FS3K8tX9AagKv/r+TefDrU7jtjDH4/YYH3trE/W9tYvWeSm4/cywLv3WCtgZURKSmplJbW9v6fN68eSxcuJC6Ousmx71791JSUsK+fftISkrimmuu4c4772T16tVhXx/OiSeeyIcffkhZWRk+n4/nnnuO2bNnU1ZWht/v57LLLuNnP/sZq1evxu/3U1RUxNy5c/nFL35BVVVVayy9qbtpRUTkJKwWwI3dfG0xUBDyPB/rk3+oG4AHjJUit4nITmAc8Hk34+q219fs5bbn17BuopBikxZBk8dPRqKb8voWmqLdIoizPvEnx4dJBM1eUuIP/XeaPjST6UOt0UC3nzWWkpom4t3O1kKyUpGSnZ3NKaecwqRJk5g/fz4PPvggGzdu5KSTTgIgJSWFZ555hm3btvGjH/0Ih8OB2+3m97//PQA333wz8+fPZ9CgQXzwwQdhzzFo0CDuv/9+5s6dizGGc889l4suuoi1a9dyww034PdbnSL3338/Pp+Pa665hurqaowx3H777WRkZPT6z93dRPBD4G7gVWPMehEZCYT/KQ9ZAYwRkRHAXuAK4Kp2x+wBzgA+EpE84DhgRzdjOiLZyVaXQ7PPQYpNagRNHh8ZSYFEEMUWQUPIJ/6kOGfrlBJB9c2HEkU4wWGnSvWFv/3tb22e33bbbdx2221tto0aNYp58+Z1eO2tt97KrbfeGvZ9ly5d2vr4qquu4qqr2l4Op06d2tqyCPXxxx93N/Qe61YiMMZ8CHwIECgalxljfnCY13hF5PvAO4ATWBhIIrcE9j8O/Az4q4h8hdWVtMAYU9bjn6YLwbtaG332WbO4yeMnLy0BqKexJYr3EbT4Wvv+U8K0CKxFafRir1S0dCsRiMjfgFsAH7AKSBeRXxtjHuzqdcaYxcDidtseD3m8Dzj7SIPuieBdrY0+sU+NwOMjMzAKJ9r3EQRrBEnxrg5DQ0NbDEqpvtfdYvEEY0wNcDHWhX0ocG2kgoqE9EQ3bqfQ4LVTi8DXWlQ93HxDTR4fLRG6+7ih2dc6Kigl3hmmReAjKU4TgVLR0t1E4A7cN3Ax8LoxxkPHEUAxzeEQclLiqfeILe4j8AbWCe5ui+C6P3/Oz97c0OUxPdWmRRDnoiFssVhXC1MqWrr7MewPwC5gLbBMRIYBvXvLXx/ITY2nrglbtAiaAp/ugy2CwyWCrSW1bZaP7C0+v6HJ42/9xJ8S72oz15DPb2j0+LRrSKko6laLwBjzsDFmiDHmXGPZDXQ9mUYMGpAaT50HW9QIghf+1AQXDun6zmKvz09Vo4fy+uZejyM4s2hyvLP1e0OLr/WmmtCZR5VS0dHdKSbSReTXIrIy8PUrIDnCsfW63NR4aj3YomsoeCdxgstJotvZZY2gssGDMVBe1/tTPAUnmEsK6Rry+k3rbKjB6Sa0RqBiwdHMPnruuece0VxA9913H7/85S97dK7e1t0awUKgFvhm4KsG+EukgoqU3JR4alvA2KBrKDjPUEKckwS3s8uuoYp6KwFUNrTg8/du6efQesTBYrF1wQ8miPp2LQaloqmrRODzdd29unjx4ojc7NUXupsIRhlj7g1MILfDGPNTYGQkA4uE3NR4PMaJGD/4ozeuvi8Eu4ISXA4SDtMiKK+zuoT8BqoaerdV0LFFEJx4zkoAn2wvBw7NMaRUNLWfhnrp0qXMnTuXq666ismTJwNw8cUXc/zxxzNx4kSeeOKJ1tcOHz6csrIydu3axfjx4/n2t7/NxIkTOfvss2lsbOzyvGvWrGHWrFlMmTKFSy65hMrKSiD8FNQffvgh06ZNY9q0aUyfPv2wU1p0R3fb440icqox5mMAETkF6Poni0G5qfEcIPDJ0+8Fx7E7X02wBZDgdpLgdtDcRY2gvL6lzePslCO/KDd5fJTUNHdYF7i1RRDXtkVQ3eiBigYeWLyRU0fncNLI7CM+pzrGvXUXHPiqd99z4GSY/0Cnu9tPQ7106VI+//xz1q1b17oY/MKFC8nKyqKxsZETTjiByy67jOzstv9/t27dynPPPccf//hHvvnNb/Lyyy9zzTXXdHre6667jkceeYTZs2dzzz338NOf/pSHHnoo7BTUv/zlL3n00Uc55ZRTqKurIyHh6G/G7G6L4BbgURHZJSK7sGYN7TgdX4zLTU3A25oIju06QWuLwO0kMa57LQKAsrqeFYz/8OEOzn7oQ2qb2t8sFlx9zEoAowek4BC4fuHnfPuplYgID1w2WReYUTFr5syZrUkArE/pU6dOZdasWRQVFbF169YOrxkxYgTTpk0D4Pjjj2fXrl2dvn91dTVVVVXMnj0bgOuvv55ly5YB4aegPuWUU7jjjjt4+OGHqaqq6pWZSLs7xcRaYGpwZlBjTI2I/BD48qgj6EMDUuPxBnPfMV4naGxtEThIcHWvRgA9Lxh/uqOMJo+fT7eXc/bEga3b69q1CMbkpfL6907lv99cz4pdlTxw6WTyM5PCvqeyuS4+ufel5ORD42KWLl3Ke++9x6effkpSUhJz5syhqampw2vi4w+1qp1O52G7hjoTbgrqu+66i/POO4/Fixcza9Ys3nvvPcaNG9ej9w86oqUqjTE1gTuMAe44qjNHQU5KPN5g7jvGh5AGL/yJ3WgRlNW34HZan8jLe9Ai8Pr8rC2qBuDjbYemijLG8NKqYpLinOSlH2q+Ts5P58XvnMSyH83VxedVTDncNNLV1dVkZmaSlJTEpk2bWL58+VGfMz09nczMTD766CMAnn76aWbPnt3pFNTbt29n8uTJLFiwgMLCQjZt2nTUMRxNm6LfteUT45y4XIGpjI/xFkFojSDe5aSsi0/6FXUtDM1KYmdZfZvWQXdtOlBLo8dajvKjrYcSwaK1+/hwSyn3XjCBtIS2U0iLSId6glLR1n4a6vPOO6/N/nPOOYfHH3+cKVOmcNxxxzFr1qxeOe+TTz7JLbfcQkNDAyNHjuQvf/lLp1NQ/+QnP+GDDz7A6XQyYcIE5s+ff9TnP5pE0K+mmAhKTIiHZo79GkFgnH6820FiXNddQ+X1zeSmxlPd6KGsB4lg1W5rhMO1s4bxx492UlTRQHK8i5++sYFpBRlcd9LwHv0MSkVD+2mo58yZ0/o4Pj6et956K+zrgnWAnJyc1nWHAe68886wx993332tj6dNmxa2dRFuCupHHnmks9B7rMuuIRGpFZGaMF+1WKuK9TtJCYG+u2O8RdAcOmrI5ThMIrBGCmUnxx+2a+ipT3dx36L1bSaoW7W7kry0eC4/wVqHaNnWUu78+1pqmzw8cNnkiExdoZTqPV22CIwxqX0VSF9JSkyAamxTI0hwdWfUUAvZyXFUJMd1Wiw2xnD/W5t4Ypm1btCeigYeu3oGCW4nq/dUcvywTEblpjAoPYEH3tpEbZOX/75oIuMGdlh5VCkVY46oWHwsSEkMFC2P8RZBo8eHQ8DtlC7vLPb4/FQ3eshOjic7Ja7NPQWhfvPeVp5YtoPrThrGzy6exD83lXDjkyvYfKCW4spGZgzNREQ4dXQOtU1eLp0+hGtnDYvkj6iU6iW2m+AlJSkRgKbmZo7lNbGaPH4S3U5EgonAjzGmw3j9ysCFPysljpyG8F1DxhheWlnEnONy+emFExERktxO7n7lKy5+9F8AzBhmrTF87UnDcIhwX+A4pY5UuP+nqvuCEzoeCdu2CCrrGqIcSWQ1eXwkuK2x+wlu65+5OczCM8EWQE5yHNnJcdQ0eTssULO1pI591U3Mmziw9Q/0suPz+fstJ5GVHEdKvIuJg60uoCn5Gfzf16eQ2MUaxEp1JiEhgfLy8h5dzJSVBMrLy4/4buOItghE5Bzgt1hrFv/JGNPhDhERmQM8BLix1kKeHcmYUlKtskdNTTWDInmiKGvy+FsTQWLge2PLoeQQFKwJZCXHtU4tUVHfwsCQcf9LN5cAMOe43DavnVqQwds/PI3Keg/xLr3wq6OXn59PcXExpaWl0Q6l30pISCA/P/+IXhOxRCAiTuBR4CygGFghIouMMRtCjskAHgPOMcbsEZEBkYonKDHNupi11JYd5sj+rcnrIz7QEghe/Ju8HesEwTUIslPiqQqsJVxW19wuEZRyXF4qg9ITO7w+NcFNart7BJTqKbfb3WY6B9U3Itk1NBPYFpittAV4Hrio3TFXAa8YY/YAGGNKIhgPAEkZVq7x1h7bnziaWnwkuDq2CNoLtgiyA11D0HYSuvpmLyt2VTC7XWtAKXXsiGTX0BCgKOR5MXBiu2PGYq2HvBRIBX5rjHmq/RuJyM3AzQBDhx7dlASpWVYi8NVXHNX7xLomr6+1nz5YIwi3SllFfQtOh5Ce6G7tGiqva+bp5bspqmhgaFYSHp9hzlhNBEodqyKZCMKV/dtXgFzA8cAZQCLwqYgsN8ZsafMiY54AngAoLCw8qipSamo6TcYNDeVH8zYxz6oRtO0aCncvQXl9M5lJcTgcQnaK1SJYvqOcl1fvbV2kJjnOSeHwrD6KXCnV1yKZCIqBgpDn+cC+MMeUGWPqgXoRWQZMBbYQIU6HUCppOJsqI3WKmNDk8ZGRaPXdBxNBc7hEUNdCTiABpMa7iHM6eHFlMemJbp6+cSZ/X1lMQVYicS7bDTBTyjYimQhWAGNEZASwF7gCqyYQ6nXgdyLiAuKwuo5+E8GYAKh1pOJuqYr0aaIqdPhoYpctgpbWloCI1SrYX93Ev589lin5GUzJz+izmJVS0RGxRGCM8YrI94F3sIaPLjTGrBeRWwL7HzfGbBSRt7HWNfBjDTFd1/m79o4GZzpJnqpInyaqmjz+jqOGQmoECz/eyXsbD7Jub3Wb9QPyMxNJT3RzlU4PrZRtRPQ+AmPMYmBxu22Pt3v+IPBgJONor8mdQU7ztr48ZZ/rqkWwo7SO/35zA2MGpHDmhDyuO+nQVBCPXj0Dt8OBy6ldQUrZhe2mmABoicsgpaHm8Af2Y00eX2sCSIhztG4DeH5FES6H8Oy3T2RAats7ENs/V0od+2z5sc+XkEkqdeDvfEbO/q7J23HUUJPHR7PXx0urijlrQp5e9JVSgE1bBP6ELBwY/A1VOFKyox1Or/P4/Pj8ps0NZSLw0dYykuJcVNS3cKXWAJRSAbZsETiSrDHx9dURv5E5KhpDFqUBcDsd/Me8cXy0tZT/fPUrCrISOXV0TjRDVErFEFsmAmeqdRGsrzw2E8GO0noAkuMPNfi+O2cUL37nJCYMSuPWuWNw6KphSqkAW3YNxQcSQWP1sTffUEOLlzteXMOA1HjmTcxrs69weBaLbzstSpEppWKVLVsECemxMQNpY4uPA9VNvfqe976+np1l9Tx0xbTWuYOUUqortkwEyZmBGUjropsIfvPeFi743ce9tgjHk5/s4u+rivn+3NGcPEprAEqp7rFlIkhLy8JjnJj66E48t2ZPFaW1zVQ1eI76vd7dcJCfvrGesybk8cMzx/ZCdEopu7BljSA9KY4qUpDG6E1FbYxh0wHrprY9FQ1kBtYCOFK7yup56tPdPPvZbiYPSee3V0zDqYVgpdQRsGUiiHM5qCI1qjOQ7q9uoqbJC0BRZQNTCzKO+D1e+2Ivd7y4BocI504exD0XTCApzpb/pEqpo2DLriGAOkdaVGcgDbYGwGoRdOWjraWc9/BHVISsHPb2ugP8+9/XcuKIbD6563QevnI6OVocVkr1gG0TQYMrnYQozkC66UAtYC36UtRFIiitbeb2F9awfl8NS9YfAGDzgVp+8NwXTMlP50/XFzIgTaeKUEr1nG0TQbM7gyRf9Cae27S/liEZiYzJS6WoojHsMcYY/uOltdQ0eclJiePdDQcBeH7FHgD+dF1hm5vGlFKqJ2x7FfHEZ5DaUAPGgPR9cXXTgRrGD0olMc7F2qKqsMe8+sVePthcyn0XTGB3RQPPfraH6kYPb6zdxxnjB+h9AkqpXmHbFoEvIRMXPmju+1ZBs9fHjtJ6jhuYytCsRPZWNeL1tV1Y3hjD75duZ9zAVK4/eThnTcijxevnf/+xkbK6Fi6ZPqTP41ZKHZtsmwhMojXrqGno+yGk20vq8foN4wamUZCZhM9v2N/uDuOlm0vZWlLHzV8biYgwc3gW6YluXlhZRGaSmznHDejzuJVSx6aIJgIROUdENovINhG5q4vjThARn4h8PZLxhHImWzOQNkVhvqHNB61WyLiBqQzNSgLoUDB+YtkOBqUncMHUwQC4nA5OH2dd/M+fMlgXk1dK9ZqIXU1ExAk8CswHJgBXisiETo77P6y1jfuMpFtdKw1lu/rytABs3F9LnNPBiJxkCgKJIHQI6dqiKj7dUc6/nTICd8iSkRdMHYRD4BuF+X0es1Lq2BXJj5UzgW3GmB3GmBbgeeCiMMfdCrwM9Omc0JIzBr8RfAc29OVpMcbw3saDHD8sE5fTwaD0BJwOaU0Edc3W7KHZyXFcMbOgzWtPH5fH5z8+kyn5GX0as1Lq2BbJRDAEKAp5XhzY1kpEhgCXAG0WtG9PRG4WkZUisrK0tHe6crIyMthjBmAObuqV9+uu9ftq2FFaz4XTDnX5DMlIpKiyEWMMd764ll3lDTxy1XRSE9wdXq83jSmlelskE0G4MZntp9l8CFhgjOly8WBjzBPGmEJjTGFubm6vBDdpSDrbycdRvrlX3i/Uf776FfctWt/6/Hf/3ModL67BGMOitftwO4X5kwa27h+alcSm/TX88IU1vL3+AHedM05nD1VK9ZlI3kdQDIT2beQD+9odUwg8L9Y4/hzgXBHxGmNei2BcgLWMY03qKDLqXwGfB5wdP3331LsbDlLf7OWu+eNwOYQ/f7yTygYPU4ak88bafcwem0tG0qFJ5gqyEvl4Wxm7yuv5wRljuOm0Eb0Wi1JKHU4kE8EKYIyIjAD2AlcAV4UeYIxpveKJyF+BN/siCQQlDJ6Ie+uL1O3fTEr+pF55z+pGD6W1zQB8st1aLL6ywcOA1Hj++80N+A3cNX9cm9dcNG0ITR4/35s7itEDUnslDqWU6q6IdQ0ZY7zA97FGA20EXjTGrBeRW0Tklkid90gMHjsDgB3rV/bae24rqWt9/O6GgyxZf5A4l4MXvnMSaYluEt1OzprQdgnJWSOz+c3l0zQJKKWiIqJTTBhjFgOL220LWxg2xnwrkrGEc9zEGfjfFKp2fdlr77k9kAgmD0nn3Q0lJLgdnDo6hxE5yfz5+kLK6lp0qmilVEyx9V1JCUkplLgG4ijrvZFD20rriHM5+LdTh1NW10xxZWPrIvLHD8ti3sSBh3kHpZTqW7ZOBAD16WMY0LyLqoaWwx/cDdtK6hiZk8zp4/JwOQQROGN83uFfqJRSUWL7RJCUP4kRcoCfvLKa+mbvUb/ftpI6Rg1IIT3RzdxxAzhtTK6O/VdKxTTbJ4KBo6bhFh9bNqzl0sc+oayuucfv1eTxUVTZwOjcFAAeu3oGf7qusLdCVUqpiLB9IpC8iQA89DUHmw/W8vqa9rc6dN+O0nqMgdEDrETgdjp0cjilVMzTq9SA8RCXynjvBvIzE1m5K/y01H6/4RuPf8Lra/Z2+lbbS60RQ8FEoJRS/YGOY3Q4oeAE2PMZJwy/no+2lmGMQdqtWrattI4VuypxOoSLprVdFObvK4twOx3sLKvHITAiJ7kvfwKllDoqmggAhp4EH/wvJ09x8eoXzewub2B4u4v5qt2VAKzcVUlNk4e0wIRwLV4/P3l9HU0eP0lxTgqykkhwO/v8R1BKqZ7SriGAghMBw8lx2wFYEaZ7aPXuSkTA6zd8tKWsdfu6fdU0efytS0lOGJTWV1ErpVSv0EQAkF8I4mRwzVoyktys3FVJVUMLNz25orUlsGpPJV8bk0t6opsPNh9aOmHFTitp/O8lk/ngzjn8/OLembNIKaX6inYNAcQlw6ApSNFnFA6bz4rdFfzk9fW8t7GEZq+fR66czo7Sei6bkU9aopulm0vw+w0Oh/D5zgpG5iSTm6r3Ciil+idtEQQNPQn2rmLm0FR2lNbzxtp9jMxJ5qOtZbz6hTVSaMbQTE4fl0tZXQtf7a3G7zes3F3JCcOzohy8Ukr1nCaCoIITwdvI11Ksi/60ggyeuelEnA7hl+9sxukQphakM3vsAETgtTV72VJSS3WjhxNGaCJQSvVf2jUUNOwUEAdjaz7hB2dczjeOz2dwRiJnT8jjrXUHmDQkjaQ4F0lx8M3jC/jrJ7sor7PmJzpRE4FSqh/TFkFQSi4MPxXH+le548wxFGQlAXDNrGGA1S0UdM8FExiWlcSitfsYmJZAfmZiVEJWSqneoIkg1MRLoWI7HDi0PsFJI7P5wRljWhMCQHK8i99eMR2XQ5g5IqvDzWdKKdWfaCIINf5CECese6V1k8Mh3HHWWMbmtV09bGpBBi98ZxZ3nzuu/bsopVS/EtFEICLniMhmEdkmIneF2X+1iHwZ+PpERKZGMp7DSs6GUXNh/StgzGEPP35YFoPStVtIKdW/RSwRiIgTeBSYD0wArhSRCe0O2wnMNsZMAX4GPBGpeLpt4qVQtQf2rop2JEop1Sci2SKYCWwzxuwwxrQAzwMXhR5gjPnEGFMZeLocyI9gPN0z7jxwuGDD69GORCml+kQkE8EQoCjkeXFgW2duBN6KYDzdk5gBI74Gm97sVveQUkr1d5FMBOGG0oS9sorIXKxEsKCT/TeLyEoRWVlaWtqLIXZi/AVQsQNKNkb+XEopFWWRTATFQEHI83ygw/JfIjIF+BNwkTGmPNwbGWOeMMYUGmMKc3NzIxJsG8edBwhsfCPy51JKqSiLZCJYAYwRkREiEgdcASwKPUBEhgKvANcaY7ZEMJYjk5oHBTNhkyYCpdSxL2KJwBjjBb4PvANsBF40xqwXkVtE5JbAYfcA2cBjIrJGRFZGKp4jNu58OPAVVO6KdiRKKRVRYvpZQbSwsNCsXNkH+aJiBzw8HebcDXM63AKhlFL9ioisMsYUhtundxZ3JmskHHcufPooNIRf0F4ppY4Fmgi6MvfH0FwLnzwS7UiUUipiNBF0ZeAkmHQpfPY41JUc/nillOqHNBEczpz/BG8z/PPn0Y5EKaUiQhPB4eSMhhNvgdVPQtGKaEejlFK9ThNBd8y9G1IHw5u3g88b7WiUUqpXaSLojvhUOOd+OPgVvHozVO+NdkRKKdVrNBF014SL4Gs/sqadeOR42LDo8K9RSql+QBNBd4nA6f8F318BmcPgg//R2UmVUscETQRHKnM4nPQ9KN0Ee5ZHOxqllDpqmgh6YtJlEJ8GKxdGOxKllDpqmgh6Ii4ZplxurWKm008opfo5TQQ9VXgD+Jph9VPRjkQppY6KJoKeypsIo8+ED39hzVTq88Di/4DP/hDtyJRS6oi4oh1Av3bBb+Gxk+HVWyAlDzYuAnFCwYkweFq0o1NKqW7RFsHRSM+H834JRZ9ZSWDuf0FyDiy6Ve9AVkr1G9oiOFqTvwEVOyF9CEy/BnLHwovXwbv3wBn3gMMFG14DVwKMPz/a0SqlVAeaCI6WCMxZcOj5hIushLD80UACiLdqCOKAq16EMWdZI42aqiFrhPUaY6wagysu/Dk2vA4f3A+XP2NNgheO3w+OLhp4niYrFpEe/ZhKqWNXRLuGROQcEdksIttEpMN6j2J5OLD/SxGZEcl4+sxFj8L1b0LmCEjOhW88aRWXX/o3q7j88DR4dCZ88SzUHoSnL4FfjYXiVR3fa+9qeOVmKN0Ir90Cfl/HY7a8A78YDiv+HD6encvgwdHw+vc7vxvaGCuZ9DcNFfDH0+Hlb/e8O87TBFV7ejcupfqRiK1ZLCJOYAtwFlAMrACuNMZsCDnmXOBW4FzgROC3xpgTu3rfPluzuLdVFcEf50J9KYyYbW3b+SHEpYLfC0lZ1mpo3/grpA6ChjKry2np/eBwW3czv70AzrgXTrvj0Pvu+hieucx67G2GS5+AKd+0nvu8sOUtePkmcCdBYwWcejuced+h1xsD61+BJfeApwFGnAajz4Jx51kxhfL7rDuqW+ph4GRwJ1rbm+usOZgqdljdXwOndN7yqCqCsi1WYkwd2HG/pwkqd1r7EjO7/p021cBTF8KBr6zf4dQr4aLHum4ZtVe8El79jhX7KT+01qd2xXf/9Ur1E12tWRzJRHAScJ8xZl7g+d0Axpj7Q475A7DUGPNc4PlmYI4xZn9n79tvEwFA6Rao3gOjzrAuqu/dC8UrrNFH8Wnw5AVQsb3ta5Ky4bpF1oXz79dbk93Fp4HTbX01VFjTXlz7ijV6afcnkFFgXeBr9oHfA4OmwTWvwD9/Bqv+Yq3HLA5ArORRvce6eOdNsloPNcVWbSNrpHVMUO1+aK6xHjvc1pxL4rBmY/XUHzouLR/cCR1//uY6qDtw6Hl6gZVMgv8HfS1QXQQm0DJJHWzN/NqZpipoKLe6zPZ/CUv/FzKGWfWY7irfCmlDoGAmrHvZGv2VkNH91yuLdjn2jenXwsnf79FLu0oEkawRDAGKQp4XY33qP9wxQ4A2iUBEbgZuBhg6dGivB9pncsdaXwBOF8z7n7b7b1wCW5dYF8fETOsCn5ZvHQtwwcMwYAI0VlkXeF8LOOOtFkLaYLjyOauWUF9qHZ8+BHKOg/EXQHwKnPcr60JXvrVtF9Fpd8CM68DhtLbvXwPrX4XK3W3jG36qdcGMS7ESWFVg/4ivWdNu5BwHG16F3Z8CYT5gOOOtYbW5x8GBddZ5/MHuHLGSypTLIWeMlcRKN4Gnsevf6bSrYOw8GHsOJKTDnk+7Pr69sfPga3dar51yOXz5QvjuN9UFnXyxz6QMiMjbRrJF8A1gnjHmpsDza4GZxphbQ475B3C/MebjwPP3gf8wxoTpLLf06xaBUkpFSVctgkgWi4uBgpDn+cC+HhyjlFIqgiKZCFYAY0RkhIjEAVcA7VdzWQRcFxg9NAuo7qo+oJRSqvdFrEZgjPGKyPeBdwAnsNAYs15EbgnsfxxYjDViaBvQANwQqXiUUkqFF9Ebyowxi7Eu9qHbHg95bIDvRTIGpZRSXdO5hpRSyuY0ESillM1pIlBKKZvTRKCUUjYXsRvKIkVESoHdhz0wvBygrBfDiQSNsXdojL1DYzx6sRLfMGNMbrgd/S4RHA0RWdnZnXWxQmPsHRpj79AYj16sxwfaNaSUUraniUAppWzObongiWgH0A0aY+/QGHuHxnj0Yj0+e9UIlFJKdWS3FoFSSql2NBEopZTN2SYRiMg5IrJZRLaJyF3RjgdARApE5AMR2Sgi60XktsD2LBF5V0S2Br4fZvHeiMfpFJEvROTNGI0vQ0ReEpFNgd/lSTEY4+2Bf+N1IvKciCREO0YRWSgiJSKyLmRbpzGJyN2Bv5/NIjIvijE+GPi3/lJEXhWRjFiLMWTfnSJiRCQnmjEeji0SgYg4gUeB+cAE4EoRmRDdqADwAv9ujBkPzAK+F4jrLuB9Y8wY4P3A82i6DdgY8jzW4vst8LYxZhwwFSvWmIlRRIYAPwAKjTGTsKZlvyIGYvwrcE67bWFjCvy/vAKYGHjNY4G/q2jE+C4wyRgzBdgC3B2DMSIiBcBZwJ6QbdGKsUu2SATATGCbMWaHMaYFeB64KMoxYYzZb4xZHXhci3UBG4IV25OBw54ELo5KgICI5APnAX8K2RxL8aUBXwP+DGCMaTHGVBFDMQa4gEQRcQFJWCvxRTVGY8wyoKLd5s5iugh43hjTbIzZibWGyMxoxGiMWWKMCS52vRxrZcOYijHgN8B/0HZR56jEeDh2SQRDgKKQ58WBbTFDRIYD04HPgLzgSm2B75FZsbp7HsL6z+wP2RZL8Y0ESoG/BLqv/iQiybEUozFmL/BLrE+G+7FW4lsSSzGG6CymWP0b+jfgrcDjmIlRRC4E9hpj1rbbFTMxhrJLIpAw22Jm3KyIpAAvAz80xtREO54gETkfKDHGrIp2LF1wATOA3xtjpgP1RL+rqo1AP/tFwAhgMJAsItdEN6ojFnN/QyLyY6zu1WeDm8Ic1ucxikgS8GPgnnC7w2yL+rXILomgGCgIeZ6P1TSPOhFxYyWBZ40xrwQ2HxSRQYH9g4CSKIV3CnChiOzC6k47XUSeiaH4wPq3LTbGfBZ4/hJWYoilGM8EdhpjSo0xHuAV4OQYizGos5hi6m9IRK4HzgeuNoduhoqVGEdhJf21gb+dfGC1iAwkdmJswy6JYAUwRkRGiEgcVrFmUZRjQkQEq297ozHm1yG7FgHXBx5fD7ze17EBGGPuNsbkG2OGY/3O/mmMuSZW4gMwxhwAikTkuMCmM4ANxFCMWF1Cs0QkKfBvfgZWPSiWYgzqLKZFwBUiEi8iI4AxwOdRiA8ROQdYAFxojGkI2RUTMRpjvjLGDDDGDA/87RQDMwL/V2Mixg6MMbb4As7FGmGwHfhxtOMJxHQqVrPwS2BN4OtcIBtrxMbWwPesGIh1DvBm4HFMxQdMA1YGfo+vAZkxGONPgU3AOuBpID7aMQLPYdUsPFgXqxu7igmru2M7sBmYH8UYt2H1swf/Zh6PtRjb7d8F5EQzxsN96RQTSillc3bpGlJKKdUJTQRKKWVzmgiUUsrmNBEopZTNaSJQSimb00SgVDsi4hORNSFfvXansogMDzdLpVLR5Ip2AErFoEZjzLRoB6FUX9EWgVLdJCK7ROT/ROTzwNfowPZhIvJ+YH7890VkaGB7XmC+/LWBr5MDb+UUkT8G1idYIiKJUfuhlEITgVLhJLbrGro8ZF+NMWYm8DusmVkJPH7KWPPjPws8HNj+MPChMWYq1vxH6wPbxwCPGmMmAlXAZRH9aZQ6DL2zWKl2RKTOGJMSZvsu4HRjzI7AZIEHjDHZIlIGDDLGeALb9xtjckSkFMg3xjSHvMdw4F1jLfyCiCwA3MaYn/fBj6ZUWNoiUOrImE4ed3ZMOM0hj31orU5FmSYCpY7M5SHfPw08/gRrdlaAq4GPA4/fB74Lres+p/VVkEodCf0kolRHiSKyJuT528aY4BDSeBH5DOtD1JWBbT8AForIj7BWS7shsP024AkRuRHrk/93sWapVCqmaI1AqW4K1AgKjTFl0Y5Fqd6kXUNKKWVz2iJQSimb0xaBUkrZnCYCpZSyOU0ESillc5oIlFLK5jQRKKWUzf1/LZxyoVokc+wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Question: Vizualize the training\n",
    "model=built_CNN(0)\n",
    "model.compile(loss='categorical_crossentropy')\n",
    "history=model.fit(X_train, y_oh_train, batch_size=32, epochs=150,validation_data=(X_test,y_oh_test))\n",
    "\n",
    "plt.plot(history.history['val_loss'],label='test loss')\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#the best epoch is 15, the maximum epoch can be up to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "R0BNHsd2wDfY",
    "outputId": "2ad8ddcb-0d88-4943-dd34-e92cc9badf82"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at confusion matrix \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(y_oh_test,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "2dMZHr9JB46t",
    "outputId": "c9009261-6c48-4109-a502-9348feb08529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_test[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    fig.set_size_inches(10, 8)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)), cmap='gray')\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pMgm-tzofyD"
   },
   "source": [
    "# Question2\n",
    "\n",
    "* **Optimizer**. Select three different optimizers and for each find the close-to-optimal hyper-parameter(s). In your answer, include a) your three choises, b) best hyper-parameters for each of the three optimizers and, c) the code that produced the results.\n",
    "    * *NOTE* that how long the training takes varies with optimizer. I.e., make sure that the model is trained for long enough to reach optimal performance.\n",
    "\n",
    "    <**a) the three choices: SGD, Adam, RMSProp \n",
    "    b) hyper-parameters for each of the three optimizers can be shown from each search and the result as well.**>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbKm2KJhD6fS",
    "outputId": "27a49f0a-c8d4-4686-bc1d-66e5665bc1e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras-tuner in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (0.23.2)\n",
      "Requirement already satisfied, skipping upgrade: terminaltables in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (4.49.0)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: tabulate in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: future in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from packaging->keras-tuner) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from requests->keras-tuner) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from requests->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from requests->keras-tuner) (1.25.10)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/apple/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->keras-tuner) (0.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiNDuK1fV0aE",
    "outputId": "357a6a66-ff4a-44a4-d5d9-cca6ffb76d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 85 Complete [00h 00m 12s]\n",
      "val_accuracy: 0.2800000011920929\n",
      "\n",
      "Best val_accuracy So Far: 0.9449999928474426\n",
      "Total elapsed time: 00h 05m 51s\n",
      "\n",
      "Search: Running Trial #86\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "learning_rate     |0.02182           |0.17306           \n",
      "tuner/epochs      |30                |10                \n",
      "tuner/initial_e...|0                 |0                 \n",
      "tuner/bracket     |0                 |1                 \n",
      "tuner/round       |0                 |0                 \n",
      "\n",
      "Epoch 1/30\n",
      "25/25 [==============================] - 2s 39ms/step - loss: 2.2648 - accuracy: 0.1675 - val_loss: 2.2046 - val_accuracy: 0.2400\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 2.1374 - accuracy: 0.3992 - val_loss: 2.0001 - val_accuracy: 0.4650\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 1.8406 - accuracy: 0.5673 - val_loss: 1.4332 - val_accuracy: 0.6750\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 1.2529 - accuracy: 0.6961 - val_loss: 1.0308 - val_accuracy: 0.7050\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.8411 - accuracy: 0.7721 - val_loss: 0.8989 - val_accuracy: 0.6850\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 1s 20ms/step - loss: 0.8034 - accuracy: 0.7410 - val_loss: 0.6944 - val_accuracy: 0.7800\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.6194 - accuracy: 0.8109 - val_loss: 0.6424 - val_accuracy: 0.8100\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.6163 - accuracy: 0.8105 - val_loss: 0.6026 - val_accuracy: 0.7700\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.5118 - accuracy: 0.8274 - val_loss: 0.5591 - val_accuracy: 0.8300\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 20ms/step - loss: 0.4223 - accuracy: 0.8701 - val_loss: 0.5742 - val_accuracy: 0.7950\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.3570 - accuracy: 0.8988 - val_loss: 0.4668 - val_accuracy: 0.8350\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.3621 - accuracy: 0.8898 - val_loss: 0.4566 - val_accuracy: 0.8550\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 0.3363 - accuracy: 0.9155 - val_loss: 0.4086 - val_accuracy: 0.8800\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 1s 23ms/step - loss: 0.3217 - accuracy: 0.9081 - val_loss: 0.4349 - val_accuracy: 0.8750\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 1s 25ms/step - loss: 0.2802 - accuracy: 0.9092 - val_loss: 0.3861 - val_accuracy: 0.8750\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 0.2233 - accuracy: 0.9387 - val_loss: 0.4481 - val_accuracy: 0.8800\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 1s 22ms/step - loss: 0.2391 - accuracy: 0.9165 - val_loss: 0.4511 - val_accuracy: 0.8300\n",
      "Epoch 18/30\n",
      "11/25 [============>.................] - ETA: 0s - loss: 0.2118 - accuracy: 0.9360"
     ]
    }
   ],
   "source": [
    "#!pip install -U keras-tuner\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "# Question: three optiomizer: SGD,Adam,RMSProp\n",
    "# the hyper-parameter is based on page13 of the article \n",
    "# Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers.\n",
    "\n",
    "#for SGD optimizer\n",
    "def build_model(hp):\n",
    "  model=built_CNN(0)\n",
    "  model.compile(optimizer=tf.keras.optimizers.SGD(hp.Float('learning_rate', min_value=1e-4, max_value=1,sampling='log')),\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "tuner = kt.Hyperband(build_model,objective='val_accuracy',max_epochs=30,project_name='SGD-1')\n",
    "tuner.search(X_train, y_oh_train,epochs=15,validation_data=(X_test,y_oh_test))\n",
    "tuner.results_summary()\n",
    "# the accur is 0.949999988079071\n",
    "#and the learning rate 0.18379900346409866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLqwRC8XoL5e"
   },
   "outputs": [],
   "source": [
    "# for Adam optimizer\n",
    "def build_model(hp):\n",
    "  model=built_CNN(0)\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1,sampling='log'),\n",
    "                                                   hp.Float('beta1', min_value=0.5, max_value=0.999,sampling='log'),\n",
    "                                                   hp.Float('beta2', min_value=0.8, max_value=0.999,sampling='log')),\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "tuner = kt.Hyperband(build_model,objective='val_accuracy',max_epochs=30,project_name='Adam')\n",
    "tuner.search(X_train, y_oh_train,epochs=15,validation_data=(X_test,y_oh_test))\n",
    "tuner.results_summary()\n",
    "#accuracy: 0.9599999785423279\n",
    "#learning_rate: 0.00880194573492123\n",
    "#beta1: 0.8059667512580438\n",
    "#beta2: 0.818272724625341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDB0Y44Hu_Jo"
   },
   "outputs": [],
   "source": [
    "#for RMS prop optimizer\n",
    "def build_model(hp):\n",
    "  model=built_CNN(0)\n",
    "  model.compile(optimizer=tf.keras.optimizers.RMSprop(hp.Float('learning_rate', min_value=1e-4, max_value=1, sampling='log'),\n",
    "                                                      hp.Float('rho', min_value=1e-3, max_value=1, sampling='log')),\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "tuner = kt.Hyperband(build_model,objective='val_accuracy',max_epochs=30,project_name='RMSprop')\n",
    "tuner.search(X_train, y_oh_train,epochs=15,validation_data=(X_test,y_oh_test))\n",
    "tuner.results_summary()\n",
    "# the val_accuracy is 0.9649999737739563\n",
    "# learning_rate: 0.017378972480895958\n",
    "# rho: 0.011239435767055977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNN7wJ7-o028"
   },
   "source": [
    "# Question3\n",
    "\n",
    "* **Dropout**. Use the best optimizer and do hyper-parameter seach and find the best value for ```Dropout()```.\n",
    "\n",
    "    <**the best optimizer is RMS Prop and the learning rate is 0.0174, the rho is 0.01124**>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9k-AWiyLRUw"
   },
   "outputs": [],
   "source": [
    "#Question: find the best value for Dropout()\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def CNN(dropout):\n",
    "  model=built_CNN(droupout=dropout)\n",
    "#Compile the model\n",
    "  RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.017378972480895958,rho=0.011239435767055977)\n",
    "  model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=[\"accuracy\"])\n",
    "# Train the model\n",
    "  return model\n",
    "\n",
    "classifier = KerasClassifier(build_fn= CNN)\n",
    "parameters = {'dropout':[0,0.25,0.5,0.75,0.9]}\n",
    "grid_search = GridSearchCV(estimator = classifier,param_grid = parameters)\n",
    "grid_search = grid_search.fit(X_train, y_oh_train,epochs=15,validation_data = (X_test,y_oh_test))\n",
    "best_parameters = grid_search.best_params_\n",
    "print(best_parameters)\n",
    "# best parameters is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "7u8oLraPy30E",
    "outputId": "9b1ee09c-5b4e-4bf9-e2a0-8d9473688c1d"
   },
   "outputs": [],
   "source": [
    "# the result for douput search\n",
    "import pandas as pd\n",
    "pd.concat([pd.DataFrame(grid_search.cv_results_[\"params\"]),pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBo5zdUHpIEB"
   },
   "source": [
    "# Question4\n",
    "\n",
    "* **Best model**. Combine the what you learned from the above three questions to build the best model. How much better is it than the worst and average models?\n",
    "\n",
    "    <span style=\"color:red\"> <**the accuracy in the best model is 0.915, the worst model is 0.59, the average one is 0.755**> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQY-cE_S5doI",
    "outputId": "41e48ef9-6d8a-4b1f-ebaf-b5d6a45e168c"
   },
   "outputs": [],
   "source": [
    "# Question: Best model: compare the best model with worst model, average model\n",
    "#for the best model：depout=0.5,epochs=15,optimizer=RMSprop(learning_rate: 0.01737897,rho=rho=0.011239435767055977,accur_score=0.96)\n",
    "model=built_CNN(0.5)\n",
    "RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.017378972480895958,rho=0.011239435767055977)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop)\n",
    "model.fit(X_train, y_oh_train, batch_size=32, epochs=15)\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])\n",
    "#accur: 0.915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rp6CM2LVG7vT",
    "outputId": "d4d86d85-b107-4507-af03-aa13946b7442"
   },
   "outputs": [],
   "source": [
    "# for the average model: drouput=0.75,epochs=43,optimizer=adam(learning_rate: 0.0065,beta1=0.5763,beta2=0.009668,accur_score=0.86), from previour running of three optimizer models\n",
    "model=built_CNN(0.75)\n",
    "adam=tf.keras.optimizers.Adam(learning_rate=0.0065,beta_1=0.05763, beta_2=0.09668)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam)\n",
    "model.fit(X_train, y_oh_train, batch_size=32, epochs=23)\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])\n",
    "#accur: 0.755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3cLRjEcuarp",
    "outputId": "a4f5c4b6-3a12-4c2c-8a1d-202204793223"
   },
   "outputs": [],
   "source": [
    "# for the worst model: drouput=0.9 epochs=27 optimizer=SGD(learning_rate: 0.031045,accur_score=0.1), from previour running of three optimizer models\n",
    "model=built_CNN(0.9)\n",
    "sgd=tf.keras.optimizers.SGD(lr=0.031045)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "model.fit(X_train, y_oh_train, batch_size=32, epochs=27)\n",
    "predictions = model.predict(X_test, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])\n",
    "# accur=0.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RX1FzCSpZS9"
   },
   "source": [
    "# Question 4\n",
    "\n",
    "* **Results on the test set**. When doing this search for good model configuration/hyper-parameter values, the data set was split into *two* parts: a training set and a test set (the term \"validation\" was used interchangably wiht \"test\"). For your final model, is the performance (i.e. accuracy) on the test set representative for the performance one would expect on a previously unseen data set (drawn from the same distribution)? Why?\n",
    "\n",
    "    <span style=\"color:red\"> <**the unseen data, drawn from the same distribution, has the similar feature with validation. Thus, the performance for the best model is representative for unseen data,as we can see from the data in between 1000 and 1100 from data code**> </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRnsCvzHTpOS"
   },
   "outputs": [],
   "source": [
    "#Question:Results on the test set--get the new data\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X, y = X[1000:1100], y[1000:1100]\n",
    "X = X.reshape(X.shape[0], 28, 28, 1)\n",
    "# Normalize\n",
    "X = X / 255.\n",
    "# number of unique classes\n",
    "num_classes = len(np.unique(y))\n",
    "y = y.astype(int)\n",
    "num_tot = y.shape[0]\n",
    "y_oh = np.zeros((num_tot, num_classes))\n",
    "y_oh[range(num_tot), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXlAZOiCTe_D",
    "outputId": "be2227ec-5450-47da-e829-eda8b2e4140a"
   },
   "outputs": [],
   "source": [
    "# for the best model\n",
    "model=built_CNN(0.5)\n",
    "RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.017378972480895958,rho=0.011239435767055977)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop)\n",
    "model.fit(X_train, y_oh_train, batch_size=32, epochs=15)\n",
    "predictions = model.predict(X, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print('Accuracy:', (predictions == y).sum() / predictions.shape[0])\n",
    "#accur: 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spvDPJtiT_o8",
    "outputId": "d64519bc-d6d7-4f51-ad03-c9e0a1e16cff"
   },
   "outputs": [],
   "source": [
    "# for the worst model: drouput=0.9 epochs=27 optimizer=SGD(learning_rate: 0.031045,accur_score=0.1), from previour running of three optimizer models\n",
    "model=built_CNN(0.9)\n",
    "sgd=tf.keras.optimizers.SGD(lr=0.031045)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
    "model.fit(X_train, y_oh_train, batch_size=32, epochs=27)\n",
    "predictions = model.predict(X, batch_size=32)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "print('Accuracy:', (predictions == y).sum() / predictions.shape[0])\n",
    "# accur=0.66"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_model1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
