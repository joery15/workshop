{
  
    
        "post0": {
            "title": "",
            "content": "Problem Statement: Produce a supervised machine learning model to predict whether a load will be paid in full or charged off . Step One: Data processing and cleaning . import pandas as pd import numpy as np pd.options.display.max_columns = 20 pd.options.display.max_rows = 20 import datetime as dt # visualization import matplotlib.pyplot as plt import seaborn as sns from IPython.display import HTML %matplotlib inline . Packages for model build: . from sklearn import metrics from sklearn.preprocessing import OneHotEncoder, LabelEncoder from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score from sklearn.metrics import classification_report, precision_recall_curve, roc_curve, auc . Load the data to Pandas: . df=pd.read_csv(&#39;CaseStudy_Dataset.csv&#39;) df.head(5) . /usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,47) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . id member_id loan_amnt funded_amnt funded_amnt_inv term int_rate installment grade sub_grade ... orig_projected_additional_accrued_interest hardship_payoff_balance_amount hardship_last_payment_amount debt_settlement_flag debt_settlement_flag_date settlement_status settlement_date settlement_amount settlement_percentage settlement_term . 0 NaN | NaN | 5000.0 | 5000.0 | 4975.0 | 36 months | 10.65% | 162.87 | B | B2 | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 1 NaN | NaN | 2500.0 | 2500.0 | 2500.0 | 60 months | 15.27% | 59.83 | C | C4 | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 2 NaN | NaN | 2400.0 | 2400.0 | 2400.0 | 36 months | 15.96% | 84.33 | C | C5 | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | NaN | 10000.0 | 10000.0 | 10000.0 | 36 months | 13.49% | 339.31 | C | C1 | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 4 NaN | NaN | 3000.0 | 3000.0 | 3000.0 | 60 months | 12.69% | 67.79 | B | B5 | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 144 columns . df.shape . (42545, 144) . Check the column information: . list(df.columns) . [&#39;id&#39;, &#39;member_id&#39;, &#39;loan_amnt&#39;, &#39;funded_amnt&#39;, &#39;funded_amnt_inv&#39;, &#39;term&#39;, &#39;int_rate&#39;, &#39;installment&#39;, &#39;grade&#39;, &#39;sub_grade&#39;, &#39;emp_title&#39;, &#39;emp_length&#39;, &#39;home_ownership&#39;, &#39;annual_inc&#39;, &#39;verification_status&#39;, &#39;issue_d&#39;, &#39;loan_status&#39;, &#39;pymnt_plan&#39;, &#39;url&#39;, &#39;desc&#39;, &#39;purpose&#39;, &#39;title&#39;, &#39;zip_code&#39;, &#39;addr_state&#39;, &#39;dti&#39;, &#39;delinq_2yrs&#39;, &#39;earliest_cr_line&#39;, &#39;inq_last_6mths&#39;, &#39;mths_since_last_delinq&#39;, &#39;mths_since_last_record&#39;, &#39;open_acc&#39;, &#39;pub_rec&#39;, &#39;revol_bal&#39;, &#39;revol_util&#39;, &#39;total_acc&#39;, &#39;initial_list_status&#39;, &#39;out_prncp&#39;, &#39;out_prncp_inv&#39;, &#39;total_pymnt&#39;, &#39;total_pymnt_inv&#39;, &#39;total_rec_prncp&#39;, &#39;total_rec_int&#39;, &#39;total_rec_late_fee&#39;, &#39;recoveries&#39;, &#39;collection_recovery_fee&#39;, &#39;last_pymnt_d&#39;, &#39;last_pymnt_amnt&#39;, &#39;next_pymnt_d&#39;, &#39;last_credit_pull_d&#39;, &#39;collections_12_mths_ex_med&#39;, &#39;mths_since_last_major_derog&#39;, &#39;policy_code&#39;, &#39;application_type&#39;, &#39;annual_inc_joint&#39;, &#39;dti_joint&#39;, &#39;verification_status_joint&#39;, &#39;acc_now_delinq&#39;, &#39;tot_coll_amt&#39;, &#39;tot_cur_bal&#39;, &#39;open_acc_6m&#39;, &#39;open_act_il&#39;, &#39;open_il_12m&#39;, &#39;open_il_24m&#39;, &#39;mths_since_rcnt_il&#39;, &#39;total_bal_il&#39;, &#39;il_util&#39;, &#39;open_rv_12m&#39;, &#39;open_rv_24m&#39;, &#39;max_bal_bc&#39;, &#39;all_util&#39;, &#39;total_rev_hi_lim&#39;, &#39;inq_fi&#39;, &#39;total_cu_tl&#39;, &#39;inq_last_12m&#39;, &#39;acc_open_past_24mths&#39;, &#39;avg_cur_bal&#39;, &#39;bc_open_to_buy&#39;, &#39;bc_util&#39;, &#39;chargeoff_within_12_mths&#39;, &#39;delinq_amnt&#39;, &#39;mo_sin_old_il_acct&#39;, &#39;mo_sin_old_rev_tl_op&#39;, &#39;mo_sin_rcnt_rev_tl_op&#39;, &#39;mo_sin_rcnt_tl&#39;, &#39;mort_acc&#39;, &#39;mths_since_recent_bc&#39;, &#39;mths_since_recent_bc_dlq&#39;, &#39;mths_since_recent_inq&#39;, &#39;mths_since_recent_revol_delinq&#39;, &#39;num_accts_ever_120_pd&#39;, &#39;num_actv_bc_tl&#39;, &#39;num_actv_rev_tl&#39;, &#39;num_bc_sats&#39;, &#39;num_bc_tl&#39;, &#39;num_il_tl&#39;, &#39;num_op_rev_tl&#39;, &#39;num_rev_accts&#39;, &#39;num_rev_tl_bal_gt_0&#39;, &#39;num_sats&#39;, &#39;num_tl_120dpd_2m&#39;, &#39;num_tl_30dpd&#39;, &#39;num_tl_90g_dpd_24m&#39;, &#39;num_tl_op_past_12m&#39;, &#39;pct_tl_nvr_dlq&#39;, &#39;percent_bc_gt_75&#39;, &#39;pub_rec_bankruptcies&#39;, &#39;tax_liens&#39;, &#39;tot_hi_cred_lim&#39;, &#39;total_bal_ex_mort&#39;, &#39;total_bc_limit&#39;, &#39;total_il_high_credit_limit&#39;, &#39;revol_bal_joint&#39;, &#39;sec_app_earliest_cr_line&#39;, &#39;sec_app_inq_last_6mths&#39;, &#39;sec_app_mort_acc&#39;, &#39;sec_app_open_acc&#39;, &#39;sec_app_revol_util&#39;, &#39;sec_app_open_act_il&#39;, &#39;sec_app_num_rev_accts&#39;, &#39;sec_app_chargeoff_within_12_mths&#39;, &#39;sec_app_collections_12_mths_ex_med&#39;, &#39;sec_app_mths_since_last_major_derog&#39;, &#39;hardship_flag&#39;, &#39;hardship_type&#39;, &#39;hardship_reason&#39;, &#39;hardship_status&#39;, &#39;deferral_term&#39;, &#39;hardship_amount&#39;, &#39;hardship_start_date&#39;, &#39;hardship_end_date&#39;, &#39;payment_plan_start_date&#39;, &#39;hardship_length&#39;, &#39;hardship_dpd&#39;, &#39;hardship_loan_status&#39;, &#39;orig_projected_additional_accrued_interest&#39;, &#39;hardship_payoff_balance_amount&#39;, &#39;hardship_last_payment_amount&#39;, &#39;debt_settlement_flag&#39;, &#39;debt_settlement_flag_date&#39;, &#39;settlement_status&#39;, &#39;settlement_date&#39;, &#39;settlement_amount&#39;, &#39;settlement_percentage&#39;, &#39;settlement_term&#39;] . Check the target variable - loan_status by using value_counts(): . df[&#39;loan_status&#39;].value_counts() . Fully Paid 34121 Charged Off 5672 Does not meet the credit policy. Status:Fully Paid 1988 Does not meet the credit policy. Status:Charged Off 761 Name: loan_status, dtype: int64 . Usually for loan applications which didn&#39;t meet credit policy, they should be declined directly before sent to the model. Although, in this case these loans still had &#39;Status&#39; information, we would remove these records from build model and assume these loans won&#39;t be processed by the model . Only keep records which passed credit policy: . df=df.loc[df[&#39;loan_status&#39;].isin([&#39;Fully Paid&#39;, &#39;Charged Off&#39;])] . df[&#39;loan_status&#39;].value_counts() . Fully Paid 34121 Charged Off 5672 Name: loan_status, dtype: int64 . Define a function to check missing values: . def missing_values_table(df): #1 Total missing values mis_val = df.isnull().sum() #2 Percentage of missing values mis_val_percent = 100 * df.isnull().sum() / len(df) #3 Make a table with the results mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) #4 Rename the columns mis_val_table_ren_columns = mis_val_table.rename( columns = {0 : &#39;Missing Values&#39;, 1 : &#39;% of Total Values&#39;}) #5 Only keep the columns with missing values mis_val_table_only = mis_val_table_ren_columns.loc[mis_val_table_ren_columns[&#39;% of Total Values&#39;] &gt; 0] #6 Return the dataframe with missing information return mis_val_table_only . missing=missing_values_table(df) missing . Missing Values % of Total Values . id 39793 | 100.000000 | . member_id 39793 | 100.000000 | . emp_title 2468 | 6.202096 | . emp_length 1078 | 2.709019 | . url 39793 | 100.000000 | . ... ... | ... | . settlement_status 39643 | 99.623049 | . settlement_date 39643 | 99.623049 | . settlement_amount 39643 | 99.623049 | . settlement_percentage 39643 | 99.623049 | . settlement_term 39643 | 99.623049 | . 102 rows × 2 columns . Usually there are three options to deal with missing values: . Imputation | Create missing flag | Drop columns with a high percentage of missing vlaues | We see there are a number of columns with a high percentage of missing values. There is no well-established threshold for removing missing values, . and the best course of action depends on the problem. . Here, to reduce the number of features, we will remove any columns that have greater than 80% missing rate (in real situations, the threshold can be 98%). . missing_columns = list(missing.index[missing[&#39;% of Total Values&#39;] &gt; 80]) missing_columns . [&#39;id&#39;, &#39;member_id&#39;, &#39;url&#39;, &#39;mths_since_last_record&#39;, &#39;next_pymnt_d&#39;, &#39;mths_since_last_major_derog&#39;, &#39;annual_inc_joint&#39;, &#39;dti_joint&#39;, &#39;verification_status_joint&#39;, &#39;tot_coll_amt&#39;, &#39;tot_cur_bal&#39;, &#39;open_acc_6m&#39;, &#39;open_act_il&#39;, &#39;open_il_12m&#39;, &#39;open_il_24m&#39;, &#39;mths_since_rcnt_il&#39;, &#39;total_bal_il&#39;, &#39;il_util&#39;, &#39;open_rv_12m&#39;, &#39;open_rv_24m&#39;, &#39;max_bal_bc&#39;, &#39;all_util&#39;, &#39;total_rev_hi_lim&#39;, &#39;inq_fi&#39;, &#39;total_cu_tl&#39;, &#39;inq_last_12m&#39;, &#39;acc_open_past_24mths&#39;, &#39;avg_cur_bal&#39;, &#39;bc_open_to_buy&#39;, &#39;bc_util&#39;, &#39;mo_sin_old_il_acct&#39;, &#39;mo_sin_old_rev_tl_op&#39;, &#39;mo_sin_rcnt_rev_tl_op&#39;, &#39;mo_sin_rcnt_tl&#39;, &#39;mort_acc&#39;, &#39;mths_since_recent_bc&#39;, &#39;mths_since_recent_bc_dlq&#39;, &#39;mths_since_recent_inq&#39;, &#39;mths_since_recent_revol_delinq&#39;, &#39;num_accts_ever_120_pd&#39;, &#39;num_actv_bc_tl&#39;, &#39;num_actv_rev_tl&#39;, &#39;num_bc_sats&#39;, &#39;num_bc_tl&#39;, &#39;num_il_tl&#39;, &#39;num_op_rev_tl&#39;, &#39;num_rev_accts&#39;, &#39;num_rev_tl_bal_gt_0&#39;, &#39;num_sats&#39;, &#39;num_tl_120dpd_2m&#39;, &#39;num_tl_30dpd&#39;, &#39;num_tl_90g_dpd_24m&#39;, &#39;num_tl_op_past_12m&#39;, &#39;pct_tl_nvr_dlq&#39;, &#39;percent_bc_gt_75&#39;, &#39;tot_hi_cred_lim&#39;, &#39;total_bal_ex_mort&#39;, &#39;total_bc_limit&#39;, &#39;total_il_high_credit_limit&#39;, &#39;revol_bal_joint&#39;, &#39;sec_app_earliest_cr_line&#39;, &#39;sec_app_inq_last_6mths&#39;, &#39;sec_app_mort_acc&#39;, &#39;sec_app_open_acc&#39;, &#39;sec_app_revol_util&#39;, &#39;sec_app_open_act_il&#39;, &#39;sec_app_num_rev_accts&#39;, &#39;sec_app_chargeoff_within_12_mths&#39;, &#39;sec_app_collections_12_mths_ex_med&#39;, &#39;sec_app_mths_since_last_major_derog&#39;, &#39;hardship_type&#39;, &#39;hardship_reason&#39;, &#39;hardship_status&#39;, &#39;deferral_term&#39;, &#39;hardship_amount&#39;, &#39;hardship_start_date&#39;, &#39;hardship_end_date&#39;, &#39;payment_plan_start_date&#39;, &#39;hardship_length&#39;, &#39;hardship_dpd&#39;, &#39;hardship_loan_status&#39;, &#39;orig_projected_additional_accrued_interest&#39;, &#39;hardship_payoff_balance_amount&#39;, &#39;hardship_last_payment_amount&#39;, &#39;debt_settlement_flag_date&#39;, &#39;settlement_status&#39;, &#39;settlement_date&#39;, &#39;settlement_amount&#39;, &#39;settlement_percentage&#39;, &#39;settlement_term&#39;] . Drop these columns with missing rate &gt; 80%: . df2 = df.drop(columns = missing_columns) missing_values_table(df2) . Missing Values % of Total Values . emp_title 2468 | 6.202096 | . emp_length 1078 | 2.709019 | . desc 12969 | 32.591159 | . title 11 | 0.027643 | . mths_since_last_delinq 25733 | 64.667153 | . revol_util 50 | 0.125650 | . last_pymnt_d 71 | 0.178423 | . last_credit_pull_d 2 | 0.005026 | . collections_12_mths_ex_med 56 | 0.140728 | . chargeoff_within_12_mths 56 | 0.140728 | . pub_rec_bankruptcies 697 | 1.751564 | . tax_liens 39 | 0.098007 | . Check duplicates in Pandas: . df2.duplicated() . 0 False 1 False 2 False 3 False 4 False ... 39788 False 39789 False 39790 False 39791 False 39792 False Length: 39793, dtype: bool . df2.loc[df2.duplicated()] . loan_amnt funded_amnt funded_amnt_inv term int_rate installment grade sub_grade emp_title emp_length ... collections_12_mths_ex_med policy_code application_type acc_now_delinq chargeoff_within_12_mths delinq_amnt pub_rec_bankruptcies tax_liens hardship_flag debt_settlement_flag . 10 3000.0 | 3000.0 | 3000.000000 | 36 months | 18.64% | 109.43 | E | E1 | MKC Accounting | 9 years | ... | 0.0 | 1.0 | Individual | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | N | N | . 11 5600.0 | 5600.0 | 5600.000000 | 60 months | 21.28% | 152.39 | F | F2 | NaN | 4 years | ... | 0.0 | 1.0 | Individual | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | N | N | . 24670 10000.0 | 10000.0 | 9932.403848 | 36 months | 7.51% | 311.11 | A | A4 | ResMed | &lt; 1 year | ... | 0.0 | 1.0 | Individual | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | N | N | . 24671 8400.0 | 8400.0 | 8350.000000 | 36 months | 7.51% | 261.34 | A | A4 | Defense Language Institute | 4 years | ... | 0.0 | 1.0 | Individual | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | N | N | . 24672 7200.0 | 4525.0 | 4522.491529 | 60 months | 6.17% | 87.84 | A | A3 | MARR Inc. | 2 years | ... | 0.0 | 1.0 | Individual | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | N | N | . 24673 12000.0 | 12000.0 | 12000.000000 | 36 months | 11.86% | 397.77 | B | B5 | Gaming Laboratories International | 3 years | ... | 0.0 | 1.0 | Individual | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | N | N | . 24674 15000.0 | 9875.0 | 9625.000000 | 60 months | 15.95% | 239.88 | E | E1 | Hospital for Special Surgery | 2 years | ... | 0.0 | 1.0 | Individual | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | N | N | . 7 rows × 54 columns . Remove duplicates: . # inplace (bool, default False): Whether to drop duplicates in place or to return a copy df2.drop_duplicates(inplace= True) df2.shape . (39786, 54) . Step Two: Exploratory Data Analysis . 2.1 Check whether the data is balanced or imbalanced . Practice: . Encode the target variable as 1 for &#39;Charged Off&#39; and 0 for &#39;Fully Paid&#39;: | Calculate the charge off rate: | df2[&#39;target&#39;] = df2[&#39;loan_status&#39;].apply(lambda x:1 if x == &#39;Charged Off&#39; else 0) . df2.target.sum() / df2.shape[0] * 100 . 14.251244156235861 . The data is imbalanced . 2.2 Drop Columns May Cause Data Leakge . Data leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed. . In this case, we want to predict from the client&#39;s information whether the loan is &quot;charged off&quot; or &quot;fully paid&quot;. The variables about the loan payment can fully infer the status of the loan, which is against the goal of prediction. . data_leakage_cols = [&#39;funded_amnt&#39;,&#39;funded_amnt_inv&#39;,&#39;total_pymnt&#39;,&#39;total_pymnt_inv&#39;,&#39;total_rec_prncp&#39;, &#39;total_rec_int&#39;,&#39;total_rec_late_fee&#39;,&#39;recoveries&#39;,&#39;collection_recovery_fee&#39;, &#39;last_pymnt_amnt&#39;,&#39;chargeoff_within_12_mths&#39;,&#39;debt_settlement_flag&#39;] . df2.drop(columns=data_leakage_cols, inplace=True) . 2.3 Explore numerical features . Basic descriptive statistics view: . df2.describe() . loan_amnt installment annual_inc dti delinq_2yrs inq_last_6mths mths_since_last_delinq open_acc pub_rec revol_bal total_acc out_prncp out_prncp_inv collections_12_mths_ex_med policy_code acc_now_delinq delinq_amnt pub_rec_bankruptcies tax_liens target . count 39786.000000 | 39786.000000 | 3.978600e+04 | 39786.000000 | 39786.000000 | 39786.000000 | 14059.000000 | 39786.000000 | 39786.000000 | 39786.000000 | 39786.000000 | 39786.0 | 39786.0 | 39730.0 | 39786.0 | 39786.0 | 39786.0 | 39089.000000 | 39747.0 | 39786.000000 | . mean 11231.360277 | 324.733637 | 6.897907e+04 | 13.317794 | 0.146534 | 0.869049 | 35.901913 | 9.294023 | 0.055145 | 13391.983914 | 22.090308 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.043286 | 0.0 | 0.142512 | . std 7464.542832 | 208.923212 | 6.376263e+04 | 6.678300 | 0.491826 | 1.070069 | 22.017895 | 4.399997 | 0.237653 | 15894.635107 | 11.401620 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.204381 | 0.0 | 0.349579 | . min 500.000000 | 15.690000 | 4.000000e+03 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | 0.000000 | 0.000000 | 2.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | . 25% 5500.000000 | 167.080000 | 4.050000e+04 | 8.180000 | 0.000000 | 0.000000 | 18.000000 | 6.000000 | 0.000000 | 3704.250000 | 13.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | . 50% 10000.000000 | 280.610000 | 5.900000e+04 | 13.410000 | 0.000000 | 1.000000 | 34.000000 | 9.000000 | 0.000000 | 8859.500000 | 20.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | . 75% 15000.000000 | 430.780000 | 8.234250e+04 | 18.600000 | 0.000000 | 1.000000 | 52.000000 | 12.000000 | 0.000000 | 17065.000000 | 29.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | . max 35000.000000 | 1305.190000 | 6.000000e+06 | 29.990000 | 11.000000 | 8.000000 | 120.000000 | 44.000000 | 4.000000 | 149588.000000 | 90.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 2.000000 | 0.0 | 1.000000 | . df2[&#39;delinq_amnt&#39;].value_counts() . 0.0 39786 Name: delinq_amnt, dtype: int64 . Drop columns with constant values: . collections_12_mths_ex_med | tax_liens | out_prncp | out_prncp_inv | delinq_amnt | acc_now_delinq | . df2.drop(columns=[&#39;collections_12_mths_ex_med&#39;,&#39;tax_liens&#39;,&#39;out_prncp&#39;,&#39;out_prncp_inv&#39;,&#39;delinq_amnt&#39;,&#39;acc_now_delinq&#39;], inplace=True) . df2.shape . (39786, 37) . 2.5 Explore Categorical features . print(df2.info(0)) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 39786 entries, 0 to 39792 Data columns (total 37 columns): # Column Non-Null Count Dtype -- -- 0 loan_amnt 39786 non-null float64 1 term 39786 non-null object 2 int_rate 39786 non-null object 3 installment 39786 non-null float64 4 grade 39786 non-null object 5 sub_grade 39786 non-null object 6 emp_title 37319 non-null object 7 emp_length 38708 non-null object 8 home_ownership 39786 non-null object 9 annual_inc 39786 non-null float64 10 verification_status 39786 non-null object 11 issue_d 39786 non-null object 12 loan_status 39786 non-null object 13 pymnt_plan 39786 non-null object 14 desc 26819 non-null object 15 purpose 39786 non-null object 16 title 39775 non-null object 17 zip_code 39786 non-null object 18 addr_state 39786 non-null object 19 dti 39786 non-null float64 20 delinq_2yrs 39786 non-null float64 21 earliest_cr_line 39786 non-null object 22 inq_last_6mths 39786 non-null float64 23 mths_since_last_delinq 14059 non-null float64 24 open_acc 39786 non-null float64 25 pub_rec 39786 non-null float64 26 revol_bal 39786 non-null float64 27 revol_util 39736 non-null object 28 total_acc 39786 non-null float64 29 initial_list_status 39786 non-null object 30 last_pymnt_d 39715 non-null object 31 last_credit_pull_d 39784 non-null object 32 policy_code 39786 non-null float64 33 application_type 39786 non-null object 34 pub_rec_bankruptcies 39089 non-null float64 35 hardship_flag 39786 non-null object 36 target 39786 non-null int64 dtypes: float64(13), int64(1), object(23) memory usage: 11.5+ MB None . Create a separate DataFrame consisting of only categorical features: . df_cat=df2.select_dtypes(include=[&#39;object&#39;]) df_cat.head() . term int_rate grade sub_grade emp_title emp_length home_ownership verification_status issue_d loan_status ... title zip_code addr_state earliest_cr_line revol_util initial_list_status last_pymnt_d last_credit_pull_d application_type hardship_flag . 0 36 months | 10.65% | B | B2 | NaN | 10+ years | RENT | Verified | 11-Dec | Fully Paid | ... | Computer | 860xx | AZ | Jan-85 | 83.70% | f | 15-Jan | 19-May | Individual | N | . 1 60 months | 15.27% | C | C4 | Ryder | &lt; 1 year | RENT | Source Verified | 11-Dec | Charged Off | ... | bike | 309xx | GA | Apr-99 | 9.40% | f | 13-Apr | 16-Oct | Individual | N | . 2 36 months | 15.96% | C | C5 | NaN | 10+ years | RENT | Not Verified | 11-Dec | Fully Paid | ... | real estate business | 606xx | IL | 1-Nov | 98.50% | f | 14-Jun | 17-Jun | Individual | N | . 3 36 months | 13.49% | C | C1 | AIR RESOURCES BOARD | 10+ years | RENT | Source Verified | 11-Dec | Fully Paid | ... | personel | 917xx | CA | Feb-96 | 21% | f | 15-Jan | 16-Apr | Individual | N | . 4 60 months | 12.69% | B | B5 | University Medical Group | 1 year | RENT | Source Verified | 11-Dec | Fully Paid | ... | Personal | 972xx | OR | Jan-96 | 53.90% | f | 17-Jan | 18-Apr | Individual | N | . 5 rows × 23 columns . Drop the target column and create a list with all categorical columns: . cat_columns=list(df_cat.drop(columns=[&#39;loan_status&#39;]).columns) cat_columns . [&#39;term&#39;, &#39;int_rate&#39;, &#39;grade&#39;, &#39;sub_grade&#39;, &#39;emp_title&#39;, &#39;emp_length&#39;, &#39;home_ownership&#39;, &#39;verification_status&#39;, &#39;issue_d&#39;, &#39;pymnt_plan&#39;, &#39;desc&#39;, &#39;purpose&#39;, &#39;title&#39;, &#39;zip_code&#39;, &#39;addr_state&#39;, &#39;earliest_cr_line&#39;, &#39;revol_util&#39;, &#39;initial_list_status&#39;, &#39;last_pymnt_d&#39;, &#39;last_credit_pull_d&#39;, &#39;application_type&#39;, &#39;hardship_flag&#39;] . Strip leading and trailing space of each categorical column: . for i in cat_columns: df2[i] = df2[i].str.strip() . Quickly explore each each categorical feature and check the frequency: . pd.set_option(&#39;display.max_columns&#39;, None) df2[cat_columns].head(5) . term int_rate grade sub_grade emp_title emp_length home_ownership verification_status issue_d pymnt_plan desc purpose title zip_code addr_state earliest_cr_line revol_util initial_list_status last_pymnt_d last_credit_pull_d application_type hardship_flag . 0 36 months | 10.65% | B | B2 | NaN | 10+ years | RENT | Verified | 11-Dec | n | Borrower added on 12/22/11 &gt; I need to upgrade... | credit_card | Computer | 860xx | AZ | Jan-85 | 83.70% | f | 15-Jan | 19-May | Individual | N | . 1 60 months | 15.27% | C | C4 | Ryder | &lt; 1 year | RENT | Source Verified | 11-Dec | n | Borrower added on 12/22/11 &gt; I plan to use thi... | car | bike | 309xx | GA | Apr-99 | 9.40% | f | 13-Apr | 16-Oct | Individual | N | . 2 36 months | 15.96% | C | C5 | NaN | 10+ years | RENT | Not Verified | 11-Dec | n | NaN | small_business | real estate business | 606xx | IL | 1-Nov | 98.50% | f | 14-Jun | 17-Jun | Individual | N | . 3 36 months | 13.49% | C | C1 | AIR RESOURCES BOARD | 10+ years | RENT | Source Verified | 11-Dec | n | Borrower added on 12/21/11 &gt; to pay for proper... | other | personel | 917xx | CA | Feb-96 | 21% | f | 15-Jan | 16-Apr | Individual | N | . 4 60 months | 12.69% | B | B5 | University Medical Group | 1 year | RENT | Source Verified | 11-Dec | n | Borrower added on 12/21/11 &gt; I plan on combini... | other | Personal | 972xx | OR | Jan-96 | 53.90% | f | 17-Jan | 18-Apr | Individual | N | . Check frequency: . for i in cat_columns: print(df2[i].value_counts().to_frame()) #for seperate columns: #df2[&#39;emp_title&#39;].value_counts() . term 36 months 29096 60 months 10690 int_rate 10.99% 958 13.49% 831 11.49% 826 7.51% 787 7.88% 725 ... ... 21.48% 1 16.20% 1 16.01% 1 18.36% 1 24.59% 1 [371 rows x 1 columns] grade B 12035 A 10085 C 8111 D 5325 E 2858 F 1054 G 318 sub_grade B3 2924 A4 2886 A5 2742 B5 2709 B4 2514 .. ... G1 105 G2 78 G4 56 G3 49 G5 30 [35 rows x 1 columns] emp_title US Army 136 Bank of America 109 IBM 67 AT&amp;T 64 USAF 57 ... ... US Senate 1 Rockwell Group 1 Bridgeport Police Dept 1 Carousel Audi 1 NBT Bank 1 [28658 rows x 1 columns] emp_length 10+ years 8899 &lt; 1 year 4590 2 years 4394 3 years 4098 4 years 3444 5 years 3286 1 year 3247 6 years 2231 7 years 1775 8 years 1485 9 years 1259 home_ownership RENT 18918 MORTGAGE 17703 OWN 3064 OTHER 98 NONE 3 verification_status Not Verified 16926 Verified 12844 Source Verified 10016 issue_d 11-Dec 2267 11-Nov 2232 11-Oct 2118 11-Sep 2067 11-Aug 1934 ... ... 7-Aug 33 8-Sep 32 7-Jul 30 7-Sep 18 7-Jun 1 [55 rows x 1 columns] pymnt_plan n 39786 desc 210 Debt Consolidation 8 Camping Membership 6 Personal Loan 4 consolidate debt 3 ... ... Borrower added on 03/11/10 &gt; Most of this loan ... 1 I&#39;ll be using this loan to payoff a Home Equity... 1 Borrower added on 11/21/11 &gt; This loan is to pu... 1 Borrower added on 04/07/11 &gt; I will use this mo... 1 Borrower added on 04/02/11 &gt; At the current tim... 1 [26562 rows x 1 columns] purpose debt_consolidation 18676 credit_card 5137 other 4001 home_improvement 2985 major_purchase 2188 small_business 1831 car 1551 wedding 948 medical 695 moving 583 house 382 vacation 381 educational 325 renewable_energy 103 title Debt Consolidation 2262 Debt Consolidation Loan 1745 Personal Loan 678 Consolidation 534 debt consolidation 530 ... ... Alot of Stupid 1 reunion vaction 1 Building Dream Cabin 1 3 Year Pay Plan Consolidated 1 Patti&#39;s Debt consolidation 1 [19353 rows x 1 columns] zip_code 100xx 597 945xx 546 112xx 517 606xx 503 070xx 473 ... ... 965xx 1 463xx 1 386xx 1 266xx 1 772xx 1 [823 rows x 1 columns] addr_state CA 7105 NY 3817 FL 2872 TX 2734 NJ 1855 .. ... IN 9 ID 6 IA 5 NE 5 ME 3 [50 rows x 1 columns] earliest_cr_line Nov-98 371 Oct-99 366 Dec-98 349 Oct-00 346 Dec-97 329 ... ... Feb-67 1 Nov-65 1 Dec-63 1 Jun-59 1 Jun-72 1 [526 rows x 1 columns] revol_util 0% 980 0.20% 63 63% 62 40.70% 59 0.10% 58 ... ... 33.14% 1 13.56% 1 43.61% 1 37.63% 1 4.85% 1 [1089 rows x 1 columns] initial_list_status f 39786 last_pymnt_d 13-Mar 1026 14-Dec 945 13-May 907 13-Feb 869 13-Apr 851 ... ... 17-Feb 6 8-Mar 5 8-Jan 4 17-Apr 1 8-Feb 1 [111 rows x 1 columns] last_credit_pull_d 19-May 7919 16-Oct 3579 19-Apr 926 19-Mar 766 18-Aug 674 ... ... 8-Jun 1 7-Dec 1 8-Jul 1 7-May 1 7-Jul 1 [141 rows x 1 columns] application_type Individual 39786 hardship_flag N 39786 . After the exploratory analysis, we have a few findings: . &#39;desc&#39; is not relevant to the mdoel build and should be dropped . | Drop all date columns to simplify the model build in this case. If we have more time, we can do some feature engineering by using date features, e.g. df_loan[&#39;issue_to_earliest_cr_line&#39;] = df_loan[&#39;issue_d&#39;] - df_loan[&#39;earliest_cr_line&#39;] . | &#39;pmnt_plan&#39;,&#39;hardship_flag&#39;,&#39;initial_list_status&#39; and &#39;application_type&#39; only have constant values and are useless for model build . | &#39;emp_title&#39;, &#39;zip_code&#39;, and &#39;title&#39; have too many unique values and are not informative, we should drop them . | To simplify the analysis, we only keep &#39;grade&#39; and drop &#39;sub_grade&#39; . | Also, some types of information filled in by customer are very difficult to verify (customers can put whatever they want to). To simplify the analysis for this case, we should drop these columns that cann&#39;t be easily verified: &#39;emp_length&#39;, &#39;purpose&#39; . | Finally, we create a list including all categorical columns should be dropped . drop_feature=[&#39;desc&#39;,&#39;issue_d&#39;,&#39;last_pymnt_d&#39;,&#39;last_credit_pull_d&#39;,&#39;earliest_cr_line&#39;, &#39;pymnt_plan&#39;,&#39;hardship_flag&#39;, &#39;emp_title&#39;, &#39;emp_length&#39;, &#39;zip_code&#39;,&#39;title&#39;, &#39;purpose&#39;,&#39;sub_grade&#39;,&#39;initial_list_status&#39;,&#39;application_type&#39;] . df2 = df2.drop(columns=drop_feature) . df2.head(5) . loan_amnt term int_rate installment grade home_ownership annual_inc verification_status loan_status addr_state dti delinq_2yrs inq_last_6mths mths_since_last_delinq open_acc pub_rec revol_bal revol_util total_acc policy_code pub_rec_bankruptcies target . 0 5000.0 | 36 months | 10.65% | 162.87 | B | RENT | 24000.0 | Verified | Fully Paid | AZ | 27.65 | 0.0 | 1.0 | NaN | 3.0 | 0.0 | 13648.0 | 83.70% | 9.0 | 1.0 | 0.0 | 0 | . 1 2500.0 | 60 months | 15.27% | 59.83 | C | RENT | 30000.0 | Source Verified | Charged Off | GA | 1.00 | 0.0 | 5.0 | NaN | 3.0 | 0.0 | 1687.0 | 9.40% | 4.0 | 1.0 | 0.0 | 1 | . 2 2400.0 | 36 months | 15.96% | 84.33 | C | RENT | 12252.0 | Not Verified | Fully Paid | IL | 8.72 | 0.0 | 2.0 | NaN | 2.0 | 0.0 | 2956.0 | 98.50% | 10.0 | 1.0 | 0.0 | 0 | . 3 10000.0 | 36 months | 13.49% | 339.31 | C | RENT | 49200.0 | Source Verified | Fully Paid | CA | 20.00 | 0.0 | 1.0 | 35.0 | 10.0 | 0.0 | 5598.0 | 21% | 37.0 | 1.0 | 0.0 | 0 | . 4 3000.0 | 60 months | 12.69% | 67.79 | B | RENT | 80000.0 | Source Verified | Fully Paid | OR | 17.94 | 0.0 | 0.0 | 38.0 | 15.0 | 0.0 | 27783.0 | 53.90% | 38.0 | 1.0 | 0.0 | 0 | . remaining_cat_fea=[i for i in cat_columns if i not in drop_feature] remaining_cat_fea . [&#39;term&#39;, &#39;int_rate&#39;, &#39;grade&#39;, &#39;home_ownership&#39;, &#39;verification_status&#39;, &#39;addr_state&#39;, &#39;revol_util&#39;] . 2.6 Encode remaining categorical features . Convert &#39;revol_util&#39; into a numerical feature: . df2[&#39;revol_util&#39;].value_counts() . 0% 980 0.20% 63 63% 62 40.70% 59 0.10% 58 ... 33.14% 1 13.56% 1 43.61% 1 37.63% 1 4.85% 1 Name: revol_util, Length: 1089, dtype: int64 . df2[&#39;revol_util&#39;] = df2[&#39;revol_util&#39;].str.replace(&#39;%&#39;, &#39;&#39;).astype(float)/100 df2[&#39;revol_util&#39;].value_counts() . 0.0000 980 0.0020 63 0.6300 62 0.4070 59 0.6670 58 ... 0.2781 1 0.0001 1 0.2311 1 0.1778 1 0.3489 1 Name: revol_util, Length: 1089, dtype: int64 . Convert &#39;int_rate&#39; into a numerical feature: | Convert &#39;verification_status&#39; into a numerical feature: if &#39;Not Verified&#39; then 0, else 1 | df2[&#39;int_rate&#39;] = df2[&#39;int_rate&#39;].str.replace(&#39;%&#39;, &#39;&#39;).astype(float)/100 df2[&#39;int_rate&#39;].value_counts() . 0.1099 958 0.1349 831 0.1149 826 0.0751 787 0.0788 725 ... 0.1836 1 0.1671 1 0.1872 1 0.2440 1 0.2264 1 Name: int_rate, Length: 371, dtype: int64 . df2[&#39;verification_status&#39;].value_counts() . Not Verified 16926 Verified 12844 Source Verified 10016 Name: verification_status, dtype: int64 . df2[&#39;verification_status&#39;] = df2[&#39;verification_status&#39;].apply(lambda x: 0 if x == &#39;Not Verified&#39; else 1) df2[&#39;verification_status&#39;].value_counts() . 1 22860 0 16926 Name: verification_status, dtype: int64 . Check remaining categorical features: . list(df2.select_dtypes(include=[&#39;object&#39;]).columns) . [&#39;term&#39;, &#39;grade&#39;, &#39;home_ownership&#39;, &#39;loan_status&#39;, &#39;addr_state&#39;] . Remove &#39;loan_status&#39;: . df2=df2.drop(columns=[&#39;loan_status&#39;]) . 2.7 One-hot encoding for remaining categorical features: . Most machine learning models unfortunately cannot deal with categorical variables . There are two mains to encode categorical variables: . Label encoding: assign each unique category in a categorical variable with an integer. . No new columns are created. . | One-hot encoding: create a new column for each unique category in a categorical variable. . Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns. . | def cate_convert(df, nan_as_category = True): original_columns = list(df.columns) categorical_columns = [col for col in df.columns if df[col].dtype == &#39;object&#39;] df = pd.get_dummies(df, columns = categorical_columns, dummy_na= nan_as_category) new_columns = [c for c in df.columns if c not in original_columns] return df, new_columns . df2,cat_cols = cate_convert(df2, nan_as_category = True) . df2.head() . loan_amnt int_rate installment annual_inc verification_status dti delinq_2yrs inq_last_6mths mths_since_last_delinq open_acc pub_rec revol_bal revol_util total_acc policy_code pub_rec_bankruptcies target term_36 months term_60 months term_nan grade_A grade_B grade_C grade_D grade_E grade_F grade_G grade_nan home_ownership_MORTGAGE home_ownership_NONE home_ownership_OTHER home_ownership_OWN home_ownership_RENT home_ownership_nan addr_state_AK addr_state_AL addr_state_AR addr_state_AZ addr_state_CA addr_state_CO addr_state_CT addr_state_DC addr_state_DE addr_state_FL addr_state_GA addr_state_HI addr_state_IA addr_state_ID addr_state_IL addr_state_IN addr_state_KS addr_state_KY addr_state_LA addr_state_MA addr_state_MD addr_state_ME addr_state_MI addr_state_MN addr_state_MO addr_state_MS addr_state_MT addr_state_NC addr_state_NE addr_state_NH addr_state_NJ addr_state_NM addr_state_NV addr_state_NY addr_state_OH addr_state_OK addr_state_OR addr_state_PA addr_state_RI addr_state_SC addr_state_SD addr_state_TN addr_state_TX addr_state_UT addr_state_VA addr_state_VT addr_state_WA addr_state_WI addr_state_WV addr_state_WY addr_state_nan . 0 5000.0 | 0.1065 | 162.87 | 24000.0 | 1 | 27.65 | 0.0 | 1.0 | NaN | 3.0 | 0.0 | 13648.0 | 0.837 | 9.0 | 1.0 | 0.0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 2500.0 | 0.1527 | 59.83 | 30000.0 | 1 | 1.00 | 0.0 | 5.0 | NaN | 3.0 | 0.0 | 1687.0 | 0.094 | 4.0 | 1.0 | 0.0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 2400.0 | 0.1596 | 84.33 | 12252.0 | 0 | 8.72 | 0.0 | 2.0 | NaN | 2.0 | 0.0 | 2956.0 | 0.985 | 10.0 | 1.0 | 0.0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 10000.0 | 0.1349 | 339.31 | 49200.0 | 1 | 20.00 | 0.0 | 1.0 | 35.0 | 10.0 | 0.0 | 5598.0 | 0.210 | 37.0 | 1.0 | 0.0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 3000.0 | 0.1269 | 67.79 | 80000.0 | 1 | 17.94 | 0.0 | 0.0 | 38.0 | 15.0 | 0.0 | 27783.0 | 0.539 | 38.0 | 1.0 | 0.0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2.8 Missing value imputation . missing=missing_values_table(df2) missing . Missing Values % of Total Values . mths_since_last_delinq 25727 | 64.663449 | . revol_util 50 | 0.125672 | . pub_rec_bankruptcies 697 | 1.751873 | . df2[&#39;pub_rec_bankruptcies&#39;].fillna(value=0,inplace=True) . Practice: . impute 0 for [&#39;revol_util&#39;]&#39;s missing values | impute 0 for [&#39;mths_since_last_delinq&#39;]&#39;s missing values | df2[&#39;revol_util&#39;].fillna(value=0,inplace=True) df2[&#39;mths_since_last_delinq&#39;].fillna(value=0,inplace=True) . missing=missing_values_table(df2) missing . Missing Values % of Total Values . Backup the dataset: . final=df2.copy() df2.to_csv(&#39;final_dataset_for_model.csv&#39;) . Step Three: ML Modelling . 3.1 Check Correlations . correlations = final.corr()[&#39;target&#39;].dropna().sort_values(ascending = False) print(&#39;Top Positive Correlations: n&#39;, correlations.head(15)) print(&#39; nTop Negative Correlations: n&#39;, correlations.tail(15)) . Top Positive Correlations: target 1.000000 int_rate 0.197918 term_60 months 0.149151 revol_util 0.095925 grade_E 0.088464 grade_D 0.078370 grade_F 0.077362 inq_last_6mths 0.071325 pub_rec 0.050927 loan_amnt 0.050035 pub_rec_bankruptcies 0.047140 grade_G 0.044960 dti 0.041984 verification_status 0.038998 grade_C 0.035708 Name: target, dtype: float64 Top Negative Correlations: addr_state_OH -0.007789 addr_state_VA -0.008817 open_acc -0.010199 addr_state_NY -0.010491 addr_state_MA -0.012153 addr_state_WY -0.012336 addr_state_PA -0.012936 addr_state_DC -0.015234 addr_state_TX -0.019222 total_acc -0.022640 home_ownership_MORTGAGE -0.024870 annual_inc -0.041168 grade_B -0.044163 grade_A -0.138053 term_36 months -0.149151 Name: target, dtype: float64 . 3.2 Split into Test and Train datasets . x=final.drop(columns=&#39;target&#39;) y=final[&#39;target&#39;] . x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0) print(x_train.shape) print(y_train.shape) print(x_test.shape) print(y_test.shape) . (29839, 84) (29839,) (9947, 84) (9947,) . 3.3 Logistic regression model . 1.Create the classifier(object) . logist= LogisticRegression() . 2.Train the model on training data . logist.fit(x_train, y_train) . LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) . 3.Make the prediction: . Now that the model has been trained, we can use it to make predictions. . We want to predict the probabilities of not paying a loan, so we use the model predict_proba method. . The first column is the probability of the target being 0 and the second column is the probability of the target being 1 . log_reg_pred = logist.predict_proba(x_test) y_pred_proba=log_reg_pred[:,1] y_pred_proba . array([0.06488344, 0.21125751, 0.19880481, ..., 0.23782679, 0.14403556, 0.15653833]) . Predict the label: . .predict() is for predicting class labels: . scikit-learn is using a threshold of P&gt;0.5 for binary classifications . y_pred = logist.predict(x_test) y_pred . array([0, 0, 0, ..., 0, 0, 0]) . Get the coefficient list: . coefficients = pd.concat([pd.DataFrame(list(x_train.columns)),pd.DataFrame(np.transpose(logist.coef_))], axis = 1) coefficients . 0 0 . 0 loan_amnt | 0.000092 | . 1 int_rate | -0.000093 | . 2 installment | -0.002933 | . 3 annual_inc | -0.000015 | . 4 verification_status | -0.000536 | . ... ... | ... | . 79 addr_state_WA | -0.000048 | . 80 addr_state_WI | -0.000035 | . 81 addr_state_WV | -0.000035 | . 82 addr_state_WY | -0.000017 | . 83 addr_state_nan | 0.000000 | . 84 rows × 2 columns . Check key metrics . An ROC curve is a plot of True Positive Rate vs False Positive Rate where False Positive Rate=FP/(TN+FP) =1-Specificity. . 4.Show the ROC_CURVE to evaluate the model performance . import numpy as np [fpr, tpr, thr] = metrics.roc_curve(y_test, y_pred_proba) idx = np.min(np.where(tpr &gt; 0.95)) # index of the first threshold for which the sensibility &gt; 0.95 plt.figure() plt.plot(fpr, tpr, color=&#39;coral&#39;, label=&#39;ROC curve (area = %0.3f)&#39; % metrics.auc(fpr, tpr)) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot([0, fpr[idx]], [tpr[idx], tpr[idx]], &#39;k--&#39;, color=&#39;blue&#39;) plt.plot([fpr[idx], fpr[idx]], [0, tpr[idx]], &#39;k--&#39;, color=&#39;blue&#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;False Positive Rate (1 - specificity)&#39;, fontsize=14) plt.ylabel(&#39;True Positive Rate (recall)&#39;, fontsize=14) plt.title(&#39;Receiver operating characteristic (ROC) curve&#39;) plt.legend(loc=&quot;lower right&quot;) plt.show() . AUROC (Area Under the Receiver Operating Characteristics) The more the area enclosed by the ROC curve, the better it is. The area under the curve can lie between 0 and 1. The closer it is to 1, the better it is . 3.4 Random Forest . Ensemble learning, in general, is a model that makes predictions based on a number of different models. By combining individual models, the ensemble model tends to be more flexible (less bias) and less data-sensitive (less variance) Two most popular ensemble methods are bagging and boosting. . Bagging: Training a bunch of individual models in a parallel way. Each model is trained by a random subset of the data =&gt; bootstrapping the data plus using the aggregate to make a decision is called bagging! . Random forest is an ensemble model using bagging as the ensemble method and decision tree as the individual model. | . Boosting: Training a bunch of individual models in a sequential way. Each individual model learns from mistakes made by the previous model . Gradient Boosting: GBT build trees one at a time, where each new tree helps to correct errors made by previously trained tree. GBT build trees one at a time, where each new tree helps to correct errors made by previously trained tree. | . 1.Create the classifier(object): . rf_model = RandomForestClassifier( n_estimators=200, max_depth=5) . 2.Train the model on training data: . rf_model.fit(x_train, y_train) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) . 3.Make the prediction: . rf_model_pred = rf_model.predict_proba(x_test) y_pred_proba=rf_model_pred[:,1] y_pred_proba . array([0.10550379, 0.11707997, 0.16681988, ..., 0.25148387, 0.08788686, 0.09433625]) . 4.Show the ROC_CURVE to evaluate the model performance: . import numpy as np [fpr, tpr, thr] = metrics.roc_curve(y_test, y_pred_proba) idx = np.min(np.where(tpr &gt; 0.95)) # index of the first threshold for which the sensibility &gt; 0.95 plt.figure() plt.plot(fpr, tpr, color=&#39;coral&#39;, label=&#39;ROC curve (area = %0.3f)&#39; % metrics.auc(fpr, tpr)) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot([0, fpr[idx]], [tpr[idx], tpr[idx]], &#39;k--&#39;, color=&#39;blue&#39;) plt.plot([fpr[idx], fpr[idx]], [0, tpr[idx]], &#39;k--&#39;, color=&#39;blue&#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;False Positive Rate (1 - specificity)&#39;, fontsize=14) plt.ylabel(&#39;True Positive Rate (recall)&#39;, fontsize=14) plt.title(&#39;Receiver operating characteristic (ROC) curve&#39;) plt.legend(loc=&quot;lower right&quot;) plt.show() . Get the feature importances for each feature using the following code: . rf_model.feature_importances_ . array([2.07642096e-02, 2.05728106e-01, 2.21873212e-02, 5.94969889e-02, 3.43595900e-03, 2.01586523e-02, 4.55410025e-03, 2.87152964e-02, 9.67583399e-03, 1.50872237e-02, 1.28938421e-02, 1.66657599e-02, 5.32754757e-02, 1.48071256e-02, 0.00000000e+00, 1.04428564e-02, 1.11456391e-01, 1.30515066e-01, 0.00000000e+00, 9.26269033e-02, 2.02492149e-02, 7.15566169e-03, 3.67560771e-02, 3.37121853e-02, 2.02383254e-02, 4.29898808e-03, 0.00000000e+00, 3.93134641e-03, 0.00000000e+00, 6.67200694e-04, 9.63066756e-04, 2.77963324e-03, 0.00000000e+00, 5.72727334e-04, 5.35960120e-04, 5.00393685e-04, 8.30400356e-04, 2.94437293e-03, 4.78268542e-04, 4.11112705e-04, 2.99049075e-04, 2.47698128e-04, 2.75967103e-03, 1.23771766e-03, 6.91525903e-04, 0.00000000e+00, 0.00000000e+00, 5.21561512e-04, 3.56241541e-06, 7.67569136e-04, 4.91500400e-04, 5.84089382e-04, 6.62329268e-04, 1.00412876e-03, 0.00000000e+00, 5.15377122e-04, 6.14898903e-04, 1.36756387e-03, 1.00092946e-04, 4.87712905e-04, 1.35013355e-03, 1.33934596e-03, 7.24579164e-04, 1.02239417e-03, 3.10550739e-04, 4.33255980e-03, 1.07144425e-03, 6.44713772e-04, 1.14631738e-04, 8.60436977e-04, 4.57857064e-04, 3.75144456e-04, 7.11524939e-04, 2.82881025e-04, 1.40524923e-04, 1.11797258e-03, 7.05168722e-04, 1.11388894e-03, 1.71599833e-04, 5.73403347e-04, 3.75321956e-04, 1.62278854e-04, 1.73548515e-04, 0.00000000e+00]) . Associate these feature importances with the corresponding features: . feature_importance_df = pd.DataFrame(list(zip(rf_model.feature_importances_, list(x_train.columns)))) feature_importance_df.columns = [&#39;feature.importance&#39;, &#39;feature&#39;] feature_importance_df.sort_values(by=&#39;feature.importance&#39;, ascending=False).head(20) # only show top 20 . feature.importance feature . 1 0.205728 | int_rate | . 17 0.130515 | term_60 months | . 16 0.111456 | term_36 months | . 19 0.092627 | grade_A | . 3 0.059497 | annual_inc | . 12 0.053275 | revol_util | . 22 0.036756 | grade_D | . 23 0.033712 | grade_E | . 7 0.028715 | inq_last_6mths | . 2 0.022187 | installment | . 0 0.020764 | loan_amnt | . 20 0.020249 | grade_B | . 24 0.020238 | grade_F | . 5 0.020159 | dti | . 11 0.016666 | revol_bal | . 9 0.015087 | open_acc | . 13 0.014807 | total_acc | . 10 0.012894 | pub_rec | . 15 0.010443 | pub_rec_bankruptcies | . 8 0.009676 | mths_since_last_delinq | .",
            "url": "https://joery15.github.io/workshop/2021/01/06/2021-01-05-ML_Model1-(3).html",
            "relUrl": "/2021/01/06/2021-01-05-ML_Model1-(3).html",
            "date": " • Jan 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "VGG_19--Playing with image",
            "content": "from __future__ import print_function from keras.preprocessing.image import load_img, save_img, img_to_array import numpy as np from scipy.optimize import fmin_l_bfgs_b import time from keras.applications import vgg19 from keras import backend as K import tensorflow as tf tf.compat.v1.disable_eager_execution() . !wget https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg !wget https://www.adaymag.com/wp-content/uploads/2016/07/adaymag-pink-city-04-650x429.jpg . --2021-01-06 00:53:01-- https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg Resolving upload.wikimedia.org (upload.wikimedia.org)... 198.35.26.112, 2620:0:863:ed1a::2:b Connecting to upload.wikimedia.org (upload.wikimedia.org)|198.35.26.112|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 37490 (37K) [image/jpeg] Saving to: ‘300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg’ 300px-Van_Gogh_-_St 100%[===================&gt;] 36.61K --.-KB/s in 0.03s 2021-01-06 00:53:01 (1.29 MB/s) - ‘300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg’ saved [37490/37490] --2021-01-06 00:53:01-- https://www.adaymag.com/wp-content/uploads/2016/07/adaymag-pink-city-04-650x429.jpg Resolving www.adaymag.com (www.adaymag.com)... 104.26.1.240, 104.26.0.240, 172.67.74.167 Connecting to www.adaymag.com (www.adaymag.com)|104.26.1.240|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 68498 (67K) [image/jpeg] Saving to: ‘adaymag-pink-city-04-650x429.jpg’ adaymag-pink-city-0 100%[===================&gt;] 66.89K --.-KB/s in 0.007s 2021-01-06 00:53:02 (9.04 MB/s) - ‘adaymag-pink-city-04-650x429.jpg’ saved [68498/68498] . %pylab inline import matplotlib.pyplot as plt import matplotlib.image as mpimg fig=plt.figure(figsize=(10,10)) fig.add_subplot(1,2,1) fig1=mpimg.imread(&#39;300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg&#39;) imgplot = plt.imshow(fig1) fig.add_subplot(1,2,2) fig2 = mpimg.imread(&#39;adaymag-pink-city-04-650x429.jpg&#39;) imgplot = plt.imshow(fig2) plt.show() . Populating the interactive namespace from numpy and matplotlib . base_image_path = &quot;adaymag-pink-city-04-650x429.jpg&quot; style_reference_image_path = &quot;300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg&quot; result_prefix = &quot;img&quot; iterations = 10 # these are the weights of the different loss components total_variation_weight = 1.0 style_weight = 1.0 content_weight = 0.025 # dimensions of the generated picture. width, height = load_img(base_image_path).size img_nrows = 400 img_ncols = int(width * img_nrows / height) . def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img . def deprocess_image(x): if K.image_data_format() == &#39;channels_first&#39;: x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # &#39;BGR&#39;-&gt;&#39;RGB&#39; x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype(&#39;uint8&#39;) return x . def gram_matrix(x): assert K.ndim(x) == 3 if K.image_data_format() == &#39;channels_first&#39;: features = K.batch_flatten(x) else: features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) gram = K.dot(features, K.transpose(features)) return gram # the &quot;style loss&quot; is designed to maintain # the style of the reference image in the generated image. # It is based on the gram matrices (which capture style) of # feature maps from the style reference image # and from the generated image def style_loss(style, combination): assert K.ndim(style) == 3 assert K.ndim(combination) == 3 S = gram_matrix(style) C = gram_matrix(combination) channels = 3 size = img_nrows * img_ncols return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2)) # an auxiliary loss function # designed to maintain the &quot;content&quot; of the # base image in the generated image def content_loss(base, combination): return K.sum(K.square(combination - base)) # the 3rd loss function, total variation loss, # designed to keep the generated image locally coherent def total_variation_loss(x): assert K.ndim(x) == 4 if K.image_data_format() == &#39;channels_first&#39;: a = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) . def eval_loss_and_grads(x): if K.image_data_format() == &#39;channels_first&#39;: x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype(&#39;float64&#39;) else: grad_values = np.array(outs[1:]).flatten().astype(&#39;float64&#39;) return loss_value, grad_values . class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values . base_image = K.variable(preprocess_image(base_image_path)) style_reference_image = K.variable(preprocess_image(style_reference_image_path)) # this will contain our generated image if K.image_data_format() == &#39;channels_first&#39;: combination_image = K.placeholder((1, 3, img_nrows, img_ncols)) else: combination_image = K.placeholder((1, img_nrows, img_ncols, 3)) # combine the 3 images into a single Keras tensor input_tensor = K.concatenate([base_image, style_reference_image, combination_image], axis=0) # build the VGG19 network with our 3 images as input # the model will be loaded with pre-trained ImageNet weights model = vgg19.VGG19(input_tensor=input_tensor, weights=&#39;imagenet&#39;, include_top=False) print(&#39;Model loaded.&#39;) # get the symbolic outputs of each &quot;key&quot; layer (we gave them unique names). outputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) # combine these loss functions into a single scalar loss = K.variable(0.0) layer_features = outputs_dict[&#39;block5_conv2&#39;] base_image_features = layer_features[0, :, :, :] combination_features = layer_features[2, :, :, :] loss = loss + content_weight * content_loss(base_image_features, combination_features) feature_layers = [&#39;block1_conv1&#39;, &#39;block2_conv1&#39;, &#39;block3_conv1&#39;, &#39;block4_conv1&#39;, &#39;block5_conv1&#39;] for layer_name in feature_layers: layer_features = outputs_dict[layer_name] style_reference_features = layer_features[1, :, :, :] combination_features = layer_features[2, :, :, :] sl = style_loss(style_reference_features, combination_features) loss = loss + (style_weight / len(feature_layers)) * sl loss = loss + total_variation_weight * total_variation_loss(combination_image) # get the gradients of the generated image wrt the loss grads = K.gradients(loss, combination_image) outputs = [loss] if isinstance(grads, (list, tuple)): outputs += grads else: outputs.append(grads) f_outputs = K.function([combination_image], outputs) . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5 80142336/80134624 [==============================] - 0s 0us/step Model loaded. . evaluator = Evaluator() . # so as to minimize the neural style loss x = preprocess_image(base_image_path) for i in range(iterations): print(&#39;Start of iteration&#39;, i) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print(&#39;Current loss value:&#39;, min_val) # save current generated image img = deprocess_image(x.copy()) fname = result_prefix + &#39;_at_iteration_%d.png&#39; % i save_img(fname, img) end_time = time.time() print(&#39;Image saved as&#39;, fname) print(&#39;Iteration %d completed in %ds&#39; % (i, end_time - start_time)) . Start of iteration 0 Current loss value: 7707641000.0 Image saved as img_at_iteration_0.png Iteration 0 completed in 464s Start of iteration 1 Current loss value: 4882982400.0 Image saved as img_at_iteration_1.png Iteration 1 completed in 458s Start of iteration 2 Current loss value: 4255487500.0 Image saved as img_at_iteration_2.png Iteration 2 completed in 460s Start of iteration 3 Current loss value: 3883568600.0 Image saved as img_at_iteration_3.png Iteration 3 completed in 458s Start of iteration 4 Current loss value: 3636776000.0 Image saved as img_at_iteration_4.png Iteration 4 completed in 459s Start of iteration 5 Current loss value: 3463846000.0 Image saved as img_at_iteration_5.png Iteration 5 completed in 458s Start of iteration 6 Current loss value: 3339007200.0 Image saved as img_at_iteration_6.png Iteration 6 completed in 457s Start of iteration 7 Current loss value: 3207090200.0 Image saved as img_at_iteration_7.png Iteration 7 completed in 456s Start of iteration 8 Current loss value: 3111734800.0 Image saved as img_at_iteration_8.png Iteration 8 completed in 458s Start of iteration 9 Current loss value: 3018798600.0 Image saved as img_at_iteration_9.png Iteration 9 completed in 459s . fig=plt.figure(figsize=(10,10)) fig.add_subplot(1,2,1) fig1=mpimg.imread(&#39;img_at_iteration_0.png&#39;) imgplot = plt.imshow(fig1) fig.add_subplot(1,2,2) fig2 = mpimg.imread(&#39;img_at_iteration_9.png&#39;) imgplot = plt.imshow(fig2) plt.show() .",
            "url": "https://joery15.github.io/workshop/2021/01/05/VGG_19_Playing_with_image.html",
            "relUrl": "/2021/01/05/VGG_19_Playing_with_image.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "ML_Model2--Titanic Case",
            "content": "0. Background Info . kaggle&#39;s case: . Titanic - Machine Learning from Disaster . information link:https://www.kaggle.com/c/titanic/overview . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, f1_score,roc_auc_score from sklearn import tree from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier from sklearn.linear_model import LogisticRegression,SGDClassifier from sklearn.naive_bayes import GaussianNB from xgboost import XGBClassifier . 1. Load the data . data1 = pd.read_csv(&quot;train.csv&quot;) data2 = pd.read_csv(&quot;test.csv&quot;) data3 = pd.read_csv(&quot;gender_submission.csv&quot;) data4=pd.merge(data3,data2) data=pd.concat([data1,data4],axis=0) data=data.reset_index() data.head() . index PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . 2. Pre-process the data (aka data wrangling) . 1, Data cleanning . data.drop([&#39;PassengerId&#39;,&#39;Cabin&#39;,&#39;Ticket&#39;],axis=1,inplace=True) data.head() . index Survived Pclass Name Sex Age SibSp Parch Fare Embarked . 0 0 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | 7.2500 | S | . 1 1 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | 71.2833 | C | . 2 2 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | 7.9250 | S | . 3 3 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 53.1000 | S | . 4 4 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 8.0500 | S | . 2, Identification and treatment of missing values and outliers. . data.isnull().sum() . index 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 263 SibSp 0 Parch 0 Fare 1 Embarked 2 dtype: int64 . data[data[&#39;Embarked&#39;].isnull()] . index Survived Pclass Name Sex Age SibSp Parch Fare Embarked . 61 61 | 1 | 1 | Icard, Miss. Amelie | female | 38.0 | 0 | 0 | 80.0 | NaN | . 829 829 | 1 | 1 | Stone, Mrs. George Nelson (Martha Evelyn) | female | 62.0 | 0 | 0 | 80.0 | NaN | . #https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html data[&#39;Embarked&#39;] = data[&#39;Embarked&#39;].fillna(&#39;S&#39;) . data.corr() . index Survived Pclass Age SibSp Parch Fare . index 1.000000 | 0.001504 | -0.018212 | 0.012723 | -0.027343 | 0.003911 | -0.003723 | . Survived 0.001504 | 1.000000 | -0.264710 | -0.053695 | 0.002370 | 0.108919 | 0.233622 | . Pclass -0.018212 | -0.264710 | 1.000000 | -0.408106 | 0.060832 | 0.018322 | -0.558629 | . Age 0.012723 | -0.053695 | -0.408106 | 1.000000 | -0.243699 | -0.150917 | 0.178740 | . SibSp -0.027343 | 0.002370 | 0.060832 | -0.243699 | 1.000000 | 0.373587 | 0.160238 | . Parch 0.003911 | 0.108919 | 0.018322 | -0.150917 | 0.373587 | 1.000000 | 0.221539 | . Fare -0.003723 | 0.233622 | -0.558629 | 0.178740 | 0.160238 | 0.221539 | 1.000000 | . data[data[&#39;Fare&#39;].isnull()] . index Survived Pclass Name Sex Age SibSp Parch Fare Embarked . 1043 152 | 0 | 3 | Storey, Mr. Thomas | male | 60.5 | 0 | 0 | NaN | S | . data[&#39;Fare&#39;] = data[&#39;Fare&#39;].fillna(data.groupby([&#39;Pclass&#39;])[&#39;Fare&#39;].mean()[3]) . #so fill the age with mean value data[&#39;Age&#39;].fillna(data[&#39;Age&#39;].mean(), inplace = True) . data.describe() . index Survived Pclass Age SibSp Parch Fare . count 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | . mean 369.478992 | 0.377387 | 2.294882 | 29.881138 | 0.498854 | 0.385027 | 33.280206 | . std 248.767105 | 0.484918 | 0.837836 | 12.883193 | 1.041658 | 0.865560 | 51.741830 | . min 0.000000 | 0.000000 | 1.000000 | 0.170000 | 0.000000 | 0.000000 | 0.000000 | . 25% 163.000000 | 0.000000 | 2.000000 | 22.000000 | 0.000000 | 0.000000 | 7.895800 | . 50% 327.000000 | 0.000000 | 3.000000 | 29.881138 | 0.000000 | 0.000000 | 14.454200 | . 75% 563.000000 | 1.000000 | 3.000000 | 35.000000 | 1.000000 | 0.000000 | 31.275000 | . max 890.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 9.000000 | 512.329200 | . sns.boxplot(x=&quot;Survived&quot;, y=&quot;Fare&quot;, data=data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f450096a080&gt; . data.drop(data[data.Fare &gt; 400].index, inplace=True) . 3, Feature engineering . table1=pd.get_dummies(data[&#39;Sex&#39;]) data=pd.concat([data, table1], axis=1) . table2=pd.get_dummies(data[&#39;Embarked&#39;]) data=pd.concat([data, table2], axis=1) . 3. Exploratory data analysis. . 1, At least two plots describing different aspects of the data set (e.g. identifying outliers, histograms of different distributions, or scatter plots to explore correlations). . table3=data.drop([&#39;Name&#39;,&#39;Sex&#39;,&#39;Embarked&#39;],axis=1) plt.figure(figsize=(8,8)) sns.heatmap(table3.astype(float).corr(), mask=np.triu(table3.astype(float).corr()), cmap = sns.diverging_palette(230, 20, as_cmap=True), annot=True, fmt=&#39;.1g&#39;, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4500430358&gt; . sns.pointplot(x=&quot;Embarked&quot;, y=&quot;Survived&quot;, hue=&quot;Sex&quot;, kind=&quot;box&quot;, data=data,palette=&quot;Set3&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4500422630&gt; . 2, Print a basic data description (e.g. number of examples, number features, number of examples in each class and such). . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1305 entries, 0 to 1308 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 index 1305 non-null int64 1 Survived 1305 non-null int64 2 Pclass 1305 non-null int64 3 Name 1305 non-null object 4 Sex 1305 non-null object 5 Age 1305 non-null float64 6 SibSp 1305 non-null int64 7 Parch 1305 non-null int64 8 Fare 1305 non-null float64 9 Embarked 1305 non-null object 10 female 1305 non-null uint8 11 male 1305 non-null uint8 12 C 1305 non-null uint8 13 Q 1305 non-null uint8 14 S 1305 non-null uint8 dtypes: float64(2), int64(5), object(3), uint8(5) memory usage: 158.5+ KB . 3, Print (or include in the plots) descriptive statistics (e.g. means, medians, standard deviation) . data.describe() . index Survived Pclass Age SibSp Parch Fare female male C Q S . count 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | . mean 369.065900 | 0.375479 | 2.298851 | 29.847057 | 0.500383 | 0.384674 | 31.811857 | 0.355556 | 0.644444 | 0.203831 | 0.094253 | 0.701916 | . std 248.772213 | 0.484432 | 0.836040 | 12.876700 | 1.042888 | 0.866421 | 44.489559 | 0.478865 | 0.478865 | 0.403000 | 0.292292 | 0.457592 | . min 0.000000 | 0.000000 | 1.000000 | 0.170000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 163.000000 | 0.000000 | 2.000000 | 22.000000 | 0.000000 | 0.000000 | 7.895800 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 326.000000 | 0.000000 | 3.000000 | 29.881138 | 0.000000 | 0.000000 | 14.454200 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | . 75% 562.000000 | 1.000000 | 3.000000 | 35.000000 | 1.000000 | 0.000000 | 31.000000 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | . max 890.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 9.000000 | 263.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . 4. Partition data into train, validation and test sets. . From Lecture06.slide: training set: 60% of total data set 13050.6= 783 Validation set: 20% of total data set 13050.2 = 261 Testing setzz: 20% of total data set 1305*0.2=261 . train_data=data[:783] valid_data=data[783:1044] test_data=data[1044:] . 5. Fit models on the training set (this can include a hyper-parameter search) and select the best based on validation set performance. . 1，building the machine learning model for both test and valid data . def build_x(df): return StandardScaler().fit_transform(df.drop(columns=[&#39;Name&#39;,&#39;Sex&#39;,&#39;Embarked&#39;,&#39;index&#39;,&#39;Survived&#39;])) . train_x=build_x(train_data) valid_x=build_x(valid_data) test_x=build_x(test_data) . train_y = train_data[&#39;Survived&#39;].values valid_y = valid_data[&#39;Survived&#39;].values test_y = test_data[&#39;Survived&#39;].values . 2, runing into different model . parameters={&#39;criterion&#39;:(&#39;gini&#39;,&#39;entropy&#39;), &#39;splitter&#39;:(&#39;random&#39;,&#39;best&#39;),&#39;max_depth&#39;:range(1,5)} clf=tree.DecisionTreeClassifier(random_state=30) clf_gs=GridSearchCV(clf,parameters) clf_gs=clf_gs.fit(train_x,train_y) clf_score=clf_gs.score(valid_x,valid_y) . parameters={&#39;criterion&#39;:(&#39;gini&#39;,&#39;entropy&#39;), &#39;max_features&#39;:(&#39;auto&#39;,&#39;sqrt&#39;,&#39;log2&#39;),&#39;max_depth&#39;:range(1,5)} random_forest=RandomForestClassifier() random_forest_rs=RandomizedSearchCV(random_forest,parameters) random_forest_rs=random_forest_rs.fit(train_x,train_y) random_forest_score=random_forest_rs.score(valid_x,valid_y) . Gradient_Boosting=GradientBoostingClassifier().fit(train_x,train_y) Gradient_Boosting_score=Gradient_Boosting.score(valid_x,valid_y) . parameters={&#39;solver&#39;:(&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;)} logis_R=LogisticRegression() logis_R_gs=GridSearchCV(logis_R,parameters) logis_R_gs=logis_R_gs.fit(train_x,train_y) logis_R_score=logis_R_gs.score(valid_x,valid_y) . GNB=GaussianNB().fit(train_x,train_y) GNB.score=GNB.score(valid_x,valid_y) . parameters={&#39;loss&#39;:(&#39;deviance&#39;,&#39;exponential&#39;),&#39;learning_rate&#39;:[0.01,0.05,0.1,0.2],&#39;n_estimators&#39;:[50,100,150]} SGD=GradientBoostingClassifier() SGD_gs=GridSearchCV(SGD,parameters) SGD_gs=SGD_gs.fit(train_x,train_y) SGD_score=SGD_gs.score(valid_x,valid_y) SGD_score . 0.8505747126436781 . Xgboost=XGBClassifier().fit(train_x,train_y) Xgboost_score=Xgboost.score(valid_x,valid_y) . 3, select the table from best performance of validation . results = pd.DataFrame({ &#39;Model&#39;: [&#39;Decision Tree&#39;, &#39;Random Forest Classifier&#39;,&#39;Gradient Boosting&#39;, &#39;Logistic Regression&#39;,&#39;Gaussian Naive Bayes&#39;,&#39;Stochastic Gradient Decent&#39;, &#39;xgbooste&#39;], &#39;Score&#39;: [clf_score,random_forest_score,Gradient_Boosting_score, logis_R_score,GNB.score,SGD_score,Xgboost_score]}) result_df = results.sort_values(by=&#39;Score&#39;, ascending=False) result_df = result_df.set_index(&#39;Score&#39;) print(result_df) . Model Score 0.915709 Random Forest Classifier 0.892720 Gaussian Naive Bayes 0.885057 Logistic Regression 0.865900 Decision Tree 0.865900 Gradient Boosting 0.858238 xgbooste 0.850575 Stochastic Gradient Decent . 6. Print the results of the final model on the test set. This should include accuracy, F1-score and AUC. . Y_prediction = random_forest_rs.predict(test_x) . accuracy=accuracy_score(test_y, Y_prediction) print(&#39;accuracy:&#39;, accuracy) . accuracy: 0.9693486590038314 . f1_score=f1_score(test_y, Y_prediction) print(&#39;F1 score:&#39;,f1_score ) . F1 score: 0.9574468085106385 . y_scores = random_forest_rs.predict_proba(test_x)[:,1] r_a_score = roc_auc_score(test_y, y_scores) print(&quot;ROC-AUC-Score:&quot;, r_a_score) . ROC-AUC-Score: 0.9959867499044465 . Final_result = pd.DataFrame({ &#39;Indicator&#39;: [&#39;Accuracy&#39;,&#39;F1 score&#39;,&#39;AUC Score&#39;], &#39;Score&#39;: [accuracy,f1_score,r_a_score]}) print(Final_result) . Indicator Score 0 Accuracy 0.969349 1 F1 score 0.957447 2 AUC Score 0.995987 .",
            "url": "https://joery15.github.io/workshop/2021/01/05/ML_Model2.html",
            "relUrl": "/2021/01/05/ML_Model2.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Data visualization--Analysis Distribution of Risk",
            "content": "import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt . data = pd.read_csv(&quot;datasets_credit_labelled.csv&quot;) data[&quot;Checking account&quot;]=data[&quot;Checking account&quot;].fillna(&quot;None&quot;) data[&quot;Saving accounts&quot;]=data[&quot;Saving accounts&quot;].fillna(&quot;None&quot;) data.head() . Unnamed: 0 Age Sex Job Housing Saving accounts Checking account Credit amount Duration Purpose Risk . 0 0 | 67 | male | 2 | own | None | little | 1169 | 6 | radio/TV | good | . 1 1 | 22 | female | 2 | own | little | moderate | 5951 | 48 | radio/TV | bad | . 2 2 | 49 | male | 1 | own | little | None | 2096 | 12 | education | good | . 3 3 | 45 | male | 2 | free | little | little | 7882 | 42 | furniture/equipment | good | . 4 4 | 53 | male | 2 | free | little | little | 4870 | 24 | car | bad | . E.g. Age Group [18-25):Student, [25,35):Yonge, [35,60):Adult, [60, ): Senior] . For Job: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled . Use charts to visulize . Age Group to Risk | Gender to Risk | Housing to Risk | Job to Risk | Saving Accounts to Risk | Credit Amount to Housing-boxplot | Job to Credit Amount | Age to Credit Amount | Any other things that revelent to analysis the relations between any of the columns | Correlation between them | data[&quot;Age_group&quot;]=pd.cut(data[&quot;Age&quot;], [18,25,35,60,100], labels=[&quot;Student&quot;,&quot;Yonge&quot;,&quot;Adult&quot;,&quot;Senior&quot;]) . plt.figure(figsize=(10,5)) sns.countplot(x=&quot;Age_group&quot;, hue=&quot;Risk&quot;, data=data) plt.show() . From the first bar chart, it can be inferred that the company should target on the group of &quot;Yong&quot; and &quot;Adult&quot;. These two groups occupied the largest amount of customers. When compared with relatives between good and bad risk(the orange and blue bar), the&quot;student&quot; group is highly risky while the &quot;Senior&quot; one is more reliable. . sns.countplot(x=&quot;Sex&quot;, hue=&quot;Risk&quot;, data=data, palette=&quot;pastel&quot;) plt.show() . As we can see from the bar chart, the major gender of their customer is male. And the man has more relative risk than women. For the male customer, the good risk amount in the male group is more than double the bad risk, while it is just double in the female group. . sns.countplot(x=&quot;Housing&quot;, hue=&quot;Risk&quot;, data=data,palette=&quot;ch:.25&quot;) plt.show() . From the overall bar in three groups, the group of own housing is the majority of our customers. Furthermore, this kind of person has a relatively good risk than other groups, when compared with the bad risk. On the opposite, the people who do not have any house are the riskiest among the three groups. . data[&quot;Job_group&quot;]=pd.cut(data[&quot;Job&quot;], [-1,0.5,1.5,2.5,3.5], labels=[&quot;unskilled and non-resident&quot;,&quot;unskilled and resident&quot;,&quot;skilled&quot;,&quot;highly skilled&quot;]) plt.figure(figsize=(10,5)) sns.countplot(x=&quot;Job_group&quot;, hue=&quot;Risk&quot;, data=data, palette=&quot;Set3&quot;) plt.show() . Based on the chart, the skilled group occupied the largest group of job. The risk of people in different job levels on good risks relatively higher than that on bad risk. . sns.countplot(x=&quot;Saving accounts&quot;, hue=&quot;Risk&quot;, data=data, palette=&quot;husl&quot;) plt.show() . From what we can see from the diagram, most borrowers have little or no money on saving accounts. But those quite rich and rich people are the safe group, whose good risk is higher relatively than bad risk. . sns.catplot(x=&quot;Housing&quot;, y=&quot;Credit amount&quot;, kind=&quot;box&quot;, data=data, palette=&quot;husl&quot;) plt.show() . From this boxplot, these people who do not have a house to live in are more likely to borrow a high amount of money. Other stable people (own a house or rent a house ) would be at a lower amount of credit. . sns.catplot(x=&quot;Job_group&quot;, y=&quot;Credit amount&quot;, kind=&quot;swarm&quot;,data=data, height=5, aspect=1.5) plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1296: UserWarning: 15.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) /usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1296: UserWarning: 42.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) . As we can see clearly from the boxplot, High skilled people will borrow a higher amount of money to satisfied their needs. For people who do not have skills, they would live at a lower live standard and borrow a lower credit amount. They should targeted on the skilled person. . sns.catplot(x=&quot;Age_group&quot;, y=&quot;Credit amount&quot;, kind=&quot;box&quot;, data=data, showfliers = False, palette=&quot;pastel&quot;) plt.show() . The group of younger and adult would borrow a higher amount than that of student and senior. From these four groups, young people would borrow the highest amount of money, while the senior would borrow the lowest one. . #Job, Credit Amount to Risk data[&quot;Checking account&quot;]=data[&quot;Checking account&quot;].fillna(&quot;none&quot;) sns.catplot(x=&quot;Job&quot;, y=&quot;Credit amount&quot;, hue=&quot;Risk&quot;, kind=&quot;box&quot;, data=data, palette=&quot;Set1&quot;) plt.show() . From the job type of 2 and 3, we can infer from the diagram that the high amount they borrow, the worse risk the debit would be. This principle would be the opposite of other groups. Moreover, a highly skilled group (job_type3) is highest on the credit amount. . data=data.merge(pd.get_dummies(data[&quot;Sex&quot;], drop_first=True, prefix=&quot;Sex&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Housing&quot;], drop_first=True, prefix=&quot;Housing&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Saving accounts&quot;], drop_first=True, prefix=&quot;Saving accounts&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Checking account&quot;], drop_first=True, prefix=&quot;Checking account&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Purpose&quot;], drop_first=True, prefix=&quot;Purpose&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Risk&quot;], drop_first=True, prefix=&quot;Risk&quot;),left_index=True, right_index=True) # remove the categorical variable data.drop(columns=[&quot;Sex&quot;, &quot;Housing&quot;, &quot;Saving accounts&quot;, &quot;Checking account&quot;, &quot;Purpose&quot;, &quot;Risk&quot;,&quot;Job_group&quot;,&quot;Age_group&quot;],inplace=True) . plt.figure(figsize=(15,15)) sns.heatmap(data.astype(float).corr(), mask=np.triu(data.astype(float).corr()), cmap = sns.diverging_palette(230, 20, as_cmap=True), annot=True, fmt=&#39;.1g&#39;, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa6897d8940&gt; . From the correlation heatmap, it can be seen that the duration has a strong positive relationship with the credit amount. The higher the credit amount, the longer the duration term. And the credit amount has a week positive relation with the job, which means that the more skilled people will borrow a higher amount of money. While for other negative relationships (-0.7,-0.4), these happen in the same category which is exclusive to others. .",
            "url": "https://joery15.github.io/workshop/2021/01/05/Data-visualization.html",
            "relUrl": "/2021/01/05/Data-visualization.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://joery15.github.io/workshop/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://joery15.github.io/workshop/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Driven one-year studies Master of Artificial Intelligence from Schulich School of Busines. Excellent communication capabilities. Experience with Convolutional Neural Network (CNN), and specific CNN frameworks like AlexNet and ResNet. Currently seeking a working opportunity in the data-related field. Driven to bring extensive theoretical and practical knowledge of Artificial Intelligence. Passionate about emerging technologies and business intelligence. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://joery15.github.io/workshop/www.linkedin.com/in/luo-jiayi/",
          "relUrl": "/www.linkedin.com/in/luo-jiayi/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joery15.github.io/workshop/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}