{
  
    
        "post0": {
            "title": "VGG_19--Playing with image",
            "content": "from __future__ import print_function from keras.preprocessing.image import load_img, save_img, img_to_array import numpy as np from scipy.optimize import fmin_l_bfgs_b import time from keras.applications import vgg19 from keras import backend as K import tensorflow as tf tf.compat.v1.disable_eager_execution() . !wget https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg !wget https://www.adaymag.com/wp-content/uploads/2016/07/adaymag-pink-city-04-650x429.jpg . --2021-01-06 00:53:01-- https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg Resolving upload.wikimedia.org (upload.wikimedia.org)... 198.35.26.112, 2620:0:863:ed1a::2:b Connecting to upload.wikimedia.org (upload.wikimedia.org)|198.35.26.112|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 37490 (37K) [image/jpeg] Saving to: ‘300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg’ 300px-Van_Gogh_-_St 100%[===================&gt;] 36.61K --.-KB/s in 0.03s 2021-01-06 00:53:01 (1.29 MB/s) - ‘300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg’ saved [37490/37490] --2021-01-06 00:53:01-- https://www.adaymag.com/wp-content/uploads/2016/07/adaymag-pink-city-04-650x429.jpg Resolving www.adaymag.com (www.adaymag.com)... 104.26.1.240, 104.26.0.240, 172.67.74.167 Connecting to www.adaymag.com (www.adaymag.com)|104.26.1.240|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 68498 (67K) [image/jpeg] Saving to: ‘adaymag-pink-city-04-650x429.jpg’ adaymag-pink-city-0 100%[===================&gt;] 66.89K --.-KB/s in 0.007s 2021-01-06 00:53:02 (9.04 MB/s) - ‘adaymag-pink-city-04-650x429.jpg’ saved [68498/68498] . %pylab inline import matplotlib.pyplot as plt import matplotlib.image as mpimg fig=plt.figure(figsize=(10,10)) fig.add_subplot(1,2,1) fig1=mpimg.imread(&#39;300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg&#39;) imgplot = plt.imshow(fig1) fig.add_subplot(1,2,2) fig2 = mpimg.imread(&#39;adaymag-pink-city-04-650x429.jpg&#39;) imgplot = plt.imshow(fig2) plt.show() . Populating the interactive namespace from numpy and matplotlib . base_image_path = &quot;adaymag-pink-city-04-650x429.jpg&quot; style_reference_image_path = &quot;300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg&quot; result_prefix = &quot;img&quot; iterations = 10 # these are the weights of the different loss components total_variation_weight = 1.0 style_weight = 1.0 content_weight = 0.025 # dimensions of the generated picture. width, height = load_img(base_image_path).size img_nrows = 400 img_ncols = int(width * img_nrows / height) . def preprocess_image(image_path): img = load_img(image_path, target_size=(img_nrows, img_ncols)) img = img_to_array(img) img = np.expand_dims(img, axis=0) img = vgg19.preprocess_input(img) return img . def deprocess_image(x): if K.image_data_format() == &#39;channels_first&#39;: x = x.reshape((3, img_nrows, img_ncols)) x = x.transpose((1, 2, 0)) else: x = x.reshape((img_nrows, img_ncols, 3)) # Remove zero-center by mean pixel x[:, :, 0] += 103.939 x[:, :, 1] += 116.779 x[:, :, 2] += 123.68 # &#39;BGR&#39;-&gt;&#39;RGB&#39; x = x[:, :, ::-1] x = np.clip(x, 0, 255).astype(&#39;uint8&#39;) return x . def gram_matrix(x): assert K.ndim(x) == 3 if K.image_data_format() == &#39;channels_first&#39;: features = K.batch_flatten(x) else: features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1))) gram = K.dot(features, K.transpose(features)) return gram # the &quot;style loss&quot; is designed to maintain # the style of the reference image in the generated image. # It is based on the gram matrices (which capture style) of # feature maps from the style reference image # and from the generated image def style_loss(style, combination): assert K.ndim(style) == 3 assert K.ndim(combination) == 3 S = gram_matrix(style) C = gram_matrix(combination) channels = 3 size = img_nrows * img_ncols return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2)) # an auxiliary loss function # designed to maintain the &quot;content&quot; of the # base image in the generated image def content_loss(base, combination): return K.sum(K.square(combination - base)) # the 3rd loss function, total variation loss, # designed to keep the generated image locally coherent def total_variation_loss(x): assert K.ndim(x) == 4 if K.image_data_format() == &#39;channels_first&#39;: a = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1]) b = K.square( x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:]) else: a = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :]) b = K.square( x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :]) return K.sum(K.pow(a + b, 1.25)) . def eval_loss_and_grads(x): if K.image_data_format() == &#39;channels_first&#39;: x = x.reshape((1, 3, img_nrows, img_ncols)) else: x = x.reshape((1, img_nrows, img_ncols, 3)) outs = f_outputs([x]) loss_value = outs[0] if len(outs[1:]) == 1: grad_values = outs[1].flatten().astype(&#39;float64&#39;) else: grad_values = np.array(outs[1:]).flatten().astype(&#39;float64&#39;) return loss_value, grad_values . class Evaluator(object): def __init__(self): self.loss_value = None self.grads_values = None def loss(self, x): assert self.loss_value is None loss_value, grad_values = eval_loss_and_grads(x) self.loss_value = loss_value self.grad_values = grad_values return self.loss_value def grads(self, x): assert self.loss_value is not None grad_values = np.copy(self.grad_values) self.loss_value = None self.grad_values = None return grad_values . base_image = K.variable(preprocess_image(base_image_path)) style_reference_image = K.variable(preprocess_image(style_reference_image_path)) # this will contain our generated image if K.image_data_format() == &#39;channels_first&#39;: combination_image = K.placeholder((1, 3, img_nrows, img_ncols)) else: combination_image = K.placeholder((1, img_nrows, img_ncols, 3)) # combine the 3 images into a single Keras tensor input_tensor = K.concatenate([base_image, style_reference_image, combination_image], axis=0) # build the VGG19 network with our 3 images as input # the model will be loaded with pre-trained ImageNet weights model = vgg19.VGG19(input_tensor=input_tensor, weights=&#39;imagenet&#39;, include_top=False) print(&#39;Model loaded.&#39;) # get the symbolic outputs of each &quot;key&quot; layer (we gave them unique names). outputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) # combine these loss functions into a single scalar loss = K.variable(0.0) layer_features = outputs_dict[&#39;block5_conv2&#39;] base_image_features = layer_features[0, :, :, :] combination_features = layer_features[2, :, :, :] loss = loss + content_weight * content_loss(base_image_features, combination_features) feature_layers = [&#39;block1_conv1&#39;, &#39;block2_conv1&#39;, &#39;block3_conv1&#39;, &#39;block4_conv1&#39;, &#39;block5_conv1&#39;] for layer_name in feature_layers: layer_features = outputs_dict[layer_name] style_reference_features = layer_features[1, :, :, :] combination_features = layer_features[2, :, :, :] sl = style_loss(style_reference_features, combination_features) loss = loss + (style_weight / len(feature_layers)) * sl loss = loss + total_variation_weight * total_variation_loss(combination_image) # get the gradients of the generated image wrt the loss grads = K.gradients(loss, combination_image) outputs = [loss] if isinstance(grads, (list, tuple)): outputs += grads else: outputs.append(grads) f_outputs = K.function([combination_image], outputs) . Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5 80142336/80134624 [==============================] - 0s 0us/step Model loaded. . evaluator = Evaluator() . # so as to minimize the neural style loss x = preprocess_image(base_image_path) for i in range(iterations): print(&#39;Start of iteration&#39;, i) start_time = time.time() x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20) print(&#39;Current loss value:&#39;, min_val) # save current generated image img = deprocess_image(x.copy()) fname = result_prefix + &#39;_at_iteration_%d.png&#39; % i save_img(fname, img) end_time = time.time() print(&#39;Image saved as&#39;, fname) print(&#39;Iteration %d completed in %ds&#39; % (i, end_time - start_time)) . Start of iteration 0 Current loss value: 7707641000.0 Image saved as img_at_iteration_0.png Iteration 0 completed in 464s Start of iteration 1 Current loss value: 4882982400.0 Image saved as img_at_iteration_1.png Iteration 1 completed in 458s Start of iteration 2 Current loss value: 4255487500.0 Image saved as img_at_iteration_2.png Iteration 2 completed in 460s Start of iteration 3 Current loss value: 3883568600.0 Image saved as img_at_iteration_3.png Iteration 3 completed in 458s Start of iteration 4 Current loss value: 3636776000.0 Image saved as img_at_iteration_4.png Iteration 4 completed in 459s Start of iteration 5 Current loss value: 3463846000.0 Image saved as img_at_iteration_5.png Iteration 5 completed in 458s Start of iteration 6 Current loss value: 3339007200.0 Image saved as img_at_iteration_6.png Iteration 6 completed in 457s Start of iteration 7 Current loss value: 3207090200.0 Image saved as img_at_iteration_7.png Iteration 7 completed in 456s Start of iteration 8 Current loss value: 3111734800.0 Image saved as img_at_iteration_8.png Iteration 8 completed in 458s Start of iteration 9 Current loss value: 3018798600.0 Image saved as img_at_iteration_9.png Iteration 9 completed in 459s . fig=plt.figure(figsize=(10,10)) fig.add_subplot(1,2,1) fig1=mpimg.imread(&#39;img_at_iteration_0.png&#39;) imgplot = plt.imshow(fig1) fig.add_subplot(1,2,2) fig2 = mpimg.imread(&#39;img_at_iteration_9.png&#39;) imgplot = plt.imshow(fig2) plt.show() .",
            "url": "https://joery15.github.io/workshop/2021/01/05/VGG_19_Playing_with_image.html",
            "relUrl": "/2021/01/05/VGG_19_Playing_with_image.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "ML_Model2--Titanic Case",
            "content": "0. Background Info . kaggle&#39;s case: . Titanic - Machine Learning from Disaster . information link:https://www.kaggle.com/c/titanic/overview . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, f1_score,roc_auc_score from sklearn import tree from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier from sklearn.linear_model import LogisticRegression,SGDClassifier from sklearn.naive_bayes import GaussianNB from xgboost import XGBClassifier . 1. Load the data . data1 = pd.read_csv(&quot;train.csv&quot;) data2 = pd.read_csv(&quot;test.csv&quot;) data3 = pd.read_csv(&quot;gender_submission.csv&quot;) data4=pd.merge(data3,data2) data=pd.concat([data1,data4],axis=0) data=data.reset_index() data.head() . index PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . 2. Pre-process the data (aka data wrangling) . 1, Data cleanning . data.drop([&#39;PassengerId&#39;,&#39;Cabin&#39;,&#39;Ticket&#39;],axis=1,inplace=True) data.head() . index Survived Pclass Name Sex Age SibSp Parch Fare Embarked . 0 0 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | 7.2500 | S | . 1 1 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | 71.2833 | C | . 2 2 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | 7.9250 | S | . 3 3 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 53.1000 | S | . 4 4 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 8.0500 | S | . 2, Identification and treatment of missing values and outliers. . data.isnull().sum() . index 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 263 SibSp 0 Parch 0 Fare 1 Embarked 2 dtype: int64 . data[data[&#39;Embarked&#39;].isnull()] . index Survived Pclass Name Sex Age SibSp Parch Fare Embarked . 61 61 | 1 | 1 | Icard, Miss. Amelie | female | 38.0 | 0 | 0 | 80.0 | NaN | . 829 829 | 1 | 1 | Stone, Mrs. George Nelson (Martha Evelyn) | female | 62.0 | 0 | 0 | 80.0 | NaN | . #https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html data[&#39;Embarked&#39;] = data[&#39;Embarked&#39;].fillna(&#39;S&#39;) . data.corr() . index Survived Pclass Age SibSp Parch Fare . index 1.000000 | 0.001504 | -0.018212 | 0.012723 | -0.027343 | 0.003911 | -0.003723 | . Survived 0.001504 | 1.000000 | -0.264710 | -0.053695 | 0.002370 | 0.108919 | 0.233622 | . Pclass -0.018212 | -0.264710 | 1.000000 | -0.408106 | 0.060832 | 0.018322 | -0.558629 | . Age 0.012723 | -0.053695 | -0.408106 | 1.000000 | -0.243699 | -0.150917 | 0.178740 | . SibSp -0.027343 | 0.002370 | 0.060832 | -0.243699 | 1.000000 | 0.373587 | 0.160238 | . Parch 0.003911 | 0.108919 | 0.018322 | -0.150917 | 0.373587 | 1.000000 | 0.221539 | . Fare -0.003723 | 0.233622 | -0.558629 | 0.178740 | 0.160238 | 0.221539 | 1.000000 | . data[data[&#39;Fare&#39;].isnull()] . index Survived Pclass Name Sex Age SibSp Parch Fare Embarked . 1043 152 | 0 | 3 | Storey, Mr. Thomas | male | 60.5 | 0 | 0 | NaN | S | . data[&#39;Fare&#39;] = data[&#39;Fare&#39;].fillna(data.groupby([&#39;Pclass&#39;])[&#39;Fare&#39;].mean()[3]) . #so fill the age with mean value data[&#39;Age&#39;].fillna(data[&#39;Age&#39;].mean(), inplace = True) . data.describe() . index Survived Pclass Age SibSp Parch Fare . count 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | . mean 369.478992 | 0.377387 | 2.294882 | 29.881138 | 0.498854 | 0.385027 | 33.280206 | . std 248.767105 | 0.484918 | 0.837836 | 12.883193 | 1.041658 | 0.865560 | 51.741830 | . min 0.000000 | 0.000000 | 1.000000 | 0.170000 | 0.000000 | 0.000000 | 0.000000 | . 25% 163.000000 | 0.000000 | 2.000000 | 22.000000 | 0.000000 | 0.000000 | 7.895800 | . 50% 327.000000 | 0.000000 | 3.000000 | 29.881138 | 0.000000 | 0.000000 | 14.454200 | . 75% 563.000000 | 1.000000 | 3.000000 | 35.000000 | 1.000000 | 0.000000 | 31.275000 | . max 890.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 9.000000 | 512.329200 | . sns.boxplot(x=&quot;Survived&quot;, y=&quot;Fare&quot;, data=data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f450096a080&gt; . data.drop(data[data.Fare &gt; 400].index, inplace=True) . 3, Feature engineering . table1=pd.get_dummies(data[&#39;Sex&#39;]) data=pd.concat([data, table1], axis=1) . table2=pd.get_dummies(data[&#39;Embarked&#39;]) data=pd.concat([data, table2], axis=1) . 3. Exploratory data analysis. . 1, At least two plots describing different aspects of the data set (e.g. identifying outliers, histograms of different distributions, or scatter plots to explore correlations). . table3=data.drop([&#39;Name&#39;,&#39;Sex&#39;,&#39;Embarked&#39;],axis=1) plt.figure(figsize=(8,8)) sns.heatmap(table3.astype(float).corr(), mask=np.triu(table3.astype(float).corr()), cmap = sns.diverging_palette(230, 20, as_cmap=True), annot=True, fmt=&#39;.1g&#39;, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4500430358&gt; . sns.pointplot(x=&quot;Embarked&quot;, y=&quot;Survived&quot;, hue=&quot;Sex&quot;, kind=&quot;box&quot;, data=data,palette=&quot;Set3&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4500422630&gt; . 2, Print a basic data description (e.g. number of examples, number features, number of examples in each class and such). . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1305 entries, 0 to 1308 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 index 1305 non-null int64 1 Survived 1305 non-null int64 2 Pclass 1305 non-null int64 3 Name 1305 non-null object 4 Sex 1305 non-null object 5 Age 1305 non-null float64 6 SibSp 1305 non-null int64 7 Parch 1305 non-null int64 8 Fare 1305 non-null float64 9 Embarked 1305 non-null object 10 female 1305 non-null uint8 11 male 1305 non-null uint8 12 C 1305 non-null uint8 13 Q 1305 non-null uint8 14 S 1305 non-null uint8 dtypes: float64(2), int64(5), object(3), uint8(5) memory usage: 158.5+ KB . 3, Print (or include in the plots) descriptive statistics (e.g. means, medians, standard deviation) . data.describe() . index Survived Pclass Age SibSp Parch Fare female male C Q S . count 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | . mean 369.065900 | 0.375479 | 2.298851 | 29.847057 | 0.500383 | 0.384674 | 31.811857 | 0.355556 | 0.644444 | 0.203831 | 0.094253 | 0.701916 | . std 248.772213 | 0.484432 | 0.836040 | 12.876700 | 1.042888 | 0.866421 | 44.489559 | 0.478865 | 0.478865 | 0.403000 | 0.292292 | 0.457592 | . min 0.000000 | 0.000000 | 1.000000 | 0.170000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 163.000000 | 0.000000 | 2.000000 | 22.000000 | 0.000000 | 0.000000 | 7.895800 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 326.000000 | 0.000000 | 3.000000 | 29.881138 | 0.000000 | 0.000000 | 14.454200 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | . 75% 562.000000 | 1.000000 | 3.000000 | 35.000000 | 1.000000 | 0.000000 | 31.000000 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | . max 890.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 9.000000 | 263.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . 4. Partition data into train, validation and test sets. . From Lecture06.slide: training set: 60% of total data set 13050.6= 783 Validation set: 20% of total data set 13050.2 = 261 Testing setzz: 20% of total data set 1305*0.2=261 . train_data=data[:783] valid_data=data[783:1044] test_data=data[1044:] . 5. Fit models on the training set (this can include a hyper-parameter search) and select the best based on validation set performance. . 1，building the machine learning model for both test and valid data . def build_x(df): return StandardScaler().fit_transform(df.drop(columns=[&#39;Name&#39;,&#39;Sex&#39;,&#39;Embarked&#39;,&#39;index&#39;,&#39;Survived&#39;])) . train_x=build_x(train_data) valid_x=build_x(valid_data) test_x=build_x(test_data) . train_y = train_data[&#39;Survived&#39;].values valid_y = valid_data[&#39;Survived&#39;].values test_y = test_data[&#39;Survived&#39;].values . 2, runing into different model . parameters={&#39;criterion&#39;:(&#39;gini&#39;,&#39;entropy&#39;), &#39;splitter&#39;:(&#39;random&#39;,&#39;best&#39;),&#39;max_depth&#39;:range(1,5)} clf=tree.DecisionTreeClassifier(random_state=30) clf_gs=GridSearchCV(clf,parameters) clf_gs=clf_gs.fit(train_x,train_y) clf_score=clf_gs.score(valid_x,valid_y) . parameters={&#39;criterion&#39;:(&#39;gini&#39;,&#39;entropy&#39;), &#39;max_features&#39;:(&#39;auto&#39;,&#39;sqrt&#39;,&#39;log2&#39;),&#39;max_depth&#39;:range(1,5)} random_forest=RandomForestClassifier() random_forest_rs=RandomizedSearchCV(random_forest,parameters) random_forest_rs=random_forest_rs.fit(train_x,train_y) random_forest_score=random_forest_rs.score(valid_x,valid_y) . Gradient_Boosting=GradientBoostingClassifier().fit(train_x,train_y) Gradient_Boosting_score=Gradient_Boosting.score(valid_x,valid_y) . parameters={&#39;solver&#39;:(&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;)} logis_R=LogisticRegression() logis_R_gs=GridSearchCV(logis_R,parameters) logis_R_gs=logis_R_gs.fit(train_x,train_y) logis_R_score=logis_R_gs.score(valid_x,valid_y) . GNB=GaussianNB().fit(train_x,train_y) GNB.score=GNB.score(valid_x,valid_y) . parameters={&#39;loss&#39;:(&#39;deviance&#39;,&#39;exponential&#39;),&#39;learning_rate&#39;:[0.01,0.05,0.1,0.2],&#39;n_estimators&#39;:[50,100,150]} SGD=GradientBoostingClassifier() SGD_gs=GridSearchCV(SGD,parameters) SGD_gs=SGD_gs.fit(train_x,train_y) SGD_score=SGD_gs.score(valid_x,valid_y) SGD_score . 0.8505747126436781 . Xgboost=XGBClassifier().fit(train_x,train_y) Xgboost_score=Xgboost.score(valid_x,valid_y) . 3, select the table from best performance of validation . results = pd.DataFrame({ &#39;Model&#39;: [&#39;Decision Tree&#39;, &#39;Random Forest Classifier&#39;,&#39;Gradient Boosting&#39;, &#39;Logistic Regression&#39;,&#39;Gaussian Naive Bayes&#39;,&#39;Stochastic Gradient Decent&#39;, &#39;xgbooste&#39;], &#39;Score&#39;: [clf_score,random_forest_score,Gradient_Boosting_score, logis_R_score,GNB.score,SGD_score,Xgboost_score]}) result_df = results.sort_values(by=&#39;Score&#39;, ascending=False) result_df = result_df.set_index(&#39;Score&#39;) print(result_df) . Model Score 0.915709 Random Forest Classifier 0.892720 Gaussian Naive Bayes 0.885057 Logistic Regression 0.865900 Decision Tree 0.865900 Gradient Boosting 0.858238 xgbooste 0.850575 Stochastic Gradient Decent . 6. Print the results of the final model on the test set. This should include accuracy, F1-score and AUC. . Y_prediction = random_forest_rs.predict(test_x) . accuracy=accuracy_score(test_y, Y_prediction) print(&#39;accuracy:&#39;, accuracy) . accuracy: 0.9693486590038314 . f1_score=f1_score(test_y, Y_prediction) print(&#39;F1 score:&#39;,f1_score ) . F1 score: 0.9574468085106385 . y_scores = random_forest_rs.predict_proba(test_x)[:,1] r_a_score = roc_auc_score(test_y, y_scores) print(&quot;ROC-AUC-Score:&quot;, r_a_score) . ROC-AUC-Score: 0.9959867499044465 . Final_result = pd.DataFrame({ &#39;Indicator&#39;: [&#39;Accuracy&#39;,&#39;F1 score&#39;,&#39;AUC Score&#39;], &#39;Score&#39;: [accuracy,f1_score,r_a_score]}) print(Final_result) . Indicator Score 0 Accuracy 0.969349 1 F1 score 0.957447 2 AUC Score 0.995987 .",
            "url": "https://joery15.github.io/workshop/2021/01/05/ML_Model2.html",
            "relUrl": "/2021/01/05/ML_Model2.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "ML_Model1-- Loan Payment Prediction",
            "content": "Problem Statement: Produce a supervised machine learning model to predict whether a load will be paid in full or charged off . Step One: Data processing and cleaning . import pandas as pd import numpy as np pd.options.display.max_columns = 20 pd.options.display.max_rows = 20 import datetime as dt # visualization import matplotlib.pyplot as plt import seaborn as sns from IPython.display import HTML %matplotlib inline . Packages for model build: . from sklearn import metrics from sklearn.preprocessing import OneHotEncoder, LabelEncoder from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score from sklearn.metrics import classification_report, precision_recall_curve, roc_curve, auc . Load the data to Pandas: . df=pd.read_csv(&#39;CaseStudy_Dataset2.csv&#39;) df.head(5) . /Users/apple/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (1,48) have mixed types.Specify dtype option on import or set low_memory=False. has_raised = await self.run_ast_nodes(code_ast.body, cell_name, . Unnamed: 0 id member_id loan_amnt funded_amnt funded_amnt_inv term int_rate installment grade ... orig_projected_additional_accrued_interest hardship_payoff_balance_amount hardship_last_payment_amount debt_settlement_flag debt_settlement_flag_date settlement_status settlement_date settlement_amount settlement_percentage settlement_term . 0 22545 | NaN | NaN | 8500.0 | 8500.0 | 8450.00000 | 36 months | 9.99% | 274.24 | B | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 1 22546 | NaN | NaN | 19000.0 | 19000.0 | 18975.00000 | 36 months | 13.72% | 646.80 | C | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 2 22547 | NaN | NaN | 4000.0 | 4000.0 | 4000.00000 | 60 months | 16.69% | 98.75 | E | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 3 22548 | NaN | NaN | 24250.0 | 17675.0 | 17431.82356 | 60 months | 12.23% | 395.23 | C | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 4 22549 | NaN | NaN | 3600.0 | 3600.0 | 3600.00000 | 36 months | 12.98% | 121.27 | C | ... | NaN | NaN | NaN | N | NaN | NaN | NaN | NaN | NaN | NaN | . 5 rows × 145 columns . df.shape . (20000, 145) . Check the column information: . list(df.columns) . [&#39;Unnamed: 0&#39;, &#39;id&#39;, &#39;member_id&#39;, &#39;loan_amnt&#39;, &#39;funded_amnt&#39;, &#39;funded_amnt_inv&#39;, &#39;term&#39;, &#39;int_rate&#39;, &#39;installment&#39;, &#39;grade&#39;, &#39;sub_grade&#39;, &#39;emp_title&#39;, &#39;emp_length&#39;, &#39;home_ownership&#39;, &#39;annual_inc&#39;, &#39;verification_status&#39;, &#39;issue_d&#39;, &#39;loan_status&#39;, &#39;pymnt_plan&#39;, &#39;url&#39;, &#39;desc&#39;, &#39;purpose&#39;, &#39;title&#39;, &#39;zip_code&#39;, &#39;addr_state&#39;, &#39;dti&#39;, &#39;delinq_2yrs&#39;, &#39;earliest_cr_line&#39;, &#39;inq_last_6mths&#39;, &#39;mths_since_last_delinq&#39;, &#39;mths_since_last_record&#39;, &#39;open_acc&#39;, &#39;pub_rec&#39;, &#39;revol_bal&#39;, &#39;revol_util&#39;, &#39;total_acc&#39;, &#39;initial_list_status&#39;, &#39;out_prncp&#39;, &#39;out_prncp_inv&#39;, &#39;total_pymnt&#39;, &#39;total_pymnt_inv&#39;, &#39;total_rec_prncp&#39;, &#39;total_rec_int&#39;, &#39;total_rec_late_fee&#39;, &#39;recoveries&#39;, &#39;collection_recovery_fee&#39;, &#39;last_pymnt_d&#39;, &#39;last_pymnt_amnt&#39;, &#39;next_pymnt_d&#39;, &#39;last_credit_pull_d&#39;, &#39;collections_12_mths_ex_med&#39;, &#39;mths_since_last_major_derog&#39;, &#39;policy_code&#39;, &#39;application_type&#39;, &#39;annual_inc_joint&#39;, &#39;dti_joint&#39;, &#39;verification_status_joint&#39;, &#39;acc_now_delinq&#39;, &#39;tot_coll_amt&#39;, &#39;tot_cur_bal&#39;, &#39;open_acc_6m&#39;, &#39;open_act_il&#39;, &#39;open_il_12m&#39;, &#39;open_il_24m&#39;, &#39;mths_since_rcnt_il&#39;, &#39;total_bal_il&#39;, &#39;il_util&#39;, &#39;open_rv_12m&#39;, &#39;open_rv_24m&#39;, &#39;max_bal_bc&#39;, &#39;all_util&#39;, &#39;total_rev_hi_lim&#39;, &#39;inq_fi&#39;, &#39;total_cu_tl&#39;, &#39;inq_last_12m&#39;, &#39;acc_open_past_24mths&#39;, &#39;avg_cur_bal&#39;, &#39;bc_open_to_buy&#39;, &#39;bc_util&#39;, &#39;chargeoff_within_12_mths&#39;, &#39;delinq_amnt&#39;, &#39;mo_sin_old_il_acct&#39;, &#39;mo_sin_old_rev_tl_op&#39;, &#39;mo_sin_rcnt_rev_tl_op&#39;, &#39;mo_sin_rcnt_tl&#39;, &#39;mort_acc&#39;, &#39;mths_since_recent_bc&#39;, &#39;mths_since_recent_bc_dlq&#39;, &#39;mths_since_recent_inq&#39;, &#39;mths_since_recent_revol_delinq&#39;, &#39;num_accts_ever_120_pd&#39;, &#39;num_actv_bc_tl&#39;, &#39;num_actv_rev_tl&#39;, &#39;num_bc_sats&#39;, &#39;num_bc_tl&#39;, &#39;num_il_tl&#39;, &#39;num_op_rev_tl&#39;, &#39;num_rev_accts&#39;, &#39;num_rev_tl_bal_gt_0&#39;, &#39;num_sats&#39;, &#39;num_tl_120dpd_2m&#39;, &#39;num_tl_30dpd&#39;, &#39;num_tl_90g_dpd_24m&#39;, &#39;num_tl_op_past_12m&#39;, &#39;pct_tl_nvr_dlq&#39;, &#39;percent_bc_gt_75&#39;, &#39;pub_rec_bankruptcies&#39;, &#39;tax_liens&#39;, &#39;tot_hi_cred_lim&#39;, &#39;total_bal_ex_mort&#39;, &#39;total_bc_limit&#39;, &#39;total_il_high_credit_limit&#39;, &#39;revol_bal_joint&#39;, &#39;sec_app_earliest_cr_line&#39;, &#39;sec_app_inq_last_6mths&#39;, &#39;sec_app_mort_acc&#39;, &#39;sec_app_open_acc&#39;, &#39;sec_app_revol_util&#39;, &#39;sec_app_open_act_il&#39;, &#39;sec_app_num_rev_accts&#39;, &#39;sec_app_chargeoff_within_12_mths&#39;, &#39;sec_app_collections_12_mths_ex_med&#39;, &#39;sec_app_mths_since_last_major_derog&#39;, &#39;hardship_flag&#39;, &#39;hardship_type&#39;, &#39;hardship_reason&#39;, &#39;hardship_status&#39;, &#39;deferral_term&#39;, &#39;hardship_amount&#39;, &#39;hardship_start_date&#39;, &#39;hardship_end_date&#39;, &#39;payment_plan_start_date&#39;, &#39;hardship_length&#39;, &#39;hardship_dpd&#39;, &#39;hardship_loan_status&#39;, &#39;orig_projected_additional_accrued_interest&#39;, &#39;hardship_payoff_balance_amount&#39;, &#39;hardship_last_payment_amount&#39;, &#39;debt_settlement_flag&#39;, &#39;debt_settlement_flag_date&#39;, &#39;settlement_status&#39;, &#39;settlement_date&#39;, &#39;settlement_amount&#39;, &#39;settlement_percentage&#39;, &#39;settlement_term&#39;] . Check the target variable - loan_status by using value_counts(): . df[&#39;loan_status&#39;].value_counts() . Fully Paid 14959 Charged Off 2289 Does not meet the credit policy. Status:Fully Paid 1988 Does not meet the credit policy. Status:Charged Off 761 Name: loan_status, dtype: int64 . Usually for loan applications which didn&#39;t meet credit policy, they should be declined directly before sent to the model. Although, in this case these loans still had &#39;Status&#39; information, we would remove these records from build model and assume these loans won&#39;t be processed by the model . Only keep records which passed credit policy: . df=df.loc[df[&#39;loan_status&#39;].isin([&#39;Fully Paid&#39;, &#39;Charged Off&#39;])] . df[&#39;loan_status&#39;].value_counts() . Fully Paid 14959 Charged Off 2289 Name: loan_status, dtype: int64 . Define a function to check missing values: . def missing_values_table(df): #1 Total missing values mis_val = df.isnull().sum() #2 Percentage of missing values mis_val_percent = 100 * df.isnull().sum() / len(df) #3 Make a table with the results mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) #4 Rename the columns mis_val_table_ren_columns = mis_val_table.rename( columns = {0 : &#39;Missing Values&#39;, 1 : &#39;% of Total Values&#39;}) #5 Only keep the columns with missing values mis_val_table_only = mis_val_table_ren_columns.loc[mis_val_table_ren_columns[&#39;% of Total Values&#39;] &gt; 0] #6 Return the dataframe with missing information return mis_val_table_only . missing=missing_values_table(df) missing . Missing Values % of Total Values . id 17248 | 100.000000 | . member_id 17248 | 100.000000 | . emp_title 973 | 5.641234 | . emp_length 288 | 1.669759 | . url 17248 | 100.000000 | . ... ... | ... | . settlement_status 17197 | 99.704314 | . settlement_date 17197 | 99.704314 | . settlement_amount 17197 | 99.704314 | . settlement_percentage 17197 | 99.704314 | . settlement_term 17197 | 99.704314 | . 102 rows × 2 columns . Usually there are three options to deal with missing values: . Imputation | Create missing flag | Drop columns with a high percentage of missing vlaues | We see there are a number of columns with a high percentage of missing values. There is no well-established threshold for removing missing values, . and the best course of action depends on the problem. . Here, to reduce the number of features, we will remove any columns that have greater than 80% missing rate (in real situations, the threshold can be 98%). . missing_columns = list(missing.index[missing[&#39;% of Total Values&#39;] &gt; 80]) missing_columns . [&#39;id&#39;, &#39;member_id&#39;, &#39;url&#39;, &#39;mths_since_last_record&#39;, &#39;next_pymnt_d&#39;, &#39;mths_since_last_major_derog&#39;, &#39;annual_inc_joint&#39;, &#39;dti_joint&#39;, &#39;verification_status_joint&#39;, &#39;tot_coll_amt&#39;, &#39;tot_cur_bal&#39;, &#39;open_acc_6m&#39;, &#39;open_act_il&#39;, &#39;open_il_12m&#39;, &#39;open_il_24m&#39;, &#39;mths_since_rcnt_il&#39;, &#39;total_bal_il&#39;, &#39;il_util&#39;, &#39;open_rv_12m&#39;, &#39;open_rv_24m&#39;, &#39;max_bal_bc&#39;, &#39;all_util&#39;, &#39;total_rev_hi_lim&#39;, &#39;inq_fi&#39;, &#39;total_cu_tl&#39;, &#39;inq_last_12m&#39;, &#39;acc_open_past_24mths&#39;, &#39;avg_cur_bal&#39;, &#39;bc_open_to_buy&#39;, &#39;bc_util&#39;, &#39;mo_sin_old_il_acct&#39;, &#39;mo_sin_old_rev_tl_op&#39;, &#39;mo_sin_rcnt_rev_tl_op&#39;, &#39;mo_sin_rcnt_tl&#39;, &#39;mort_acc&#39;, &#39;mths_since_recent_bc&#39;, &#39;mths_since_recent_bc_dlq&#39;, &#39;mths_since_recent_inq&#39;, &#39;mths_since_recent_revol_delinq&#39;, &#39;num_accts_ever_120_pd&#39;, &#39;num_actv_bc_tl&#39;, &#39;num_actv_rev_tl&#39;, &#39;num_bc_sats&#39;, &#39;num_bc_tl&#39;, &#39;num_il_tl&#39;, &#39;num_op_rev_tl&#39;, &#39;num_rev_accts&#39;, &#39;num_rev_tl_bal_gt_0&#39;, &#39;num_sats&#39;, &#39;num_tl_120dpd_2m&#39;, &#39;num_tl_30dpd&#39;, &#39;num_tl_90g_dpd_24m&#39;, &#39;num_tl_op_past_12m&#39;, &#39;pct_tl_nvr_dlq&#39;, &#39;percent_bc_gt_75&#39;, &#39;tot_hi_cred_lim&#39;, &#39;total_bal_ex_mort&#39;, &#39;total_bc_limit&#39;, &#39;total_il_high_credit_limit&#39;, &#39;revol_bal_joint&#39;, &#39;sec_app_earliest_cr_line&#39;, &#39;sec_app_inq_last_6mths&#39;, &#39;sec_app_mort_acc&#39;, &#39;sec_app_open_acc&#39;, &#39;sec_app_revol_util&#39;, &#39;sec_app_open_act_il&#39;, &#39;sec_app_num_rev_accts&#39;, &#39;sec_app_chargeoff_within_12_mths&#39;, &#39;sec_app_collections_12_mths_ex_med&#39;, &#39;sec_app_mths_since_last_major_derog&#39;, &#39;hardship_type&#39;, &#39;hardship_reason&#39;, &#39;hardship_status&#39;, &#39;deferral_term&#39;, &#39;hardship_amount&#39;, &#39;hardship_start_date&#39;, &#39;hardship_end_date&#39;, &#39;payment_plan_start_date&#39;, &#39;hardship_length&#39;, &#39;hardship_dpd&#39;, &#39;hardship_loan_status&#39;, &#39;orig_projected_additional_accrued_interest&#39;, &#39;hardship_payoff_balance_amount&#39;, &#39;hardship_last_payment_amount&#39;, &#39;debt_settlement_flag_date&#39;, &#39;settlement_status&#39;, &#39;settlement_date&#39;, &#39;settlement_amount&#39;, &#39;settlement_percentage&#39;, &#39;settlement_term&#39;] . Drop these columns with missing rate &gt; 80%: . df2 = df.drop(columns = missing_columns) missing_values_table(df2) . Missing Values % of Total Values . emp_title 973 | 5.641234 | . emp_length 288 | 1.669759 | . desc 3671 | 21.283627 | . title 11 | 0.063776 | . mths_since_last_delinq 10816 | 62.708720 | . revol_util 40 | 0.231911 | . last_pymnt_d 33 | 0.191327 | . last_credit_pull_d 2 | 0.011596 | . collections_12_mths_ex_med 56 | 0.324675 | . chargeoff_within_12_mths 56 | 0.324675 | . pub_rec_bankruptcies 697 | 4.041048 | . tax_liens 39 | 0.226113 | . Check duplicates in Pandas: . df2.duplicated() . 0 False 1 False 2 False 3 False 4 False ... 17243 False 17244 False 17245 False 17246 False 17247 False Length: 17248, dtype: bool . df2.loc[df2.duplicated()] . Unnamed: 0 loan_amnt funded_amnt funded_amnt_inv term int_rate installment grade sub_grade emp_title ... collections_12_mths_ex_med policy_code application_type acc_now_delinq chargeoff_within_12_mths delinq_amnt pub_rec_bankruptcies tax_liens hardship_flag debt_settlement_flag . 0 rows × 55 columns . Remove duplicates: . # inplace (bool, default False): Whether to drop duplicates in place or to return a copy df2.drop_duplicates(inplace= True) df2.shape . (17248, 55) . Step Two: Exploratory Data Analysis . 2.1 Check whether the data is balanced or imbalanced . Practice: . Encode the target variable as 1 for &#39;Charged Off&#39; and 0 for &#39;Fully Paid&#39;: | Calculate the charge off rate: | df2[&#39;target&#39;] = df2[&#39;loan_status&#39;].apply(lambda x:1 if x == &#39;Charged Off&#39; else 0) . df2.target.sum() / df2.shape[0] * 100 . 13.271103896103897 . The data is imbalanced . 2.2 Drop Columns May Cause Data Leakge . Data leakage is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed. . In this case, we want to predict from the client&#39;s information whether the loan is &quot;charged off&quot; or &quot;fully paid&quot;. The variables about the loan payment can fully infer the status of the loan, which is against the goal of prediction. . data_leakage_cols = [&#39;funded_amnt&#39;,&#39;funded_amnt_inv&#39;,&#39;total_pymnt&#39;,&#39;total_pymnt_inv&#39;,&#39;total_rec_prncp&#39;, &#39;total_rec_int&#39;,&#39;total_rec_late_fee&#39;,&#39;recoveries&#39;,&#39;collection_recovery_fee&#39;, &#39;last_pymnt_amnt&#39;,&#39;chargeoff_within_12_mths&#39;,&#39;debt_settlement_flag&#39;] . df2.drop(columns=data_leakage_cols, inplace=True) . 2.3 Explore numerical features . Basic descriptive statistics view: . df2.describe() . Unnamed: 0 loan_amnt installment annual_inc dti delinq_2yrs inq_last_6mths mths_since_last_delinq open_acc pub_rec ... total_acc out_prncp out_prncp_inv collections_12_mths_ex_med policy_code acc_now_delinq delinq_amnt pub_rec_bankruptcies tax_liens target . count 17248.000000 | 17248.000000 | 17248.000000 | 1.724800e+04 | 17248.000000 | 17248.000000 | 17248.000000 | 6432.000000 | 17248.000000 | 17248.000000 | ... | 17248.000000 | 17248.0 | 17248.0 | 17192.0 | 17248.0 | 17248.0 | 17248.0 | 16551.000000 | 17209.0 | 17248.000000 | . mean 31168.500000 | 10227.206053 | 308.455227 | 6.829683e+04 | 12.672873 | 0.144133 | 0.921382 | 34.708955 | 9.173643 | 0.056876 | ... | 21.502377 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.041206 | 0.0 | 0.132711 | . std 4979.213057 | 6375.972589 | 196.981532 | 8.021522e+04 | 6.589699 | 0.471344 | 1.105954 | 22.569229 | 4.447792 | 0.244758 | ... | 11.539355 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.199682 | 0.0 | 0.339272 | . min 22545.000000 | 500.000000 | 15.690000 | 4.000000e+03 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | 0.000000 | ... | 2.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | . 25% 26856.750000 | 5000.000000 | 161.470000 | 4.000000e+04 | 7.590000 | 0.000000 | 0.000000 | 17.000000 | 6.000000 | 0.000000 | ... | 13.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | . 50% 31168.500000 | 9000.000000 | 263.290000 | 5.649600e+04 | 12.790000 | 0.000000 | 1.000000 | 32.000000 | 8.000000 | 0.000000 | ... | 20.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | . 75% 35480.250000 | 14500.000000 | 407.090000 | 8.000000e+04 | 17.900000 | 0.000000 | 2.000000 | 51.000000 | 12.000000 | 0.000000 | ... | 28.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.000000 | 0.0 | 0.000000 | . max 39792.000000 | 25000.000000 | 950.120000 | 6.000000e+06 | 29.420000 | 7.000000 | 8.000000 | 107.000000 | 44.000000 | 4.000000 | ... | 90.000000 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 2.000000 | 0.0 | 1.000000 | . 8 rows × 21 columns . df2[&#39;delinq_amnt&#39;].value_counts() . 0.0 17248 Name: delinq_amnt, dtype: int64 . Drop columns with constant values: . collections_12_mths_ex_med | tax_liens | out_prncp | out_prncp_inv | delinq_amnt | acc_now_delinq | . df2.drop(columns=[&#39;collections_12_mths_ex_med&#39;,&#39;tax_liens&#39;,&#39;out_prncp&#39;,&#39;out_prncp_inv&#39;,&#39;delinq_amnt&#39;,&#39;acc_now_delinq&#39;], inplace=True) . df2.shape . (17248, 38) . 2.5 Explore Categorical features . print(df2.info(0)) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 17248 entries, 0 to 17247 Data columns (total 38 columns): # Column Non-Null Count Dtype -- -- 0 Unnamed: 0 17248 non-null int64 1 loan_amnt 17248 non-null float64 2 term 17248 non-null object 3 int_rate 17248 non-null object 4 installment 17248 non-null float64 5 grade 17248 non-null object 6 sub_grade 17248 non-null object 7 emp_title 16275 non-null object 8 emp_length 16960 non-null object 9 home_ownership 17248 non-null object 10 annual_inc 17248 non-null float64 11 verification_status 17248 non-null object 12 issue_d 17248 non-null object 13 loan_status 17248 non-null object 14 pymnt_plan 17248 non-null object 15 desc 13577 non-null object 16 purpose 17248 non-null object 17 title 17237 non-null object 18 zip_code 17248 non-null object 19 addr_state 17248 non-null object 20 dti 17248 non-null float64 21 delinq_2yrs 17248 non-null float64 22 earliest_cr_line 17248 non-null object 23 inq_last_6mths 17248 non-null float64 24 mths_since_last_delinq 6432 non-null float64 25 open_acc 17248 non-null float64 26 pub_rec 17248 non-null float64 27 revol_bal 17248 non-null float64 28 revol_util 17208 non-null object 29 total_acc 17248 non-null float64 30 initial_list_status 17248 non-null object 31 last_pymnt_d 17215 non-null object 32 last_credit_pull_d 17246 non-null object 33 policy_code 17248 non-null float64 34 application_type 17248 non-null object 35 pub_rec_bankruptcies 16551 non-null float64 36 hardship_flag 17248 non-null object 37 target 17248 non-null int64 dtypes: float64(13), int64(2), object(23) memory usage: 5.1+ MB None . Create a separate DataFrame consisting of only categorical features: . df_cat=df2.select_dtypes(include=[&#39;object&#39;]) df_cat.head() . term int_rate grade sub_grade emp_title emp_length home_ownership verification_status issue_d loan_status ... title zip_code addr_state earliest_cr_line revol_util initial_list_status last_pymnt_d last_credit_pull_d application_type hardship_flag . 0 36 months | 9.99% | B | B4 | TCVCG | &lt; 1 year | RENT | Not Verified | 10-Dec | Fully Paid | ... | tracy | 071xx | NJ | Jan-74 | 80.10% | f | 11-Jan | 11-Jan | Individual | N | . 1 36 months | 13.72% | C | C5 | Santa Fe Springs Storage Park | 10+ years | RENT | Verified | 10-Dec | Fully Paid | ... | Bye Bye BofA | 906xx | CA | Sep-96 | 57.50% | f | 12-Jun | 12-Jul | Individual | N | . 2 60 months | 16.69% | E | E3 | NaN | NaN | RENT | Verified | 10-Dec | Fully Paid | ... | rita loan | 330xx | FL | 2-Jun | 65.10% | f | 15-Sep | 15-Sep | Individual | N | . 3 60 months | 12.23% | C | C1 | Norfolk Southern Corp. | 10+ years | MORTGAGE | Verified | 10-Dec | Fully Paid | ... | connie1958 | 450xx | OH | Jun-90 | 68.70% | f | 13-Oct | 13-Nov | Individual | N | . 4 36 months | 12.98% | C | C3 | Eaton drilling | 4 years | MORTGAGE | Not Verified | 10-Dec | Charged Off | ... | Bill help | 956xx | CA | 7-Mar | 33.60% | f | 12-Sep | 16-Oct | Individual | N | . 5 rows × 23 columns . Drop the target column and create a list with all categorical columns: . cat_columns=list(df_cat.drop(columns=[&#39;loan_status&#39;]).columns) cat_columns . [&#39;term&#39;, &#39;int_rate&#39;, &#39;grade&#39;, &#39;sub_grade&#39;, &#39;emp_title&#39;, &#39;emp_length&#39;, &#39;home_ownership&#39;, &#39;verification_status&#39;, &#39;issue_d&#39;, &#39;pymnt_plan&#39;, &#39;desc&#39;, &#39;purpose&#39;, &#39;title&#39;, &#39;zip_code&#39;, &#39;addr_state&#39;, &#39;earliest_cr_line&#39;, &#39;revol_util&#39;, &#39;initial_list_status&#39;, &#39;last_pymnt_d&#39;, &#39;last_credit_pull_d&#39;, &#39;application_type&#39;, &#39;hardship_flag&#39;] . Strip leading and trailing space of each categorical column: . for i in cat_columns: df2[i] = df2[i].str.strip() . Quickly explore each each categorical feature and check the frequency: . pd.set_option(&#39;display.max_columns&#39;, None) df2[cat_columns].head(5) . term int_rate grade sub_grade emp_title emp_length home_ownership verification_status issue_d pymnt_plan desc purpose title zip_code addr_state earliest_cr_line revol_util initial_list_status last_pymnt_d last_credit_pull_d application_type hardship_flag . 0 36 months | 9.99% | B | B4 | TCVCG | &lt; 1 year | RENT | Not Verified | 10-Dec | n | Borrower added on 12/06/10 &gt; I would like to c... | debt_consolidation | tracy | 071xx | NJ | Jan-74 | 80.10% | f | 11-Jan | 11-Jan | Individual | N | . 1 36 months | 13.72% | C | C5 | Santa Fe Springs Storage Park | 10+ years | RENT | Verified | 10-Dec | n | Borrower added on 12/06/10 &gt; I am planning on ... | credit_card | Bye Bye BofA | 906xx | CA | Sep-96 | 57.50% | f | 12-Jun | 12-Jul | Individual | N | . 2 60 months | 16.69% | E | E3 | NaN | NaN | RENT | Verified | 10-Dec | n | Borrower added on 12/08/10 &gt; the funds are goi... | other | rita loan | 330xx | FL | 2-Jun | 65.10% | f | 15-Sep | 15-Sep | Individual | N | . 3 60 months | 12.23% | C | C1 | Norfolk Southern Corp. | 10+ years | MORTGAGE | Verified | 10-Dec | n | NaN | credit_card | connie1958 | 450xx | OH | Jun-90 | 68.70% | f | 13-Oct | 13-Nov | Individual | N | . 4 36 months | 12.98% | C | C3 | Eaton drilling | 4 years | MORTGAGE | Not Verified | 10-Dec | n | NaN | debt_consolidation | Bill help | 956xx | CA | 7-Mar | 33.60% | f | 12-Sep | 16-Oct | Individual | N | . Check frequency: . for i in cat_columns: print(df2[i].value_counts().to_frame()) #for seperate columns: #df2[&#39;emp_title&#39;].value_counts() . term 36 months 14433 60 months 2815 int_rate 7.88% 725 7.51% 486 11.86% 392 7.14% 341 11.49% 336 ... ... 16.15% 1 16.40% 1 16.96% 1 17.46% 1 16.01% 1 [268 rows x 1 columns] grade B 5251 A 4082 C 4005 D 2429 E 1066 F 309 G 106 sub_grade A5 1492 B5 1266 B4 1168 A4 1104 B3 1041 .. ... F5 26 G2 25 G4 25 G3 15 G5 13 [35 rows x 1 columns] emp_title US Army 49 Bank of America 41 Self Employed 35 Self 29 US Air Force 29 ... ... Flo tv, Qualcomm 1 University of California Santa Cruz 1 Colorado Business Bank 1 ANS Marketing LLC 1 Joseph Productions, Inc. 1 [13485 rows x 1 columns] emp_length 10+ years 3362 &lt; 1 year 2354 2 years 2191 3 years 1900 1 year 1635 4 years 1475 5 years 1359 6 years 798 7 years 696 8 years 633 9 years 557 home_ownership RENT 8599 MORTGAGE 7165 OWN 1384 OTHER 97 NONE 3 verification_status Not Verified 9719 Verified 4723 Source Verified 2806 issue_d 10-Oct 1138 10-Nov 1121 10-Jul 1119 10-Sep 1087 10-Aug 1078 ... ... 7-Aug 33 8-Sep 32 7-Jul 30 7-Sep 18 7-Jun 1 [43 rows x 1 columns] pymnt_plan n 17248 desc 207 Camping Membership 6 Debt Consolidation 6 Personal Loan 4 credit card debt consolidation 3 ... ... Borrower added on 05/06/10 &gt; With this loan we ... 1 Borrower added on 04/15/10 &gt; I will be selling ... 1 Borrower added on 03/08/10 &gt; I have been in my ... 1 4,000.00 will be used to pay off my Veterinaria... 1 I am currently paying off my credit cards but w... 1 [13328 rows x 1 columns] purpose debt_consolidation 7617 credit_card 2201 other 2066 home_improvement 1244 major_purchase 1070 small_business 827 car 576 wedding 427 educational 325 medical 289 moving 266 house 159 vacation 138 renewable_energy 43 title Debt Consolidation 709 Personal Loan 318 Debt Consolidation Loan 228 Consolidation 194 debt consolidation 152 ... ... In need of a used car for transportation 1 Debit Consol 1 NY 1 cc pay off 1 DEBT FREE LOAN 1 [11114 rows x 1 columns] zip_code 100xx 290 945xx 244 112xx 222 900xx 221 606xx 214 ... ... 094xx 1 859xx 1 838xx 1 833xx 1 499xx 1 [783 rows x 1 columns] addr_state CA 3099 NY 1716 FL 1216 TX 1191 NJ 826 .. ... IN 9 ID 6 NE 5 IA 5 ME 3 [50 rows x 1 columns] earliest_cr_line Oct-99 172 Dec-98 160 Dec-97 154 Dec-95 151 Nov-98 148 ... ... Aug-73 1 Nov-61 1 Feb-72 1 Oct-74 1 Jun-70 1 [481 rows x 1 columns] revol_util 0% 497 0.50% 33 0.20% 32 35.30% 32 23.80% 31 ... ... 10.17% 1 56.26% 1 70.94% 1 5.79% 1 7.28% 1 [1030 rows x 1 columns] initial_list_status f 17248 last_pymnt_d 13-Mar 555 13-May 489 13-Feb 479 12-Mar 477 11-Mar 454 ... ... 8-Mar 5 8-Jan 4 16-Feb 2 16-Jul 1 8-Feb 1 [99 rows x 1 columns] last_credit_pull_d 19-May 3260 16-Oct 1260 19-Apr 391 13-Feb 315 19-Mar 310 ... ... 8-Jul 1 7-Jul 1 8-Jun 1 7-May 1 7-Dec 1 [141 rows x 1 columns] application_type Individual 17248 hardship_flag N 17248 . After the exploratory analysis, we have a few findings: . &#39;desc&#39; is not relevant to the mdoel build and should be dropped . | Drop all date columns to simplify the model build in this case. If we have more time, we can do some feature engineering by using date features, e.g. df_loan[&#39;issue_to_earliest_cr_line&#39;] = df_loan[&#39;issue_d&#39;] - df_loan[&#39;earliest_cr_line&#39;] . | &#39;pmnt_plan&#39;,&#39;hardship_flag&#39;,&#39;initial_list_status&#39; and &#39;application_type&#39; only have constant values and are useless for model build . | &#39;emp_title&#39;, &#39;zip_code&#39;, and &#39;title&#39; have too many unique values and are not informative, we should drop them . | To simplify the analysis, we only keep &#39;grade&#39; and drop &#39;sub_grade&#39; . | Also, some types of information filled in by customer are very difficult to verify (customers can put whatever they want to). To simplify the analysis for this case, we should drop these columns that cann&#39;t be easily verified: &#39;emp_length&#39;, &#39;purpose&#39; . | Finally, we create a list including all categorical columns should be dropped . drop_feature=[&#39;desc&#39;,&#39;issue_d&#39;,&#39;last_pymnt_d&#39;,&#39;last_credit_pull_d&#39;,&#39;earliest_cr_line&#39;, &#39;pymnt_plan&#39;,&#39;hardship_flag&#39;, &#39;emp_title&#39;, &#39;emp_length&#39;, &#39;zip_code&#39;,&#39;title&#39;, &#39;purpose&#39;,&#39;sub_grade&#39;,&#39;initial_list_status&#39;,&#39;application_type&#39;] . df2 = df2.drop(columns=drop_feature) . df2.head(5) . Unnamed: 0 loan_amnt term int_rate installment grade home_ownership annual_inc verification_status loan_status addr_state dti delinq_2yrs inq_last_6mths mths_since_last_delinq open_acc pub_rec revol_bal revol_util total_acc policy_code pub_rec_bankruptcies target . 0 22545 | 8500.0 | 36 months | 9.99% | 274.24 | B | RENT | 19200.0 | Not Verified | Fully Paid | NJ | 15.56 | 0.0 | 0.0 | NaN | 7.0 | 1.0 | 8011.0 | 80.10% | 8.0 | 1.0 | 1.0 | 0 | . 1 22546 | 19000.0 | 36 months | 13.72% | 646.80 | C | RENT | 35576.0 | Verified | Fully Paid | CA | 13.50 | 0.0 | 0.0 | NaN | 9.0 | 0.0 | 11219.0 | 57.50% | 12.0 | 1.0 | 0.0 | 0 | . 2 22547 | 4000.0 | 60 months | 16.69% | 98.75 | E | RENT | 14500.0 | Verified | Fully Paid | FL | 12.25 | 0.0 | 0.0 | NaN | 3.0 | 0.0 | 4364.0 | 65.10% | 7.0 | 1.0 | 0.0 | 0 | . 3 22548 | 24250.0 | 60 months | 12.23% | 395.23 | C | MORTGAGE | 70000.0 | Verified | Fully Paid | OH | 23.79 | 0.0 | 2.0 | NaN | 6.0 | 0.0 | 31061.0 | 68.70% | 25.0 | 1.0 | 0.0 | 0 | . 4 22549 | 3600.0 | 36 months | 12.98% | 121.27 | C | MORTGAGE | 65000.0 | Not Verified | Charged Off | CA | 12.41 | 0.0 | 1.0 | NaN | 7.0 | 0.0 | 1446.0 | 33.60% | 8.0 | 1.0 | 0.0 | 1 | . remaining_cat_fea=[i for i in cat_columns if i not in drop_feature] remaining_cat_fea . [&#39;term&#39;, &#39;int_rate&#39;, &#39;grade&#39;, &#39;home_ownership&#39;, &#39;verification_status&#39;, &#39;addr_state&#39;, &#39;revol_util&#39;] . 2.6 Encode remaining categorical features . Convert &#39;revol_util&#39; into a numerical feature: . df2[&#39;revol_util&#39;].value_counts() . 0% 497 0.50% 33 0.20% 32 35.30% 32 23.80% 31 ... 10.17% 1 56.26% 1 70.94% 1 5.79% 1 7.28% 1 Name: revol_util, Length: 1030, dtype: int64 . df2[&#39;revol_util&#39;] = df2[&#39;revol_util&#39;].str.replace(&#39;%&#39;, &#39;&#39;).astype(float)/100 df2[&#39;revol_util&#39;].value_counts() . 0.0000 497 0.0050 33 0.3530 32 0.0020 32 0.2380 31 ... 0.0579 1 0.0004 1 0.1163 1 0.0728 1 0.3326 1 Name: revol_util, Length: 1030, dtype: int64 . Convert &#39;int_rate&#39; into a numerical feature: | Convert &#39;verification_status&#39; into a numerical feature: if &#39;Not Verified&#39; then 0, else 1 | df2[&#39;int_rate&#39;] = df2[&#39;int_rate&#39;].str.replace(&#39;%&#39;, &#39;&#39;).astype(float)/100 df2[&#39;int_rate&#39;].value_counts() . 0.0788 725 0.0751 486 0.1186 392 0.0714 341 0.1149 336 ... 0.1746 1 0.1836 1 0.1467 1 0.1601 1 0.1640 1 Name: int_rate, Length: 268, dtype: int64 . df2[&#39;verification_status&#39;].value_counts() . Not Verified 9719 Verified 4723 Source Verified 2806 Name: verification_status, dtype: int64 . df2[&#39;verification_status&#39;] = df2[&#39;verification_status&#39;].apply(lambda x: 0 if x == &#39;Not Verified&#39; else 1) df2[&#39;verification_status&#39;].value_counts() . 0 9719 1 7529 Name: verification_status, dtype: int64 . Check remaining categorical features: . list(df2.select_dtypes(include=[&#39;object&#39;]).columns) . [&#39;term&#39;, &#39;grade&#39;, &#39;home_ownership&#39;, &#39;loan_status&#39;, &#39;addr_state&#39;] . Remove &#39;loan_status&#39;: . df2=df2.drop(columns=[&#39;loan_status&#39;]) . 2.7 One-hot encoding for remaining categorical features: . Most machine learning models unfortunately cannot deal with categorical variables . There are two mains to encode categorical variables: . Label encoding: assign each unique category in a categorical variable with an integer. . No new columns are created. . | One-hot encoding: create a new column for each unique category in a categorical variable. . Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns. . | def cate_convert(df, nan_as_category = True): original_columns = list(df.columns) categorical_columns = [col for col in df.columns if df[col].dtype == &#39;object&#39;] df = pd.get_dummies(df, columns = categorical_columns, dummy_na= nan_as_category) new_columns = [c for c in df.columns if c not in original_columns] return df, new_columns . df2,cat_cols = cate_convert(df2, nan_as_category = True) . df2.head() . Unnamed: 0 loan_amnt int_rate installment annual_inc verification_status dti delinq_2yrs inq_last_6mths mths_since_last_delinq open_acc pub_rec revol_bal revol_util total_acc policy_code pub_rec_bankruptcies target term_36 months term_60 months term_nan grade_A grade_B grade_C grade_D grade_E grade_F grade_G grade_nan home_ownership_MORTGAGE home_ownership_NONE home_ownership_OTHER home_ownership_OWN home_ownership_RENT home_ownership_nan addr_state_AK addr_state_AL addr_state_AR addr_state_AZ addr_state_CA addr_state_CO addr_state_CT addr_state_DC addr_state_DE addr_state_FL addr_state_GA addr_state_HI addr_state_IA addr_state_ID addr_state_IL addr_state_IN addr_state_KS addr_state_KY addr_state_LA addr_state_MA addr_state_MD addr_state_ME addr_state_MI addr_state_MN addr_state_MO addr_state_MS addr_state_MT addr_state_NC addr_state_NE addr_state_NH addr_state_NJ addr_state_NM addr_state_NV addr_state_NY addr_state_OH addr_state_OK addr_state_OR addr_state_PA addr_state_RI addr_state_SC addr_state_SD addr_state_TN addr_state_TX addr_state_UT addr_state_VA addr_state_VT addr_state_WA addr_state_WI addr_state_WV addr_state_WY addr_state_nan . 0 22545 | 8500.0 | 0.0999 | 274.24 | 19200.0 | 0 | 15.56 | 0.0 | 0.0 | NaN | 7.0 | 1.0 | 8011.0 | 0.801 | 8.0 | 1.0 | 1.0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 22546 | 19000.0 | 0.1372 | 646.80 | 35576.0 | 1 | 13.50 | 0.0 | 0.0 | NaN | 9.0 | 0.0 | 11219.0 | 0.575 | 12.0 | 1.0 | 0.0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 22547 | 4000.0 | 0.1669 | 98.75 | 14500.0 | 1 | 12.25 | 0.0 | 0.0 | NaN | 3.0 | 0.0 | 4364.0 | 0.651 | 7.0 | 1.0 | 0.0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 22548 | 24250.0 | 0.1223 | 395.23 | 70000.0 | 1 | 23.79 | 0.0 | 2.0 | NaN | 6.0 | 0.0 | 31061.0 | 0.687 | 25.0 | 1.0 | 0.0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 22549 | 3600.0 | 0.1298 | 121.27 | 65000.0 | 0 | 12.41 | 0.0 | 1.0 | NaN | 7.0 | 0.0 | 1446.0 | 0.336 | 8.0 | 1.0 | 0.0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2.8 Missing value imputation . missing=missing_values_table(df2) missing . Missing Values % of Total Values . mths_since_last_delinq 10816 | 62.708720 | . revol_util 40 | 0.231911 | . pub_rec_bankruptcies 697 | 4.041048 | . df2[&#39;pub_rec_bankruptcies&#39;].fillna(value=0,inplace=True) . Practice: . impute 0 for [&#39;revol_util&#39;]&#39;s missing values | impute 0 for [&#39;mths_since_last_delinq&#39;]&#39;s missing values | df2[&#39;revol_util&#39;].fillna(value=0,inplace=True) df2[&#39;mths_since_last_delinq&#39;].fillna(value=0,inplace=True) . missing=missing_values_table(df2) missing . Missing Values % of Total Values . Backup the dataset: . final=df2.copy() df2.to_csv(&#39;final_dataset_for_model.csv&#39;) . Step Three: ML Modelling . 3.1 Check Correlations . correlations = final.corr()[&#39;target&#39;].dropna().sort_values(ascending = False) print(&#39;Top Positive Correlations: n&#39;, correlations.head(15)) print(&#39; nTop Negative Correlations: n&#39;, correlations.tail(15)) . Top Positive Correlations: target 1.000000 int_rate 0.166554 term_60 months 0.107015 revol_util 0.082058 inq_last_6mths 0.079574 grade_E 0.074894 grade_D 0.074011 grade_F 0.061832 pub_rec 0.056425 pub_rec_bankruptcies 0.049313 grade_G 0.041399 addr_state_FL 0.033128 grade_C 0.032578 loan_amnt 0.031894 dti 0.031063 Name: target, dtype: float64 Top Negative Correlations: addr_state_DC -0.014116 addr_state_IL -0.014298 addr_state_VA -0.014718 addr_state_WY -0.015859 addr_state_MA -0.017767 home_ownership_MORTGAGE -0.018336 addr_state_NY -0.018688 addr_state_TX -0.019586 addr_state_OH -0.020771 addr_state_PA -0.023690 total_acc -0.024317 annual_inc -0.026927 grade_B -0.029660 term_36 months -0.107015 grade_A -0.130161 Name: target, dtype: float64 . 3.2 Split into Test and Train datasets . x=final.drop(columns=&#39;target&#39;) y=final[&#39;target&#39;] . x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0) print(x_train.shape) print(y_train.shape) print(x_test.shape) print(y_test.shape) . (12936, 85) (12936,) (4312, 85) (4312,) . 3.3 Logistic regression model . 1.Create the classifier(object) . logist= LogisticRegression() . 2.Train the model on training data . logist.fit(x_train, y_train) . LogisticRegression() . 3.Make the prediction: . Now that the model has been trained, we can use it to make predictions. . We want to predict the probabilities of not paying a loan, so we use the model predict_proba method. . The first column is the probability of the target being 0 and the second column is the probability of the target being 1 . log_reg_pred = logist.predict_proba(x_test) y_pred_proba=log_reg_pred[:,1] y_pred_proba . array([0.07169809, 0.10618282, 0.11190762, ..., 0.08849368, 0.15391882, 0.19361028]) . Predict the label: . .predict() is for predicting class labels: . scikit-learn is using a threshold of P&gt;0.5 for binary classifications . y_pred = logist.predict(x_test) y_pred . array([0, 0, 0, ..., 0, 0, 0]) . Get the coefficient list: . coefficients = pd.concat([pd.DataFrame(list(x_train.columns)),pd.DataFrame(np.transpose(logist.coef_))], axis = 1) coefficients . 0 0 . 0 Unnamed: 0 | -0.000047 | . 1 loan_amnt | 0.000010 | . 2 int_rate | 0.000101 | . 3 installment | 0.000126 | . 4 annual_inc | -0.000005 | . ... ... | ... | . 80 addr_state_WA | 0.000036 | . 81 addr_state_WI | -0.000012 | . 82 addr_state_WV | -0.000017 | . 83 addr_state_WY | -0.000027 | . 84 addr_state_nan | 0.000000 | . 85 rows × 2 columns . Check key metrics . An ROC curve is a plot of True Positive Rate vs False Positive Rate where False Positive Rate=FP/(TN+FP) =1-Specificity. . 4.Show the ROC_CURVE to evaluate the model performance . import numpy as np [fpr, tpr, thr] = metrics.roc_curve(y_test, y_pred_proba) idx = np.min(np.where(tpr &gt; 0.95)) # index of the first threshold for which the sensibility &gt; 0.95 plt.figure() plt.plot(fpr, tpr, color=&#39;coral&#39;, label=&#39;ROC curve (area = %0.3f)&#39; % metrics.auc(fpr, tpr)) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot([0, fpr[idx]], [tpr[idx], tpr[idx]], &#39;k--&#39;, color=&#39;blue&#39;) plt.plot([fpr[idx], fpr[idx]], [0, tpr[idx]], &#39;k--&#39;, color=&#39;blue&#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;False Positive Rate (1 - specificity)&#39;, fontsize=14) plt.ylabel(&#39;True Positive Rate (recall)&#39;, fontsize=14) plt.title(&#39;Receiver operating characteristic (ROC) curve&#39;) plt.legend(loc=&quot;lower right&quot;) plt.show() . AUROC (Area Under the Receiver Operating Characteristics) The more the area enclosed by the ROC curve, the better it is. The area under the curve can lie between 0 and 1. The closer it is to 1, the better it is . 3.4 Random Forest . Ensemble learning, in general, is a model that makes predictions based on a number of different models. By combining individual models, the ensemble model tends to be more flexible (less bias) and less data-sensitive (less variance) Two most popular ensemble methods are bagging and boosting. . Bagging: Training a bunch of individual models in a parallel way. Each model is trained by a random subset of the data =&gt; bootstrapping the data plus using the aggregate to make a decision is called bagging! . Random forest is an ensemble model using bagging as the ensemble method and decision tree as the individual model. | . Boosting: Training a bunch of individual models in a sequential way. Each individual model learns from mistakes made by the previous model . Gradient Boosting: GBT build trees one at a time, where each new tree helps to correct errors made by previously trained tree. GBT build trees one at a time, where each new tree helps to correct errors made by previously trained tree. | . 1.Create the classifier(object): . rf_model = RandomForestClassifier( n_estimators=200, max_depth=5) . 2.Train the model on training data: . rf_model.fit(x_train, y_train) . RandomForestClassifier(max_depth=5, n_estimators=200) . 3.Make the prediction: . rf_model_pred = rf_model.predict_proba(x_test) y_pred_proba=rf_model_pred[:,1] y_pred_proba . array([0.05757971, 0.13336961, 0.14290489, ..., 0.17055502, 0.19339283, 0.15164095]) . 4.Show the ROC_CURVE to evaluate the model performance: . import numpy as np [fpr, tpr, thr] = metrics.roc_curve(y_test, y_pred_proba) idx = np.min(np.where(tpr &gt; 0.95)) # index of the first threshold for which the sensibility &gt; 0.95 plt.figure() plt.plot(fpr, tpr, color=&#39;coral&#39;, label=&#39;ROC curve (area = %0.3f)&#39; % metrics.auc(fpr, tpr)) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot([0, fpr[idx]], [tpr[idx], tpr[idx]], &#39;k--&#39;, color=&#39;blue&#39;) plt.plot([fpr[idx], fpr[idx]], [0, tpr[idx]], &#39;k--&#39;, color=&#39;blue&#39;) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;False Positive Rate (1 - specificity)&#39;, fontsize=14) plt.ylabel(&#39;True Positive Rate (recall)&#39;, fontsize=14) plt.title(&#39;Receiver operating characteristic (ROC) curve&#39;) plt.legend(loc=&quot;lower right&quot;) plt.show() . Get the feature importances for each feature using the following code: . rf_model.feature_importances_ . array([4.61377971e-02, 2.97105362e-02, 1.62231988e-01, 3.89757400e-02, 7.30151978e-02, 2.98122869e-03, 2.98329504e-02, 3.61074199e-03, 4.64316499e-02, 2.09856563e-02, 2.51282204e-02, 1.59478493e-02, 2.81354984e-02, 5.98684273e-02, 3.44167034e-02, 0.00000000e+00, 9.33163593e-03, 4.70999082e-02, 6.17266104e-02, 0.00000000e+00, 9.07662583e-02, 1.07515734e-02, 7.77978981e-03, 2.16740452e-02, 2.50193546e-02, 1.96116262e-02, 8.08726743e-03, 0.00000000e+00, 5.54558102e-03, 0.00000000e+00, 1.35893766e-03, 3.18257423e-03, 3.12336399e-03, 0.00000000e+00, 7.65486750e-04, 5.06130129e-04, 4.38679604e-04, 7.66545059e-04, 4.47092865e-03, 7.14533917e-04, 8.97249147e-04, 6.27451235e-04, 8.43911819e-04, 9.69664434e-03, 3.10051500e-03, 2.98018765e-03, 0.00000000e+00, 7.03180003e-05, 1.16219812e-03, 1.30180874e-05, 7.13557020e-04, 3.93950967e-04, 1.94544262e-03, 1.01483709e-03, 9.23024250e-04, 0.00000000e+00, 1.40851297e-03, 2.26232470e-03, 1.73190385e-03, 4.52148511e-05, 1.53059390e-04, 2.28931752e-03, 1.63859973e-03, 4.41690787e-04, 2.39725564e-03, 1.72947319e-03, 5.58364977e-03, 1.73622330e-03, 6.18965000e-04, 5.74647881e-04, 1.28336584e-03, 2.23536273e-03, 5.99216480e-04, 7.43410399e-04, 1.11160364e-04, 1.80089494e-04, 1.47540630e-03, 1.56457743e-03, 1.29073499e-03, 0.00000000e+00, 1.20681896e-03, 1.02738431e-03, 9.46209997e-04, 2.12103170e-04, 0.00000000e+00]) . Associate these feature importances with the corresponding features: . feature_importance_df = pd.DataFrame(list(zip(rf_model.feature_importances_, list(x_train.columns)))) feature_importance_df.columns = [&#39;feature.importance&#39;, &#39;feature&#39;] feature_importance_df.sort_values(by=&#39;feature.importance&#39;, ascending=False).head(20) # only show top 20 . feature.importance feature . 2 0.162232 | int_rate | . 20 0.090766 | grade_A | . 4 0.073015 | annual_inc | . 18 0.061727 | term_60 months | . 13 0.059868 | revol_util | . 17 0.047100 | term_36 months | . 8 0.046432 | inq_last_6mths | . 0 0.046138 | Unnamed: 0 | . 3 0.038976 | installment | . 14 0.034417 | total_acc | . 6 0.029833 | dti | . 1 0.029711 | loan_amnt | . 12 0.028135 | revol_bal | . 10 0.025128 | open_acc | . 24 0.025019 | grade_E | . 23 0.021674 | grade_D | . 9 0.020986 | mths_since_last_delinq | . 25 0.019612 | grade_F | . 11 0.015948 | pub_rec | . 21 0.010752 | grade_B | .",
            "url": "https://joery15.github.io/workshop/2021/01/05/ML_Model1.html",
            "relUrl": "/2021/01/05/ML_Model1.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Data visualization--Analysis Distribution of Risk",
            "content": "import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt . data = pd.read_csv(&quot;datasets_credit_labelled.csv&quot;) data[&quot;Checking account&quot;]=data[&quot;Checking account&quot;].fillna(&quot;None&quot;) data[&quot;Saving accounts&quot;]=data[&quot;Saving accounts&quot;].fillna(&quot;None&quot;) data.head() . Unnamed: 0 Age Sex Job Housing Saving accounts Checking account Credit amount Duration Purpose Risk . 0 0 | 67 | male | 2 | own | None | little | 1169 | 6 | radio/TV | good | . 1 1 | 22 | female | 2 | own | little | moderate | 5951 | 48 | radio/TV | bad | . 2 2 | 49 | male | 1 | own | little | None | 2096 | 12 | education | good | . 3 3 | 45 | male | 2 | free | little | little | 7882 | 42 | furniture/equipment | good | . 4 4 | 53 | male | 2 | free | little | little | 4870 | 24 | car | bad | . E.g. Age Group [18-25):Student, [25,35):Yonge, [35,60):Adult, [60, ): Senior] . For Job: 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled . Use charts to visulize . Age Group to Risk | Gender to Risk | Housing to Risk | Job to Risk | Saving Accounts to Risk | Credit Amount to Housing-boxplot | Job to Credit Amount | Age to Credit Amount | Any other things that revelent to analysis the relations between any of the columns | Correlation between them | data[&quot;Age_group&quot;]=pd.cut(data[&quot;Age&quot;], [18,25,35,60,100], labels=[&quot;Student&quot;,&quot;Yonge&quot;,&quot;Adult&quot;,&quot;Senior&quot;]) . plt.figure(figsize=(10,5)) sns.countplot(x=&quot;Age_group&quot;, hue=&quot;Risk&quot;, data=data) plt.show() . From the first bar chart, it can be inferred that the company should target on the group of &quot;Yong&quot; and &quot;Adult&quot;. These two groups occupied the largest amount of customers. When compared with relatives between good and bad risk(the orange and blue bar), the&quot;student&quot; group is highly risky while the &quot;Senior&quot; one is more reliable. . sns.countplot(x=&quot;Sex&quot;, hue=&quot;Risk&quot;, data=data, palette=&quot;pastel&quot;) plt.show() . As we can see from the bar chart, the major gender of their customer is male. And the man has more relative risk than women. For the male customer, the good risk amount in the male group is more than double the bad risk, while it is just double in the female group. . sns.countplot(x=&quot;Housing&quot;, hue=&quot;Risk&quot;, data=data,palette=&quot;ch:.25&quot;) plt.show() . From the overall bar in three groups, the group of own housing is the majority of our customers. Furthermore, this kind of person has a relatively good risk than other groups, when compared with the bad risk. On the opposite, the people who do not have any house are the riskiest among the three groups. . data[&quot;Job_group&quot;]=pd.cut(data[&quot;Job&quot;], [-1,0.5,1.5,2.5,3.5], labels=[&quot;unskilled and non-resident&quot;,&quot;unskilled and resident&quot;,&quot;skilled&quot;,&quot;highly skilled&quot;]) plt.figure(figsize=(10,5)) sns.countplot(x=&quot;Job_group&quot;, hue=&quot;Risk&quot;, data=data, palette=&quot;Set3&quot;) plt.show() . Based on the chart, the skilled group occupied the largest group of job. The risk of people in different job levels on good risks relatively higher than that on bad risk. . sns.countplot(x=&quot;Saving accounts&quot;, hue=&quot;Risk&quot;, data=data, palette=&quot;husl&quot;) plt.show() . From what we can see from the diagram, most borrowers have little or no money on saving accounts. But those quite rich and rich people are the safe group, whose good risk is higher relatively than bad risk. . sns.catplot(x=&quot;Housing&quot;, y=&quot;Credit amount&quot;, kind=&quot;box&quot;, data=data, palette=&quot;husl&quot;) plt.show() . From this boxplot, these people who do not have a house to live in are more likely to borrow a high amount of money. Other stable people (own a house or rent a house ) would be at a lower amount of credit. . sns.catplot(x=&quot;Job_group&quot;, y=&quot;Credit amount&quot;, kind=&quot;swarm&quot;,data=data, height=5, aspect=1.5) plt.show() . /usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1296: UserWarning: 15.5% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) /usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1296: UserWarning: 42.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) . As we can see clearly from the boxplot, High skilled people will borrow a higher amount of money to satisfied their needs. For people who do not have skills, they would live at a lower live standard and borrow a lower credit amount. They should targeted on the skilled person. . sns.catplot(x=&quot;Age_group&quot;, y=&quot;Credit amount&quot;, kind=&quot;box&quot;, data=data, showfliers = False, palette=&quot;pastel&quot;) plt.show() . The group of younger and adult would borrow a higher amount than that of student and senior. From these four groups, young people would borrow the highest amount of money, while the senior would borrow the lowest one. . #Job, Credit Amount to Risk data[&quot;Checking account&quot;]=data[&quot;Checking account&quot;].fillna(&quot;none&quot;) sns.catplot(x=&quot;Job&quot;, y=&quot;Credit amount&quot;, hue=&quot;Risk&quot;, kind=&quot;box&quot;, data=data, palette=&quot;Set1&quot;) plt.show() . From the job type of 2 and 3, we can infer from the diagram that the high amount they borrow, the worse risk the debit would be. This principle would be the opposite of other groups. Moreover, a highly skilled group (job_type3) is highest on the credit amount. . data=data.merge(pd.get_dummies(data[&quot;Sex&quot;], drop_first=True, prefix=&quot;Sex&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Housing&quot;], drop_first=True, prefix=&quot;Housing&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Saving accounts&quot;], drop_first=True, prefix=&quot;Saving accounts&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Checking account&quot;], drop_first=True, prefix=&quot;Checking account&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Purpose&quot;], drop_first=True, prefix=&quot;Purpose&quot;),left_index=True, right_index=True) data=data.merge(pd.get_dummies(data[&quot;Risk&quot;], drop_first=True, prefix=&quot;Risk&quot;),left_index=True, right_index=True) # remove the categorical variable data.drop(columns=[&quot;Sex&quot;, &quot;Housing&quot;, &quot;Saving accounts&quot;, &quot;Checking account&quot;, &quot;Purpose&quot;, &quot;Risk&quot;,&quot;Job_group&quot;,&quot;Age_group&quot;],inplace=True) . plt.figure(figsize=(15,15)) sns.heatmap(data.astype(float).corr(), mask=np.triu(data.astype(float).corr()), cmap = sns.diverging_palette(230, 20, as_cmap=True), annot=True, fmt=&#39;.1g&#39;, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa6897d8940&gt; . From the correlation heatmap, it can be seen that the duration has a strong positive relationship with the credit amount. The higher the credit amount, the longer the duration term. And the credit amount has a week positive relation with the job, which means that the more skilled people will borrow a higher amount of money. While for other negative relationships (-0.7,-0.4), these happen in the same category which is exclusive to others. .",
            "url": "https://joery15.github.io/workshop/2021/01/05/Data-visualization.html",
            "relUrl": "/2021/01/05/Data-visualization.html",
            "date": " • Jan 5, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://joery15.github.io/workshop/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://joery15.github.io/workshop/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Driven one-year studies Master of Artificial Intelligence from Schulich School of Busines. Excellent communication capabilities. Experience with Convolutional Neural Network (CNN), and specific CNN frameworks like AlexNet and ResNet. Currently seeking a working opportunity in the data-related field. Driven to bring extensive theoretical and practical knowledge of Artificial Intelligence. Passionate about emerging technologies and business intelligence. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://joery15.github.io/workshop/www.linkedin.com/in/luo-jiayi/",
          "relUrl": "/www.linkedin.com/in/luo-jiayi/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joery15.github.io/workshop/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}