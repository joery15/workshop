{
  
    
        "post0": {
            "title": "1. Load the data",
            "content": "0. Background Info . kaggle&#39;s case: . Titanic - Machine Learning from Disaster . information link:https://www.kaggle.com/c/titanic/overview . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, f1_score,roc_auc_score from sklearn import tree from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier from sklearn.linear_model import LogisticRegression,SGDClassifier from sklearn.naive_bayes import GaussianNB from xgboost import XGBClassifier . data1 = pd.read_csv(&quot;train.csv&quot;) data2 = pd.read_csv(&quot;test.csv&quot;) data3 = pd.read_csv(&quot;gender_submission.csv&quot;) data4=pd.merge(data3,data2) data=pd.concat([data1,data4],axis=0) data=data.reset_index() data.head() . index PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 0 | 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 1 | 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 2 | 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 3 | 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 4 | 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . 2. Pre-process the data (aka data wrangling) . 1, Data cleanning . data.drop([&#39;PassengerId&#39;,&#39;Cabin&#39;,&#39;Ticket&#39;],axis=1,inplace=True) data.head() . index Survived Pclass Name Sex Age SibSp Parch Fare Embarked . 0 0 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | 7.2500 | S | . 1 1 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | 71.2833 | C | . 2 2 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | 7.9250 | S | . 3 3 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 53.1000 | S | . 4 4 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 8.0500 | S | . 2, Identification and treatment of missing values and outliers. . data.isnull().sum() . index 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 263 SibSp 0 Parch 0 Fare 1 Embarked 2 dtype: int64 . data[data[&#39;Embarked&#39;].isnull()] . index Survived Pclass Name Sex Age SibSp Parch Fare Embarked . 61 61 | 1 | 1 | Icard, Miss. Amelie | female | 38.0 | 0 | 0 | 80.0 | NaN | . 829 829 | 1 | 1 | Stone, Mrs. George Nelson (Martha Evelyn) | female | 62.0 | 0 | 0 | 80.0 | NaN | . #https://www.encyclopedia-titanica.org/titanic-survivor/martha-evelyn-stone.html data[&#39;Embarked&#39;] = data[&#39;Embarked&#39;].fillna(&#39;S&#39;) . data.corr() . index Survived Pclass Age SibSp Parch Fare . index 1.000000 | 0.001504 | -0.018212 | 0.012723 | -0.027343 | 0.003911 | -0.003723 | . Survived 0.001504 | 1.000000 | -0.264710 | -0.053695 | 0.002370 | 0.108919 | 0.233622 | . Pclass -0.018212 | -0.264710 | 1.000000 | -0.408106 | 0.060832 | 0.018322 | -0.558629 | . Age 0.012723 | -0.053695 | -0.408106 | 1.000000 | -0.243699 | -0.150917 | 0.178740 | . SibSp -0.027343 | 0.002370 | 0.060832 | -0.243699 | 1.000000 | 0.373587 | 0.160238 | . Parch 0.003911 | 0.108919 | 0.018322 | -0.150917 | 0.373587 | 1.000000 | 0.221539 | . Fare -0.003723 | 0.233622 | -0.558629 | 0.178740 | 0.160238 | 0.221539 | 1.000000 | . data[data[&#39;Fare&#39;].isnull()] . index Survived Pclass Name Sex Age SibSp Parch Fare Embarked . 1043 152 | 0 | 3 | Storey, Mr. Thomas | male | 60.5 | 0 | 0 | NaN | S | . data[&#39;Fare&#39;] = data[&#39;Fare&#39;].fillna(data.groupby([&#39;Pclass&#39;])[&#39;Fare&#39;].mean()[3]) . #so fill the age with mean value data[&#39;Age&#39;].fillna(data[&#39;Age&#39;].mean(), inplace = True) . data.describe() . index Survived Pclass Age SibSp Parch Fare . count 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | 1309.000000 | . mean 369.478992 | 0.377387 | 2.294882 | 29.881138 | 0.498854 | 0.385027 | 33.280206 | . std 248.767105 | 0.484918 | 0.837836 | 12.883193 | 1.041658 | 0.865560 | 51.741830 | . min 0.000000 | 0.000000 | 1.000000 | 0.170000 | 0.000000 | 0.000000 | 0.000000 | . 25% 163.000000 | 0.000000 | 2.000000 | 22.000000 | 0.000000 | 0.000000 | 7.895800 | . 50% 327.000000 | 0.000000 | 3.000000 | 29.881138 | 0.000000 | 0.000000 | 14.454200 | . 75% 563.000000 | 1.000000 | 3.000000 | 35.000000 | 1.000000 | 0.000000 | 31.275000 | . max 890.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 9.000000 | 512.329200 | . sns.boxplot(x=&quot;Survived&quot;, y=&quot;Fare&quot;, data=data) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f450096a080&gt; . data.drop(data[data.Fare &gt; 400].index, inplace=True) . 3, Feature engineering . table1=pd.get_dummies(data[&#39;Sex&#39;]) data=pd.concat([data, table1], axis=1) . table2=pd.get_dummies(data[&#39;Embarked&#39;]) data=pd.concat([data, table2], axis=1) . 3. Exploratory data analysis. . 1, At least two plots describing different aspects of the data set (e.g. identifying outliers, histograms of different distributions, or scatter plots to explore correlations). . table3=data.drop([&#39;Name&#39;,&#39;Sex&#39;,&#39;Embarked&#39;],axis=1) plt.figure(figsize=(8,8)) sns.heatmap(table3.astype(float).corr(), mask=np.triu(table3.astype(float).corr()), cmap = sns.diverging_palette(230, 20, as_cmap=True), annot=True, fmt=&#39;.1g&#39;, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4500430358&gt; . sns.pointplot(x=&quot;Embarked&quot;, y=&quot;Survived&quot;, hue=&quot;Sex&quot;, kind=&quot;box&quot;, data=data,palette=&quot;Set3&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4500422630&gt; . 2, Print a basic data description (e.g. number of examples, number features, number of examples in each class and such). . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1305 entries, 0 to 1308 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 index 1305 non-null int64 1 Survived 1305 non-null int64 2 Pclass 1305 non-null int64 3 Name 1305 non-null object 4 Sex 1305 non-null object 5 Age 1305 non-null float64 6 SibSp 1305 non-null int64 7 Parch 1305 non-null int64 8 Fare 1305 non-null float64 9 Embarked 1305 non-null object 10 female 1305 non-null uint8 11 male 1305 non-null uint8 12 C 1305 non-null uint8 13 Q 1305 non-null uint8 14 S 1305 non-null uint8 dtypes: float64(2), int64(5), object(3), uint8(5) memory usage: 158.5+ KB . 3, Print (or include in the plots) descriptive statistics (e.g. means, medians, standard deviation) . data.describe() . index Survived Pclass Age SibSp Parch Fare female male C Q S . count 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | 1305.000000 | . mean 369.065900 | 0.375479 | 2.298851 | 29.847057 | 0.500383 | 0.384674 | 31.811857 | 0.355556 | 0.644444 | 0.203831 | 0.094253 | 0.701916 | . std 248.772213 | 0.484432 | 0.836040 | 12.876700 | 1.042888 | 0.866421 | 44.489559 | 0.478865 | 0.478865 | 0.403000 | 0.292292 | 0.457592 | . min 0.000000 | 0.000000 | 1.000000 | 0.170000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 163.000000 | 0.000000 | 2.000000 | 22.000000 | 0.000000 | 0.000000 | 7.895800 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 326.000000 | 0.000000 | 3.000000 | 29.881138 | 0.000000 | 0.000000 | 14.454200 | 0.000000 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | . 75% 562.000000 | 1.000000 | 3.000000 | 35.000000 | 1.000000 | 0.000000 | 31.000000 | 1.000000 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | . max 890.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 9.000000 | 263.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | 1.000000 | . 4. Partition data into train, validation and test sets. . From Lecture06.slide: training set: 60% of total data set 13050.6= 783 Validation set: 20% of total data set 13050.2 = 261 Testing setzz: 20% of total data set 1305*0.2=261 . train_data=data[:783] valid_data=data[783:1044] test_data=data[1044:] . 5. Fit models on the training set (this can include a hyper-parameter search) and select the best based on validation set performance. . 1，building the machine learning model for both test and valid data . def build_x(df): return StandardScaler().fit_transform(df.drop(columns=[&#39;Name&#39;,&#39;Sex&#39;,&#39;Embarked&#39;,&#39;index&#39;,&#39;Survived&#39;])) . train_x=build_x(train_data) valid_x=build_x(valid_data) test_x=build_x(test_data) . train_y = train_data[&#39;Survived&#39;].values valid_y = valid_data[&#39;Survived&#39;].values test_y = test_data[&#39;Survived&#39;].values . 2, runing into different model . parameters={&#39;criterion&#39;:(&#39;gini&#39;,&#39;entropy&#39;), &#39;splitter&#39;:(&#39;random&#39;,&#39;best&#39;),&#39;max_depth&#39;:range(1,5)} clf=tree.DecisionTreeClassifier(random_state=30) clf_gs=GridSearchCV(clf,parameters) clf_gs=clf_gs.fit(train_x,train_y) clf_score=clf_gs.score(valid_x,valid_y) . parameters={&#39;criterion&#39;:(&#39;gini&#39;,&#39;entropy&#39;), &#39;max_features&#39;:(&#39;auto&#39;,&#39;sqrt&#39;,&#39;log2&#39;),&#39;max_depth&#39;:range(1,5)} random_forest=RandomForestClassifier() random_forest_rs=RandomizedSearchCV(random_forest,parameters) random_forest_rs=random_forest_rs.fit(train_x,train_y) random_forest_score=random_forest_rs.score(valid_x,valid_y) . Gradient_Boosting=GradientBoostingClassifier().fit(train_x,train_y) Gradient_Boosting_score=Gradient_Boosting.score(valid_x,valid_y) . parameters={&#39;solver&#39;:(&#39;newton-cg&#39;, &#39;lbfgs&#39;, &#39;liblinear&#39;, &#39;sag&#39;, &#39;saga&#39;)} logis_R=LogisticRegression() logis_R_gs=GridSearchCV(logis_R,parameters) logis_R_gs=logis_R_gs.fit(train_x,train_y) logis_R_score=logis_R_gs.score(valid_x,valid_y) . GNB=GaussianNB().fit(train_x,train_y) GNB.score=GNB.score(valid_x,valid_y) . parameters={&#39;loss&#39;:(&#39;deviance&#39;,&#39;exponential&#39;),&#39;learning_rate&#39;:[0.01,0.05,0.1,0.2],&#39;n_estimators&#39;:[50,100,150]} SGD=GradientBoostingClassifier() SGD_gs=GridSearchCV(SGD,parameters) SGD_gs=SGD_gs.fit(train_x,train_y) SGD_score=SGD_gs.score(valid_x,valid_y) SGD_score . 0.8505747126436781 . Xgboost=XGBClassifier().fit(train_x,train_y) Xgboost_score=Xgboost.score(valid_x,valid_y) . 3, select the table from best performance of validation . results = pd.DataFrame({ &#39;Model&#39;: [&#39;Decision Tree&#39;, &#39;Random Forest Classifier&#39;,&#39;Gradient Boosting&#39;, &#39;Logistic Regression&#39;,&#39;Gaussian Naive Bayes&#39;,&#39;Stochastic Gradient Decent&#39;, &#39;xgbooste&#39;], &#39;Score&#39;: [clf_score,random_forest_score,Gradient_Boosting_score, logis_R_score,GNB.score,SGD_score,Xgboost_score]}) result_df = results.sort_values(by=&#39;Score&#39;, ascending=False) result_df = result_df.set_index(&#39;Score&#39;) print(result_df) . Model Score 0.915709 Random Forest Classifier 0.892720 Gaussian Naive Bayes 0.885057 Logistic Regression 0.865900 Decision Tree 0.865900 Gradient Boosting 0.858238 xgbooste 0.850575 Stochastic Gradient Decent . 6. Print the results of the final model on the test set. This should include accuracy, F1-score and AUC. . Y_prediction = random_forest_rs.predict(test_x) . accuracy=accuracy_score(test_y, Y_prediction) print(&#39;accuracy:&#39;, accuracy) . accuracy: 0.9693486590038314 . f1_score=f1_score(test_y, Y_prediction) print(&#39;F1 score:&#39;,f1_score ) . F1 score: 0.9574468085106385 . y_scores = random_forest_rs.predict_proba(test_x)[:,1] r_a_score = roc_auc_score(test_y, y_scores) print(&quot;ROC-AUC-Score:&quot;, r_a_score) . ROC-AUC-Score: 0.9959867499044465 . Final_result = pd.DataFrame({ &#39;Indicator&#39;: [&#39;Accuracy&#39;,&#39;F1 score&#39;,&#39;AUC Score&#39;], &#39;Score&#39;: [accuracy,f1_score,r_a_score]}) print(Final_result) . Indicator Score 0 Accuracy 0.969349 1 F1 score 0.957447 2 AUC Score 0.995987 .",
            "url": "https://joery15.github.io/workshop/2021/01/03/ML_Model2.html",
            "relUrl": "/2021/01/03/ML_Model2.html",
            "date": " • Jan 3, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://joery15.github.io/workshop/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://joery15.github.io/workshop/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Driven one-year studies Master of Artificial Intelligence from Schulich School of Busines. Excellent communication capabilities. Experience with Convolutional Neural Network (CNN), and specific CNN frameworks like AlexNet and ResNet. Currently seeking a working opportunity in the data-related field. Driven to bring extensive theoretical and practical knowledge of Artificial Intelligence. Passionate about emerging technologies and business intelligence. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://joery15.github.io/workshop/www.linkedin.com/in/luo-jiayi/",
          "relUrl": "/www.linkedin.com/in/luo-jiayi/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://joery15.github.io/workshop/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}